{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saf02Sam/general/blob/main/Saffa_Samreen_assignment2_problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUwNhdPCf8Xb"
      },
      "source": [
        "#q1\n",
        "1- Modify your create_nn method to use Mini-batch gradient descent (1pt) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SVypEyw_ELzE",
        "outputId": "a0e5da2e-3849-4ed6-de36-0db55ea4a8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 0, Batch 52/100: Batch Loss = 1.3671\n",
            "Iteration 0, Batch 53/100: Batch Loss = 1.3672\n",
            "Iteration 0, Batch 54/100: Batch Loss = 1.3561\n",
            "Iteration 0, Batch 55/100: Batch Loss = 1.3582\n",
            "Iteration 0, Batch 56/100: Batch Loss = 1.3540\n",
            "Iteration 0, Batch 57/100: Batch Loss = 1.3511\n",
            "Iteration 0, Batch 58/100: Batch Loss = 1.3652\n",
            "Iteration 0, Batch 59/100: Batch Loss = 1.3547\n",
            "Iteration 0, Batch 60/100: Batch Loss = 1.3605\n",
            "Iteration 0, Batch 61/100: Batch Loss = 1.3449\n",
            "Iteration 0, Batch 62/100: Batch Loss = 1.3521\n",
            "Iteration 0, Batch 63/100: Batch Loss = 1.3748\n",
            "Iteration 0, Batch 64/100: Batch Loss = 1.3629\n",
            "Iteration 0, Batch 65/100: Batch Loss = 1.3621\n",
            "Iteration 0, Batch 66/100: Batch Loss = 1.3674\n",
            "Iteration 0, Batch 67/100: Batch Loss = 1.3568\n",
            "Iteration 0, Batch 68/100: Batch Loss = 1.3544\n",
            "Iteration 0, Batch 69/100: Batch Loss = 1.3486\n",
            "Iteration 0, Batch 70/100: Batch Loss = 1.3569\n",
            "Iteration 0, Batch 71/100: Batch Loss = 1.3655\n",
            "Iteration 0, Batch 72/100: Batch Loss = 1.3395\n",
            "Iteration 0, Batch 73/100: Batch Loss = 1.3516\n",
            "Iteration 0, Batch 74/100: Batch Loss = 1.3494\n",
            "Iteration 0, Batch 75/100: Batch Loss = 1.3416\n",
            "Iteration 0, Batch 76/100: Batch Loss = 1.3389\n",
            "Iteration 0, Batch 77/100: Batch Loss = 1.3570\n",
            "Iteration 0, Batch 78/100: Batch Loss = 1.3687\n",
            "Iteration 0, Batch 79/100: Batch Loss = 1.3565\n",
            "Iteration 0, Batch 80/100: Batch Loss = 1.3428\n",
            "Iteration 0, Batch 81/100: Batch Loss = 1.3695\n",
            "Iteration 0, Batch 82/100: Batch Loss = 1.3897\n",
            "Iteration 0, Batch 83/100: Batch Loss = 1.3457\n",
            "Iteration 0, Batch 84/100: Batch Loss = 1.3670\n",
            "Iteration 0, Batch 85/100: Batch Loss = 1.3614\n",
            "Iteration 0, Batch 86/100: Batch Loss = 1.3832\n",
            "Iteration 0, Batch 87/100: Batch Loss = 1.3399\n",
            "Iteration 0, Batch 88/100: Batch Loss = 1.3660\n",
            "Iteration 0, Batch 89/100: Batch Loss = 1.3658\n",
            "Iteration 0, Batch 90/100: Batch Loss = 1.3655\n",
            "Iteration 0, Batch 91/100: Batch Loss = 1.3313\n",
            "Iteration 0, Batch 92/100: Batch Loss = 1.3378\n",
            "Iteration 0, Batch 93/100: Batch Loss = 1.3477\n",
            "Iteration 0, Batch 94/100: Batch Loss = 1.3424\n",
            "Iteration 0, Batch 95/100: Batch Loss = 1.3730\n",
            "Iteration 0, Batch 96/100: Batch Loss = 1.3363\n",
            "Iteration 0, Batch 97/100: Batch Loss = 1.3409\n",
            "Iteration 0, Batch 98/100: Batch Loss = 1.3377\n",
            "Iteration 0, Batch 99/100: Batch Loss = 1.3343\n",
            "Iteration 0, Batch 100/100: Batch Loss = 1.3287\n",
            "Iteration 0: Train Loss = 1.3654, Val Loss = 1.3467\n",
            "Iteration 1, Batch 1/100: Batch Loss = 1.2998\n",
            "Iteration 1, Batch 2/100: Batch Loss = 1.3377\n",
            "Iteration 1, Batch 3/100: Batch Loss = 1.3516\n",
            "Iteration 1, Batch 4/100: Batch Loss = 1.3396\n",
            "Iteration 1, Batch 5/100: Batch Loss = 1.3569\n",
            "Iteration 1, Batch 6/100: Batch Loss = 1.3272\n",
            "Iteration 1, Batch 7/100: Batch Loss = 1.3363\n",
            "Iteration 1, Batch 8/100: Batch Loss = 1.3235\n",
            "Iteration 1, Batch 9/100: Batch Loss = 1.3404\n",
            "Iteration 1, Batch 10/100: Batch Loss = 1.3592\n",
            "Iteration 1, Batch 11/100: Batch Loss = 1.3612\n",
            "Iteration 1, Batch 12/100: Batch Loss = 1.3481\n",
            "Iteration 1, Batch 13/100: Batch Loss = 1.3792\n",
            "Iteration 1, Batch 14/100: Batch Loss = 1.3184\n",
            "Iteration 1, Batch 15/100: Batch Loss = 1.3535\n",
            "Iteration 1, Batch 16/100: Batch Loss = 1.3270\n",
            "Iteration 1, Batch 17/100: Batch Loss = 1.3101\n",
            "Iteration 1, Batch 18/100: Batch Loss = 1.3320\n",
            "Iteration 1, Batch 19/100: Batch Loss = 1.3473\n",
            "Iteration 1, Batch 20/100: Batch Loss = 1.3371\n",
            "Iteration 1, Batch 21/100: Batch Loss = 1.3510\n",
            "Iteration 1, Batch 22/100: Batch Loss = 1.3426\n",
            "Iteration 1, Batch 23/100: Batch Loss = 1.3343\n",
            "Iteration 1, Batch 24/100: Batch Loss = 1.4015\n",
            "Iteration 1, Batch 25/100: Batch Loss = 1.3626\n",
            "Iteration 1, Batch 26/100: Batch Loss = 1.3195\n",
            "Iteration 1, Batch 27/100: Batch Loss = 1.3595\n",
            "Iteration 1, Batch 28/100: Batch Loss = 1.3358\n",
            "Iteration 1, Batch 29/100: Batch Loss = 1.3362\n",
            "Iteration 1, Batch 30/100: Batch Loss = 1.3604\n",
            "Iteration 1, Batch 31/100: Batch Loss = 1.3108\n",
            "Iteration 1, Batch 32/100: Batch Loss = 1.3135\n",
            "Iteration 1, Batch 33/100: Batch Loss = 1.3344\n",
            "Iteration 1, Batch 34/100: Batch Loss = 1.3229\n",
            "Iteration 1, Batch 35/100: Batch Loss = 1.3483\n",
            "Iteration 1, Batch 36/100: Batch Loss = 1.3621\n",
            "Iteration 1, Batch 37/100: Batch Loss = 1.3295\n",
            "Iteration 1, Batch 38/100: Batch Loss = 1.3521\n",
            "Iteration 1, Batch 39/100: Batch Loss = 1.3106\n",
            "Iteration 1, Batch 40/100: Batch Loss = 1.3287\n",
            "Iteration 1, Batch 41/100: Batch Loss = 1.3357\n",
            "Iteration 1, Batch 42/100: Batch Loss = 1.3462\n",
            "Iteration 1, Batch 43/100: Batch Loss = 1.3326\n",
            "Iteration 1, Batch 44/100: Batch Loss = 1.3128\n",
            "Iteration 1, Batch 45/100: Batch Loss = 1.3275\n",
            "Iteration 1, Batch 46/100: Batch Loss = 1.3654\n",
            "Iteration 1, Batch 47/100: Batch Loss = 1.3035\n",
            "Iteration 1, Batch 48/100: Batch Loss = 1.3223\n",
            "Iteration 1, Batch 49/100: Batch Loss = 1.3217\n",
            "Iteration 1, Batch 50/100: Batch Loss = 1.2858\n",
            "Iteration 1, Batch 51/100: Batch Loss = 1.3530\n",
            "Iteration 1, Batch 52/100: Batch Loss = 1.3646\n",
            "Iteration 1, Batch 53/100: Batch Loss = 1.3407\n",
            "Iteration 1, Batch 54/100: Batch Loss = 1.3485\n",
            "Iteration 1, Batch 55/100: Batch Loss = 1.3403\n",
            "Iteration 1, Batch 56/100: Batch Loss = 1.3321\n",
            "Iteration 1, Batch 57/100: Batch Loss = 1.3319\n",
            "Iteration 1, Batch 58/100: Batch Loss = 1.3642\n",
            "Iteration 1, Batch 59/100: Batch Loss = 1.3723\n",
            "Iteration 1, Batch 60/100: Batch Loss = 1.3275\n",
            "Iteration 1, Batch 61/100: Batch Loss = 1.3067\n",
            "Iteration 1, Batch 62/100: Batch Loss = 1.3347\n",
            "Iteration 1, Batch 63/100: Batch Loss = 1.3138\n",
            "Iteration 1, Batch 64/100: Batch Loss = 1.3174\n",
            "Iteration 1, Batch 65/100: Batch Loss = 1.3385\n",
            "Iteration 1, Batch 66/100: Batch Loss = 1.3593\n",
            "Iteration 1, Batch 67/100: Batch Loss = 1.3422\n",
            "Iteration 1, Batch 68/100: Batch Loss = 1.2963\n",
            "Iteration 1, Batch 69/100: Batch Loss = 1.3503\n",
            "Iteration 1, Batch 70/100: Batch Loss = 1.3550\n",
            "Iteration 1, Batch 71/100: Batch Loss = 1.3248\n",
            "Iteration 1, Batch 72/100: Batch Loss = 1.3630\n",
            "Iteration 1, Batch 73/100: Batch Loss = 1.3413\n",
            "Iteration 1, Batch 74/100: Batch Loss = 1.3087\n",
            "Iteration 1, Batch 75/100: Batch Loss = 1.3464\n",
            "Iteration 1, Batch 76/100: Batch Loss = 1.2888\n",
            "Iteration 1, Batch 77/100: Batch Loss = 1.3451\n",
            "Iteration 1, Batch 78/100: Batch Loss = 1.3284\n",
            "Iteration 1, Batch 79/100: Batch Loss = 1.3593\n",
            "Iteration 1, Batch 80/100: Batch Loss = 1.3499\n",
            "Iteration 1, Batch 81/100: Batch Loss = 1.2912\n",
            "Iteration 1, Batch 82/100: Batch Loss = 1.3004\n",
            "Iteration 1, Batch 83/100: Batch Loss = 1.2949\n",
            "Iteration 1, Batch 84/100: Batch Loss = 1.3542\n",
            "Iteration 1, Batch 85/100: Batch Loss = 1.3401\n",
            "Iteration 1, Batch 86/100: Batch Loss = 1.2986\n",
            "Iteration 1, Batch 87/100: Batch Loss = 1.3584\n",
            "Iteration 1, Batch 88/100: Batch Loss = 1.3211\n",
            "Iteration 1, Batch 89/100: Batch Loss = 1.3067\n",
            "Iteration 1, Batch 90/100: Batch Loss = 1.3346\n",
            "Iteration 1, Batch 91/100: Batch Loss = 1.3252\n",
            "Iteration 1, Batch 92/100: Batch Loss = 1.2823\n",
            "Iteration 1, Batch 93/100: Batch Loss = 1.3484\n",
            "Iteration 1, Batch 94/100: Batch Loss = 1.3339\n",
            "Iteration 1, Batch 95/100: Batch Loss = 1.3004\n",
            "Iteration 1, Batch 96/100: Batch Loss = 1.3384\n",
            "Iteration 1, Batch 97/100: Batch Loss = 1.2995\n",
            "Iteration 1, Batch 98/100: Batch Loss = 1.3381\n",
            "Iteration 1, Batch 99/100: Batch Loss = 1.3429\n",
            "Iteration 1, Batch 100/100: Batch Loss = 1.3380\n",
            "Iteration 1: Train Loss = 1.3345, Val Loss = 1.3225\n",
            "Iteration 2, Batch 1/100: Batch Loss = 1.2885\n",
            "Iteration 2, Batch 2/100: Batch Loss = 1.3430\n",
            "Iteration 2, Batch 3/100: Batch Loss = 1.3377\n",
            "Iteration 2, Batch 4/100: Batch Loss = 1.3671\n",
            "Iteration 2, Batch 5/100: Batch Loss = 1.2878\n",
            "Iteration 2, Batch 6/100: Batch Loss = 1.3074\n",
            "Iteration 2, Batch 7/100: Batch Loss = 1.2718\n",
            "Iteration 2, Batch 8/100: Batch Loss = 1.2862\n",
            "Iteration 2, Batch 9/100: Batch Loss = 1.3264\n",
            "Iteration 2, Batch 10/100: Batch Loss = 1.3264\n",
            "Iteration 2, Batch 11/100: Batch Loss = 1.2898\n",
            "Iteration 2, Batch 12/100: Batch Loss = 1.2637\n",
            "Iteration 2, Batch 13/100: Batch Loss = 1.3365\n",
            "Iteration 2, Batch 14/100: Batch Loss = 1.3259\n",
            "Iteration 2, Batch 15/100: Batch Loss = 1.3565\n",
            "Iteration 2, Batch 16/100: Batch Loss = 1.3360\n",
            "Iteration 2, Batch 17/100: Batch Loss = 1.3461\n",
            "Iteration 2, Batch 18/100: Batch Loss = 1.2934\n",
            "Iteration 2, Batch 19/100: Batch Loss = 1.3358\n",
            "Iteration 2, Batch 20/100: Batch Loss = 1.2983\n",
            "Iteration 2, Batch 21/100: Batch Loss = 1.3514\n",
            "Iteration 2, Batch 22/100: Batch Loss = 1.3082\n",
            "Iteration 2, Batch 23/100: Batch Loss = 1.3455\n",
            "Iteration 2, Batch 24/100: Batch Loss = 1.2920\n",
            "Iteration 2, Batch 25/100: Batch Loss = 1.3235\n",
            "Iteration 2, Batch 26/100: Batch Loss = 1.2794\n",
            "Iteration 2, Batch 27/100: Batch Loss = 1.3295\n",
            "Iteration 2, Batch 28/100: Batch Loss = 1.2852\n",
            "Iteration 2, Batch 29/100: Batch Loss = 1.3334\n",
            "Iteration 2, Batch 30/100: Batch Loss = 1.3012\n",
            "Iteration 2, Batch 31/100: Batch Loss = 1.3173\n",
            "Iteration 2, Batch 32/100: Batch Loss = 1.3110\n",
            "Iteration 2, Batch 33/100: Batch Loss = 1.3181\n",
            "Iteration 2, Batch 34/100: Batch Loss = 1.3123\n",
            "Iteration 2, Batch 35/100: Batch Loss = 1.3383\n",
            "Iteration 2, Batch 36/100: Batch Loss = 1.3490\n",
            "Iteration 2, Batch 37/100: Batch Loss = 1.3070\n",
            "Iteration 2, Batch 38/100: Batch Loss = 1.2930\n",
            "Iteration 2, Batch 39/100: Batch Loss = 1.3089\n",
            "Iteration 2, Batch 40/100: Batch Loss = 1.3392\n",
            "Iteration 2, Batch 41/100: Batch Loss = 1.3323\n",
            "Iteration 2, Batch 42/100: Batch Loss = 1.3523\n",
            "Iteration 2, Batch 43/100: Batch Loss = 1.3048\n",
            "Iteration 2, Batch 44/100: Batch Loss = 1.3034\n",
            "Iteration 2, Batch 45/100: Batch Loss = 1.3503\n",
            "Iteration 2, Batch 46/100: Batch Loss = 1.3267\n",
            "Iteration 2, Batch 47/100: Batch Loss = 1.3437\n",
            "Iteration 2, Batch 48/100: Batch Loss = 1.3968\n",
            "Iteration 2, Batch 49/100: Batch Loss = 1.3101\n",
            "Iteration 2, Batch 50/100: Batch Loss = 1.3483\n",
            "Iteration 2, Batch 51/100: Batch Loss = 1.3381\n",
            "Iteration 2, Batch 52/100: Batch Loss = 1.3091\n",
            "Iteration 2, Batch 53/100: Batch Loss = 1.2818\n",
            "Iteration 2, Batch 54/100: Batch Loss = 1.2697\n",
            "Iteration 2, Batch 55/100: Batch Loss = 1.3506\n",
            "Iteration 2, Batch 56/100: Batch Loss = 1.3199\n",
            "Iteration 2, Batch 57/100: Batch Loss = 1.2962\n",
            "Iteration 2, Batch 58/100: Batch Loss = 1.2909\n",
            "Iteration 2, Batch 59/100: Batch Loss = 1.3301\n",
            "Iteration 2, Batch 60/100: Batch Loss = 1.3554\n",
            "Iteration 2, Batch 61/100: Batch Loss = 1.3271\n",
            "Iteration 2, Batch 62/100: Batch Loss = 1.3253\n",
            "Iteration 2, Batch 63/100: Batch Loss = 1.2655\n",
            "Iteration 2, Batch 64/100: Batch Loss = 1.3244\n",
            "Iteration 2, Batch 65/100: Batch Loss = 1.3125\n",
            "Iteration 2, Batch 66/100: Batch Loss = 1.3146\n",
            "Iteration 2, Batch 67/100: Batch Loss = 1.3067\n",
            "Iteration 2, Batch 68/100: Batch Loss = 1.3245\n",
            "Iteration 2, Batch 69/100: Batch Loss = 1.3424\n",
            "Iteration 2, Batch 70/100: Batch Loss = 1.3659\n",
            "Iteration 2, Batch 71/100: Batch Loss = 1.3175\n",
            "Iteration 2, Batch 72/100: Batch Loss = 1.3442\n",
            "Iteration 2, Batch 73/100: Batch Loss = 1.2941\n",
            "Iteration 2, Batch 74/100: Batch Loss = 1.2944\n",
            "Iteration 2, Batch 75/100: Batch Loss = 1.2807\n",
            "Iteration 2, Batch 76/100: Batch Loss = 1.3187\n",
            "Iteration 2, Batch 77/100: Batch Loss = 1.2509\n",
            "Iteration 2, Batch 78/100: Batch Loss = 1.3108\n",
            "Iteration 2, Batch 79/100: Batch Loss = 1.3106\n",
            "Iteration 2, Batch 80/100: Batch Loss = 1.3243\n",
            "Iteration 2, Batch 81/100: Batch Loss = 1.2937\n",
            "Iteration 2, Batch 82/100: Batch Loss = 1.2922\n",
            "Iteration 2, Batch 83/100: Batch Loss = 1.2992\n",
            "Iteration 2, Batch 84/100: Batch Loss = 1.3246\n",
            "Iteration 2, Batch 85/100: Batch Loss = 1.2981\n",
            "Iteration 2, Batch 86/100: Batch Loss = 1.3031\n",
            "Iteration 2, Batch 87/100: Batch Loss = 1.2854\n",
            "Iteration 2, Batch 88/100: Batch Loss = 1.2405\n",
            "Iteration 2, Batch 89/100: Batch Loss = 1.3420\n",
            "Iteration 2, Batch 90/100: Batch Loss = 1.2904\n",
            "Iteration 2, Batch 91/100: Batch Loss = 1.2776\n",
            "Iteration 2, Batch 92/100: Batch Loss = 1.3093\n",
            "Iteration 2, Batch 93/100: Batch Loss = 1.2767\n",
            "Iteration 2, Batch 94/100: Batch Loss = 1.3810\n",
            "Iteration 2, Batch 95/100: Batch Loss = 1.3416\n",
            "Iteration 2, Batch 96/100: Batch Loss = 1.3676\n",
            "Iteration 2, Batch 97/100: Batch Loss = 1.2833\n",
            "Iteration 2, Batch 98/100: Batch Loss = 1.2630\n",
            "Iteration 2, Batch 99/100: Batch Loss = 1.3282\n",
            "Iteration 2, Batch 100/100: Batch Loss = 1.3281\n",
            "Iteration 2: Train Loss = 1.3156, Val Loss = 1.3076\n",
            "Iteration 3, Batch 1/100: Batch Loss = 1.3273\n",
            "Iteration 3, Batch 2/100: Batch Loss = 1.3285\n",
            "Iteration 3, Batch 3/100: Batch Loss = 1.2359\n",
            "Iteration 3, Batch 4/100: Batch Loss = 1.2824\n",
            "Iteration 3, Batch 5/100: Batch Loss = 1.2745\n",
            "Iteration 3, Batch 6/100: Batch Loss = 1.2474\n",
            "Iteration 3, Batch 7/100: Batch Loss = 1.3277\n",
            "Iteration 3, Batch 8/100: Batch Loss = 1.3406\n",
            "Iteration 3, Batch 9/100: Batch Loss = 1.3138\n",
            "Iteration 3, Batch 10/100: Batch Loss = 1.2936\n",
            "Iteration 3, Batch 11/100: Batch Loss = 1.3816\n",
            "Iteration 3, Batch 12/100: Batch Loss = 1.3669\n",
            "Iteration 3, Batch 13/100: Batch Loss = 1.3612\n",
            "Iteration 3, Batch 14/100: Batch Loss = 1.3416\n",
            "Iteration 3, Batch 15/100: Batch Loss = 1.2934\n",
            "Iteration 3, Batch 16/100: Batch Loss = 1.2740\n",
            "Iteration 3, Batch 17/100: Batch Loss = 1.3408\n",
            "Iteration 3, Batch 18/100: Batch Loss = 1.2995\n",
            "Iteration 3, Batch 19/100: Batch Loss = 1.3409\n",
            "Iteration 3, Batch 20/100: Batch Loss = 1.3200\n",
            "Iteration 3, Batch 21/100: Batch Loss = 1.2998\n",
            "Iteration 3, Batch 22/100: Batch Loss = 1.3069\n",
            "Iteration 3, Batch 23/100: Batch Loss = 1.2990\n",
            "Iteration 3, Batch 24/100: Batch Loss = 1.3741\n",
            "Iteration 3, Batch 25/100: Batch Loss = 1.2786\n",
            "Iteration 3, Batch 26/100: Batch Loss = 1.3752\n",
            "Iteration 3, Batch 27/100: Batch Loss = 1.3258\n",
            "Iteration 3, Batch 28/100: Batch Loss = 1.3279\n",
            "Iteration 3, Batch 29/100: Batch Loss = 1.2994\n",
            "Iteration 3, Batch 30/100: Batch Loss = 1.3772\n",
            "Iteration 3, Batch 31/100: Batch Loss = 1.3269\n",
            "Iteration 3, Batch 32/100: Batch Loss = 1.2713\n",
            "Iteration 3, Batch 33/100: Batch Loss = 1.3469\n",
            "Iteration 3, Batch 34/100: Batch Loss = 1.3050\n",
            "Iteration 3, Batch 35/100: Batch Loss = 1.2913\n",
            "Iteration 3, Batch 36/100: Batch Loss = 1.3196\n",
            "Iteration 3, Batch 37/100: Batch Loss = 1.2221\n",
            "Iteration 3, Batch 38/100: Batch Loss = 1.2906\n",
            "Iteration 3, Batch 39/100: Batch Loss = 1.3193\n",
            "Iteration 3, Batch 40/100: Batch Loss = 1.2758\n",
            "Iteration 3, Batch 41/100: Batch Loss = 1.3043\n",
            "Iteration 3, Batch 42/100: Batch Loss = 1.2630\n",
            "Iteration 3, Batch 43/100: Batch Loss = 1.2330\n",
            "Iteration 3, Batch 44/100: Batch Loss = 1.3169\n",
            "Iteration 3, Batch 45/100: Batch Loss = 1.2962\n",
            "Iteration 3, Batch 46/100: Batch Loss = 1.2751\n",
            "Iteration 3, Batch 47/100: Batch Loss = 1.2901\n",
            "Iteration 3, Batch 48/100: Batch Loss = 1.3539\n",
            "Iteration 3, Batch 49/100: Batch Loss = 1.2383\n",
            "Iteration 3, Batch 50/100: Batch Loss = 1.2601\n",
            "Iteration 3, Batch 51/100: Batch Loss = 1.1869\n",
            "Iteration 3, Batch 52/100: Batch Loss = 1.3553\n",
            "Iteration 3, Batch 53/100: Batch Loss = 1.3026\n",
            "Iteration 3, Batch 54/100: Batch Loss = 1.3620\n",
            "Iteration 3, Batch 55/100: Batch Loss = 1.3476\n",
            "Iteration 3, Batch 56/100: Batch Loss = 1.2596\n",
            "Iteration 3, Batch 57/100: Batch Loss = 1.3018\n",
            "Iteration 3, Batch 58/100: Batch Loss = 1.3009\n",
            "Iteration 3, Batch 59/100: Batch Loss = 1.2370\n",
            "Iteration 3, Batch 60/100: Batch Loss = 1.3380\n",
            "Iteration 3, Batch 61/100: Batch Loss = 1.3032\n",
            "Iteration 3, Batch 62/100: Batch Loss = 1.3699\n",
            "Iteration 3, Batch 63/100: Batch Loss = 1.2572\n",
            "Iteration 3, Batch 64/100: Batch Loss = 1.3477\n",
            "Iteration 3, Batch 65/100: Batch Loss = 1.3525\n",
            "Iteration 3, Batch 66/100: Batch Loss = 1.2576\n",
            "Iteration 3, Batch 67/100: Batch Loss = 1.2124\n",
            "Iteration 3, Batch 68/100: Batch Loss = 1.3306\n",
            "Iteration 3, Batch 69/100: Batch Loss = 1.2941\n",
            "Iteration 3, Batch 70/100: Batch Loss = 1.2947\n",
            "Iteration 3, Batch 71/100: Batch Loss = 1.3398\n",
            "Iteration 3, Batch 72/100: Batch Loss = 1.2883\n",
            "Iteration 3, Batch 73/100: Batch Loss = 1.2937\n",
            "Iteration 3, Batch 74/100: Batch Loss = 1.3596\n",
            "Iteration 3, Batch 75/100: Batch Loss = 1.2803\n",
            "Iteration 3, Batch 76/100: Batch Loss = 1.2692\n",
            "Iteration 3, Batch 77/100: Batch Loss = 1.3309\n",
            "Iteration 3, Batch 78/100: Batch Loss = 1.3229\n",
            "Iteration 3, Batch 79/100: Batch Loss = 1.3221\n",
            "Iteration 3, Batch 80/100: Batch Loss = 1.2485\n",
            "Iteration 3, Batch 81/100: Batch Loss = 1.3317\n",
            "Iteration 3, Batch 82/100: Batch Loss = 1.3610\n",
            "Iteration 3, Batch 83/100: Batch Loss = 1.2789\n",
            "Iteration 3, Batch 84/100: Batch Loss = 1.2336\n",
            "Iteration 3, Batch 85/100: Batch Loss = 1.1938\n",
            "Iteration 3, Batch 86/100: Batch Loss = 1.3067\n",
            "Iteration 3, Batch 87/100: Batch Loss = 1.3720\n",
            "Iteration 3, Batch 88/100: Batch Loss = 1.3012\n",
            "Iteration 3, Batch 89/100: Batch Loss = 1.2719\n",
            "Iteration 3, Batch 90/100: Batch Loss = 1.2683\n",
            "Iteration 3, Batch 91/100: Batch Loss = 1.3627\n",
            "Iteration 3, Batch 92/100: Batch Loss = 1.3012\n",
            "Iteration 3, Batch 93/100: Batch Loss = 1.3544\n",
            "Iteration 3, Batch 94/100: Batch Loss = 1.2765\n",
            "Iteration 3, Batch 95/100: Batch Loss = 1.3239\n",
            "Iteration 3, Batch 96/100: Batch Loss = 1.2683\n",
            "Iteration 3, Batch 97/100: Batch Loss = 1.2847\n",
            "Iteration 3, Batch 98/100: Batch Loss = 1.2302\n",
            "Iteration 3, Batch 99/100: Batch Loss = 1.3069\n",
            "Iteration 3, Batch 100/100: Batch Loss = 1.3379\n",
            "Iteration 3: Train Loss = 1.3039, Val Loss = 1.2982\n",
            "Iteration 4, Batch 1/100: Batch Loss = 1.3215\n",
            "Iteration 4, Batch 2/100: Batch Loss = 1.2205\n",
            "Iteration 4, Batch 3/100: Batch Loss = 1.3159\n",
            "Iteration 4, Batch 4/100: Batch Loss = 1.3309\n",
            "Iteration 4, Batch 5/100: Batch Loss = 1.2607\n",
            "Iteration 4, Batch 6/100: Batch Loss = 1.3141\n",
            "Iteration 4, Batch 7/100: Batch Loss = 1.2901\n",
            "Iteration 4, Batch 8/100: Batch Loss = 1.2269\n",
            "Iteration 4, Batch 9/100: Batch Loss = 1.2914\n",
            "Iteration 4, Batch 10/100: Batch Loss = 1.3468\n",
            "Iteration 4, Batch 11/100: Batch Loss = 1.2665\n",
            "Iteration 4, Batch 12/100: Batch Loss = 1.2735\n",
            "Iteration 4, Batch 13/100: Batch Loss = 1.2900\n",
            "Iteration 4, Batch 14/100: Batch Loss = 1.2728\n",
            "Iteration 4, Batch 15/100: Batch Loss = 1.3058\n",
            "Iteration 4, Batch 16/100: Batch Loss = 1.3211\n",
            "Iteration 4, Batch 17/100: Batch Loss = 1.3544\n",
            "Iteration 4, Batch 18/100: Batch Loss = 1.4031\n",
            "Iteration 4, Batch 19/100: Batch Loss = 1.2905\n",
            "Iteration 4, Batch 20/100: Batch Loss = 1.3458\n",
            "Iteration 4, Batch 21/100: Batch Loss = 1.2812\n",
            "Iteration 4, Batch 22/100: Batch Loss = 1.2998\n",
            "Iteration 4, Batch 23/100: Batch Loss = 1.2245\n",
            "Iteration 4, Batch 24/100: Batch Loss = 1.2670\n",
            "Iteration 4, Batch 25/100: Batch Loss = 1.2561\n",
            "Iteration 4, Batch 26/100: Batch Loss = 1.3616\n",
            "Iteration 4, Batch 27/100: Batch Loss = 1.2776\n",
            "Iteration 4, Batch 28/100: Batch Loss = 1.2507\n",
            "Iteration 4, Batch 29/100: Batch Loss = 1.3121\n",
            "Iteration 4, Batch 30/100: Batch Loss = 1.2734\n",
            "Iteration 4, Batch 31/100: Batch Loss = 1.3297\n",
            "Iteration 4, Batch 32/100: Batch Loss = 1.2817\n",
            "Iteration 4, Batch 33/100: Batch Loss = 1.3492\n",
            "Iteration 4, Batch 34/100: Batch Loss = 1.2473\n",
            "Iteration 4, Batch 35/100: Batch Loss = 1.3047\n",
            "Iteration 4, Batch 36/100: Batch Loss = 1.3617\n",
            "Iteration 4, Batch 37/100: Batch Loss = 1.3295\n",
            "Iteration 4, Batch 38/100: Batch Loss = 1.4113\n",
            "Iteration 4, Batch 39/100: Batch Loss = 1.3211\n",
            "Iteration 4, Batch 40/100: Batch Loss = 1.3046\n",
            "Iteration 4, Batch 41/100: Batch Loss = 1.3065\n",
            "Iteration 4, Batch 42/100: Batch Loss = 1.2718\n",
            "Iteration 4, Batch 43/100: Batch Loss = 1.1565\n",
            "Iteration 4, Batch 44/100: Batch Loss = 1.2324\n",
            "Iteration 4, Batch 45/100: Batch Loss = 1.2473\n",
            "Iteration 4, Batch 46/100: Batch Loss = 1.3550\n",
            "Iteration 4, Batch 47/100: Batch Loss = 1.3201\n",
            "Iteration 4, Batch 48/100: Batch Loss = 1.2787\n",
            "Iteration 4, Batch 49/100: Batch Loss = 1.3110\n",
            "Iteration 4, Batch 50/100: Batch Loss = 1.3145\n",
            "Iteration 4, Batch 51/100: Batch Loss = 1.3283\n",
            "Iteration 4, Batch 52/100: Batch Loss = 1.3394\n",
            "Iteration 4, Batch 53/100: Batch Loss = 1.3888\n",
            "Iteration 4, Batch 54/100: Batch Loss = 1.2609\n",
            "Iteration 4, Batch 55/100: Batch Loss = 1.3117\n",
            "Iteration 4, Batch 56/100: Batch Loss = 1.2617\n",
            "Iteration 4, Batch 57/100: Batch Loss = 1.2522\n",
            "Iteration 4, Batch 58/100: Batch Loss = 1.2959\n",
            "Iteration 4, Batch 59/100: Batch Loss = 1.3795\n",
            "Iteration 4, Batch 60/100: Batch Loss = 1.3283\n",
            "Iteration 4, Batch 61/100: Batch Loss = 1.2946\n",
            "Iteration 4, Batch 62/100: Batch Loss = 1.2767\n",
            "Iteration 4, Batch 63/100: Batch Loss = 1.2404\n",
            "Iteration 4, Batch 64/100: Batch Loss = 1.2944\n",
            "Iteration 4, Batch 65/100: Batch Loss = 1.2887\n",
            "Iteration 4, Batch 66/100: Batch Loss = 1.3658\n",
            "Iteration 4, Batch 67/100: Batch Loss = 1.2605\n",
            "Iteration 4, Batch 68/100: Batch Loss = 1.3058\n",
            "Iteration 4, Batch 69/100: Batch Loss = 1.2198\n",
            "Iteration 4, Batch 70/100: Batch Loss = 1.1923\n",
            "Iteration 4, Batch 71/100: Batch Loss = 1.2914\n",
            "Iteration 4, Batch 72/100: Batch Loss = 1.3030\n",
            "Iteration 4, Batch 73/100: Batch Loss = 1.3561\n",
            "Iteration 4, Batch 74/100: Batch Loss = 1.2609\n",
            "Iteration 4, Batch 75/100: Batch Loss = 1.2937\n",
            "Iteration 4, Batch 76/100: Batch Loss = 1.3016\n",
            "Iteration 4, Batch 77/100: Batch Loss = 1.3913\n",
            "Iteration 4, Batch 78/100: Batch Loss = 1.1584\n",
            "Iteration 4, Batch 79/100: Batch Loss = 1.2096\n",
            "Iteration 4, Batch 80/100: Batch Loss = 1.3544\n",
            "Iteration 4, Batch 81/100: Batch Loss = 1.3370\n",
            "Iteration 4, Batch 82/100: Batch Loss = 1.3796\n",
            "Iteration 4, Batch 83/100: Batch Loss = 1.1995\n",
            "Iteration 4, Batch 84/100: Batch Loss = 1.2404\n",
            "Iteration 4, Batch 85/100: Batch Loss = 1.4163\n",
            "Iteration 4, Batch 86/100: Batch Loss = 1.2447\n",
            "Iteration 4, Batch 87/100: Batch Loss = 1.2342\n",
            "Iteration 4, Batch 88/100: Batch Loss = 1.3784\n",
            "Iteration 4, Batch 89/100: Batch Loss = 1.3196\n",
            "Iteration 4, Batch 90/100: Batch Loss = 1.3474\n",
            "Iteration 4, Batch 91/100: Batch Loss = 1.3462\n",
            "Iteration 4, Batch 92/100: Batch Loss = 1.2009\n",
            "Iteration 4, Batch 93/100: Batch Loss = 1.3098\n",
            "Iteration 4, Batch 94/100: Batch Loss = 1.2650\n",
            "Iteration 4, Batch 95/100: Batch Loss = 1.2222\n",
            "Iteration 4, Batch 96/100: Batch Loss = 1.3369\n",
            "Iteration 4, Batch 97/100: Batch Loss = 1.2667\n",
            "Iteration 4, Batch 98/100: Batch Loss = 1.3989\n",
            "Iteration 4, Batch 99/100: Batch Loss = 1.2561\n",
            "Iteration 4, Batch 100/100: Batch Loss = 1.2678\n",
            "Iteration 4: Train Loss = 1.2965, Val Loss = 1.2922\n",
            "Iteration 5, Batch 1/100: Batch Loss = 1.2059\n",
            "Iteration 5, Batch 2/100: Batch Loss = 1.2871\n",
            "Iteration 5, Batch 3/100: Batch Loss = 1.2931\n",
            "Iteration 5, Batch 4/100: Batch Loss = 1.2824\n",
            "Iteration 5, Batch 5/100: Batch Loss = 1.2827\n",
            "Iteration 5, Batch 6/100: Batch Loss = 1.2838\n",
            "Iteration 5, Batch 7/100: Batch Loss = 1.2654\n",
            "Iteration 5, Batch 8/100: Batch Loss = 1.2720\n",
            "Iteration 5, Batch 9/100: Batch Loss = 1.2832\n",
            "Iteration 5, Batch 10/100: Batch Loss = 1.2113\n",
            "Iteration 5, Batch 11/100: Batch Loss = 1.4095\n",
            "Iteration 5, Batch 12/100: Batch Loss = 1.2210\n",
            "Iteration 5, Batch 13/100: Batch Loss = 1.3302\n",
            "Iteration 5, Batch 14/100: Batch Loss = 1.2591\n",
            "Iteration 5, Batch 15/100: Batch Loss = 1.2663\n",
            "Iteration 5, Batch 16/100: Batch Loss = 1.2109\n",
            "Iteration 5, Batch 17/100: Batch Loss = 1.2554\n",
            "Iteration 5, Batch 18/100: Batch Loss = 1.2573\n",
            "Iteration 5, Batch 19/100: Batch Loss = 1.2293\n",
            "Iteration 5, Batch 20/100: Batch Loss = 1.2839\n",
            "Iteration 5, Batch 21/100: Batch Loss = 1.2490\n",
            "Iteration 5, Batch 22/100: Batch Loss = 1.3818\n",
            "Iteration 5, Batch 23/100: Batch Loss = 1.2907\n",
            "Iteration 5, Batch 24/100: Batch Loss = 1.2947\n",
            "Iteration 5, Batch 25/100: Batch Loss = 1.3552\n",
            "Iteration 5, Batch 26/100: Batch Loss = 1.2656\n",
            "Iteration 5, Batch 27/100: Batch Loss = 1.2907\n",
            "Iteration 5, Batch 28/100: Batch Loss = 1.3114\n",
            "Iteration 5, Batch 29/100: Batch Loss = 1.3011\n",
            "Iteration 5, Batch 30/100: Batch Loss = 1.2635\n",
            "Iteration 5, Batch 31/100: Batch Loss = 1.2948\n",
            "Iteration 5, Batch 32/100: Batch Loss = 1.3775\n",
            "Iteration 5, Batch 33/100: Batch Loss = 1.3125\n",
            "Iteration 5, Batch 34/100: Batch Loss = 1.3084\n",
            "Iteration 5, Batch 35/100: Batch Loss = 1.3653\n",
            "Iteration 5, Batch 36/100: Batch Loss = 1.2988\n",
            "Iteration 5, Batch 37/100: Batch Loss = 1.3208\n",
            "Iteration 5, Batch 38/100: Batch Loss = 1.3120\n",
            "Iteration 5, Batch 39/100: Batch Loss = 1.2619\n",
            "Iteration 5, Batch 40/100: Batch Loss = 1.2983\n",
            "Iteration 5, Batch 41/100: Batch Loss = 1.3006\n",
            "Iteration 5, Batch 42/100: Batch Loss = 1.3204\n",
            "Iteration 5, Batch 43/100: Batch Loss = 1.2437\n",
            "Iteration 5, Batch 44/100: Batch Loss = 1.3654\n",
            "Iteration 5, Batch 45/100: Batch Loss = 1.2983\n",
            "Iteration 5, Batch 46/100: Batch Loss = 1.3180\n",
            "Iteration 5, Batch 47/100: Batch Loss = 1.2849\n",
            "Iteration 5, Batch 48/100: Batch Loss = 1.3268\n",
            "Iteration 5, Batch 49/100: Batch Loss = 1.2926\n",
            "Iteration 5, Batch 50/100: Batch Loss = 1.2891\n",
            "Iteration 5, Batch 51/100: Batch Loss = 1.2704\n",
            "Iteration 5, Batch 52/100: Batch Loss = 1.2517\n",
            "Iteration 5, Batch 53/100: Batch Loss = 1.3019\n",
            "Iteration 5, Batch 54/100: Batch Loss = 1.3362\n",
            "Iteration 5, Batch 55/100: Batch Loss = 1.2972\n",
            "Iteration 5, Batch 56/100: Batch Loss = 1.3287\n",
            "Iteration 5, Batch 57/100: Batch Loss = 1.2289\n",
            "Iteration 5, Batch 58/100: Batch Loss = 1.2726\n",
            "Iteration 5, Batch 59/100: Batch Loss = 1.3455\n",
            "Iteration 5, Batch 60/100: Batch Loss = 1.3522\n",
            "Iteration 5, Batch 61/100: Batch Loss = 1.2935\n",
            "Iteration 5, Batch 62/100: Batch Loss = 1.2870\n",
            "Iteration 5, Batch 63/100: Batch Loss = 1.2459\n",
            "Iteration 5, Batch 64/100: Batch Loss = 1.3090\n",
            "Iteration 5, Batch 65/100: Batch Loss = 1.2992\n",
            "Iteration 5, Batch 66/100: Batch Loss = 1.2610\n",
            "Iteration 5, Batch 67/100: Batch Loss = 1.2759\n",
            "Iteration 5, Batch 68/100: Batch Loss = 1.3200\n",
            "Iteration 5, Batch 69/100: Batch Loss = 1.3298\n",
            "Iteration 5, Batch 70/100: Batch Loss = 1.2965\n",
            "Iteration 5, Batch 71/100: Batch Loss = 1.2150\n",
            "Iteration 5, Batch 72/100: Batch Loss = 1.3681\n",
            "Iteration 5, Batch 73/100: Batch Loss = 1.2348\n",
            "Iteration 5, Batch 74/100: Batch Loss = 1.3250\n",
            "Iteration 5, Batch 75/100: Batch Loss = 1.2478\n",
            "Iteration 5, Batch 76/100: Batch Loss = 1.3008\n",
            "Iteration 5, Batch 77/100: Batch Loss = 1.2506\n",
            "Iteration 5, Batch 78/100: Batch Loss = 1.4200\n",
            "Iteration 5, Batch 79/100: Batch Loss = 1.3157\n",
            "Iteration 5, Batch 80/100: Batch Loss = 1.2719\n",
            "Iteration 5, Batch 81/100: Batch Loss = 1.2247\n",
            "Iteration 5, Batch 82/100: Batch Loss = 1.2965\n",
            "Iteration 5, Batch 83/100: Batch Loss = 1.2759\n",
            "Iteration 5, Batch 84/100: Batch Loss = 1.2811\n",
            "Iteration 5, Batch 85/100: Batch Loss = 1.2850\n",
            "Iteration 5, Batch 86/100: Batch Loss = 1.2676\n",
            "Iteration 5, Batch 87/100: Batch Loss = 1.2647\n",
            "Iteration 5, Batch 88/100: Batch Loss = 1.2670\n",
            "Iteration 5, Batch 89/100: Batch Loss = 1.2793\n",
            "Iteration 5, Batch 90/100: Batch Loss = 1.2720\n",
            "Iteration 5, Batch 91/100: Batch Loss = 1.2039\n",
            "Iteration 5, Batch 92/100: Batch Loss = 1.3202\n",
            "Iteration 5, Batch 93/100: Batch Loss = 1.3110\n",
            "Iteration 5, Batch 94/100: Batch Loss = 1.2949\n",
            "Iteration 5, Batch 95/100: Batch Loss = 1.3362\n",
            "Iteration 5, Batch 96/100: Batch Loss = 1.3644\n",
            "Iteration 5, Batch 97/100: Batch Loss = 1.2460\n",
            "Iteration 5, Batch 98/100: Batch Loss = 1.3818\n",
            "Iteration 5, Batch 99/100: Batch Loss = 1.3177\n",
            "Iteration 5, Batch 100/100: Batch Loss = 1.2536\n",
            "Iteration 5: Train Loss = 1.2918, Val Loss = 1.2883\n",
            "Iteration 6, Batch 1/100: Batch Loss = 1.2822\n",
            "Iteration 6, Batch 2/100: Batch Loss = 1.2550\n",
            "Iteration 6, Batch 3/100: Batch Loss = 1.2875\n",
            "Iteration 6, Batch 4/100: Batch Loss = 1.2420\n",
            "Iteration 6, Batch 5/100: Batch Loss = 1.3655\n",
            "Iteration 6, Batch 6/100: Batch Loss = 1.3053\n",
            "Iteration 6, Batch 7/100: Batch Loss = 1.4197\n",
            "Iteration 6, Batch 8/100: Batch Loss = 1.4361\n",
            "Iteration 6, Batch 9/100: Batch Loss = 1.3110\n",
            "Iteration 6, Batch 10/100: Batch Loss = 1.2328\n",
            "Iteration 6, Batch 11/100: Batch Loss = 1.2801\n",
            "Iteration 6, Batch 12/100: Batch Loss = 1.3837\n",
            "Iteration 6, Batch 13/100: Batch Loss = 1.1550\n",
            "Iteration 6, Batch 14/100: Batch Loss = 1.4072\n",
            "Iteration 6, Batch 15/100: Batch Loss = 1.3888\n",
            "Iteration 6, Batch 16/100: Batch Loss = 1.1865\n",
            "Iteration 6, Batch 17/100: Batch Loss = 1.3652\n",
            "Iteration 6, Batch 18/100: Batch Loss = 1.2864\n",
            "Iteration 6, Batch 19/100: Batch Loss = 1.2722\n",
            "Iteration 6, Batch 20/100: Batch Loss = 1.2906\n",
            "Iteration 6, Batch 21/100: Batch Loss = 1.2630\n",
            "Iteration 6, Batch 22/100: Batch Loss = 1.2059\n",
            "Iteration 6, Batch 23/100: Batch Loss = 1.2702\n",
            "Iteration 6, Batch 24/100: Batch Loss = 1.2912\n",
            "Iteration 6, Batch 25/100: Batch Loss = 1.2999\n",
            "Iteration 6, Batch 26/100: Batch Loss = 1.3675\n",
            "Iteration 6, Batch 27/100: Batch Loss = 1.3581\n",
            "Iteration 6, Batch 28/100: Batch Loss = 1.2941\n",
            "Iteration 6, Batch 29/100: Batch Loss = 1.3139\n",
            "Iteration 6, Batch 30/100: Batch Loss = 1.1582\n",
            "Iteration 6, Batch 31/100: Batch Loss = 1.2427\n",
            "Iteration 6, Batch 32/100: Batch Loss = 1.2786\n",
            "Iteration 6, Batch 33/100: Batch Loss = 1.4146\n",
            "Iteration 6, Batch 34/100: Batch Loss = 1.2888\n",
            "Iteration 6, Batch 35/100: Batch Loss = 1.3501\n",
            "Iteration 6, Batch 36/100: Batch Loss = 1.1633\n",
            "Iteration 6, Batch 37/100: Batch Loss = 1.2878\n",
            "Iteration 6, Batch 38/100: Batch Loss = 1.2474\n",
            "Iteration 6, Batch 39/100: Batch Loss = 1.2905\n",
            "Iteration 6, Batch 40/100: Batch Loss = 1.3386\n",
            "Iteration 6, Batch 41/100: Batch Loss = 1.3696\n",
            "Iteration 6, Batch 42/100: Batch Loss = 1.2718\n",
            "Iteration 6, Batch 43/100: Batch Loss = 1.2598\n",
            "Iteration 6, Batch 44/100: Batch Loss = 1.2855\n",
            "Iteration 6, Batch 45/100: Batch Loss = 1.3119\n",
            "Iteration 6, Batch 46/100: Batch Loss = 1.3410\n",
            "Iteration 6, Batch 47/100: Batch Loss = 1.2576\n",
            "Iteration 6, Batch 48/100: Batch Loss = 1.1998\n",
            "Iteration 6, Batch 49/100: Batch Loss = 1.1567\n",
            "Iteration 6, Batch 50/100: Batch Loss = 1.2092\n",
            "Iteration 6, Batch 51/100: Batch Loss = 1.2846\n",
            "Iteration 6, Batch 52/100: Batch Loss = 1.2891\n",
            "Iteration 6, Batch 53/100: Batch Loss = 1.3478\n",
            "Iteration 6, Batch 54/100: Batch Loss = 1.2765\n",
            "Iteration 6, Batch 55/100: Batch Loss = 1.2858\n",
            "Iteration 6, Batch 56/100: Batch Loss = 1.2574\n",
            "Iteration 6, Batch 57/100: Batch Loss = 1.3670\n",
            "Iteration 6, Batch 58/100: Batch Loss = 1.3235\n",
            "Iteration 6, Batch 59/100: Batch Loss = 1.2344\n",
            "Iteration 6, Batch 60/100: Batch Loss = 1.3029\n",
            "Iteration 6, Batch 61/100: Batch Loss = 1.1827\n",
            "Iteration 6, Batch 62/100: Batch Loss = 1.3074\n",
            "Iteration 6, Batch 63/100: Batch Loss = 1.2903\n",
            "Iteration 6, Batch 64/100: Batch Loss = 1.3731\n",
            "Iteration 6, Batch 65/100: Batch Loss = 1.3530\n",
            "Iteration 6, Batch 66/100: Batch Loss = 1.3082\n",
            "Iteration 6, Batch 67/100: Batch Loss = 1.2002\n",
            "Iteration 6, Batch 68/100: Batch Loss = 1.2662\n",
            "Iteration 6, Batch 69/100: Batch Loss = 1.3240\n",
            "Iteration 6, Batch 70/100: Batch Loss = 1.3209\n",
            "Iteration 6, Batch 71/100: Batch Loss = 1.1897\n",
            "Iteration 6, Batch 72/100: Batch Loss = 1.3196\n",
            "Iteration 6, Batch 73/100: Batch Loss = 1.1867\n",
            "Iteration 6, Batch 74/100: Batch Loss = 1.2542\n",
            "Iteration 6, Batch 75/100: Batch Loss = 1.3300\n",
            "Iteration 6, Batch 76/100: Batch Loss = 1.3356\n",
            "Iteration 6, Batch 77/100: Batch Loss = 1.3896\n",
            "Iteration 6, Batch 78/100: Batch Loss = 1.2305\n",
            "Iteration 6, Batch 79/100: Batch Loss = 1.3364\n",
            "Iteration 6, Batch 80/100: Batch Loss = 1.2985\n",
            "Iteration 6, Batch 81/100: Batch Loss = 1.3185\n",
            "Iteration 6, Batch 82/100: Batch Loss = 1.2096\n",
            "Iteration 6, Batch 83/100: Batch Loss = 1.3265\n",
            "Iteration 6, Batch 84/100: Batch Loss = 1.2921\n",
            "Iteration 6, Batch 85/100: Batch Loss = 1.3024\n",
            "Iteration 6, Batch 86/100: Batch Loss = 1.3874\n",
            "Iteration 6, Batch 87/100: Batch Loss = 1.2318\n",
            "Iteration 6, Batch 88/100: Batch Loss = 1.2974\n",
            "Iteration 6, Batch 89/100: Batch Loss = 1.2340\n",
            "Iteration 6, Batch 90/100: Batch Loss = 1.2064\n",
            "Iteration 6, Batch 91/100: Batch Loss = 1.2014\n",
            "Iteration 6, Batch 92/100: Batch Loss = 1.1548\n",
            "Iteration 6, Batch 93/100: Batch Loss = 1.2941\n",
            "Iteration 6, Batch 94/100: Batch Loss = 1.4022\n",
            "Iteration 6, Batch 95/100: Batch Loss = 1.1794\n",
            "Iteration 6, Batch 96/100: Batch Loss = 1.2555\n",
            "Iteration 6, Batch 97/100: Batch Loss = 1.3275\n",
            "Iteration 6, Batch 98/100: Batch Loss = 1.3210\n",
            "Iteration 6, Batch 99/100: Batch Loss = 1.3336\n",
            "Iteration 6, Batch 100/100: Batch Loss = 1.2869\n",
            "Iteration 6: Train Loss = 1.2887, Val Loss = 1.2856\n",
            "Iteration 7, Batch 1/100: Batch Loss = 1.1925\n",
            "Iteration 7, Batch 2/100: Batch Loss = 1.2634\n",
            "Iteration 7, Batch 3/100: Batch Loss = 1.2944\n",
            "Iteration 7, Batch 4/100: Batch Loss = 1.2734\n",
            "Iteration 7, Batch 5/100: Batch Loss = 1.3393\n",
            "Iteration 7, Batch 6/100: Batch Loss = 1.3517\n",
            "Iteration 7, Batch 7/100: Batch Loss = 1.2717\n",
            "Iteration 7, Batch 8/100: Batch Loss = 1.2715\n",
            "Iteration 7, Batch 9/100: Batch Loss = 1.3977\n",
            "Iteration 7, Batch 10/100: Batch Loss = 1.3298\n",
            "Iteration 7, Batch 11/100: Batch Loss = 1.2642\n",
            "Iteration 7, Batch 12/100: Batch Loss = 1.3241\n",
            "Iteration 7, Batch 13/100: Batch Loss = 1.2333\n",
            "Iteration 7, Batch 14/100: Batch Loss = 1.3302\n",
            "Iteration 7, Batch 15/100: Batch Loss = 1.2925\n",
            "Iteration 7, Batch 16/100: Batch Loss = 1.2680\n",
            "Iteration 7, Batch 17/100: Batch Loss = 1.3145\n",
            "Iteration 7, Batch 18/100: Batch Loss = 1.2769\n",
            "Iteration 7, Batch 19/100: Batch Loss = 1.3549\n",
            "Iteration 7, Batch 20/100: Batch Loss = 1.2572\n",
            "Iteration 7, Batch 21/100: Batch Loss = 1.2471\n",
            "Iteration 7, Batch 22/100: Batch Loss = 1.2183\n",
            "Iteration 7, Batch 23/100: Batch Loss = 1.2776\n",
            "Iteration 7, Batch 24/100: Batch Loss = 1.3690\n",
            "Iteration 7, Batch 25/100: Batch Loss = 1.3422\n",
            "Iteration 7, Batch 26/100: Batch Loss = 1.2151\n",
            "Iteration 7, Batch 27/100: Batch Loss = 1.2386\n",
            "Iteration 7, Batch 28/100: Batch Loss = 1.4045\n",
            "Iteration 7, Batch 29/100: Batch Loss = 1.2864\n",
            "Iteration 7, Batch 30/100: Batch Loss = 1.3829\n",
            "Iteration 7, Batch 31/100: Batch Loss = 1.2187\n",
            "Iteration 7, Batch 32/100: Batch Loss = 1.4487\n",
            "Iteration 7, Batch 33/100: Batch Loss = 1.2437\n",
            "Iteration 7, Batch 34/100: Batch Loss = 1.2383\n",
            "Iteration 7, Batch 35/100: Batch Loss = 1.2626\n",
            "Iteration 7, Batch 36/100: Batch Loss = 1.3284\n",
            "Iteration 7, Batch 37/100: Batch Loss = 1.2271\n",
            "Iteration 7, Batch 38/100: Batch Loss = 1.2997\n",
            "Iteration 7, Batch 39/100: Batch Loss = 1.2620\n",
            "Iteration 7, Batch 40/100: Batch Loss = 1.3036\n",
            "Iteration 7, Batch 41/100: Batch Loss = 1.2380\n",
            "Iteration 7, Batch 42/100: Batch Loss = 1.3232\n",
            "Iteration 7, Batch 43/100: Batch Loss = 1.3628\n",
            "Iteration 7, Batch 44/100: Batch Loss = 1.2023\n",
            "Iteration 7, Batch 45/100: Batch Loss = 1.2232\n",
            "Iteration 7, Batch 46/100: Batch Loss = 1.3017\n",
            "Iteration 7, Batch 47/100: Batch Loss = 1.2462\n",
            "Iteration 7, Batch 48/100: Batch Loss = 1.1727\n",
            "Iteration 7, Batch 49/100: Batch Loss = 1.2417\n",
            "Iteration 7, Batch 50/100: Batch Loss = 1.1790\n",
            "Iteration 7, Batch 51/100: Batch Loss = 1.1786\n",
            "Iteration 7, Batch 52/100: Batch Loss = 1.3151\n",
            "Iteration 7, Batch 53/100: Batch Loss = 1.2713\n",
            "Iteration 7, Batch 54/100: Batch Loss = 1.3230\n",
            "Iteration 7, Batch 55/100: Batch Loss = 1.3566\n",
            "Iteration 7, Batch 56/100: Batch Loss = 1.3515\n",
            "Iteration 7, Batch 57/100: Batch Loss = 1.2417\n",
            "Iteration 7, Batch 58/100: Batch Loss = 1.2184\n",
            "Iteration 7, Batch 59/100: Batch Loss = 1.2642\n",
            "Iteration 7, Batch 60/100: Batch Loss = 1.2720\n",
            "Iteration 7, Batch 61/100: Batch Loss = 1.3059\n",
            "Iteration 7, Batch 62/100: Batch Loss = 1.3132\n",
            "Iteration 7, Batch 63/100: Batch Loss = 1.2184\n",
            "Iteration 7, Batch 64/100: Batch Loss = 1.2711\n",
            "Iteration 7, Batch 65/100: Batch Loss = 1.2595\n",
            "Iteration 7, Batch 66/100: Batch Loss = 1.2299\n",
            "Iteration 7, Batch 67/100: Batch Loss = 1.2758\n",
            "Iteration 7, Batch 68/100: Batch Loss = 1.2521\n",
            "Iteration 7, Batch 69/100: Batch Loss = 1.2131\n",
            "Iteration 7, Batch 70/100: Batch Loss = 1.3313\n",
            "Iteration 7, Batch 71/100: Batch Loss = 1.2927\n",
            "Iteration 7, Batch 72/100: Batch Loss = 1.3295\n",
            "Iteration 7, Batch 73/100: Batch Loss = 1.3400\n",
            "Iteration 7, Batch 74/100: Batch Loss = 1.2471\n",
            "Iteration 7, Batch 75/100: Batch Loss = 1.3763\n",
            "Iteration 7, Batch 76/100: Batch Loss = 1.3086\n",
            "Iteration 7, Batch 77/100: Batch Loss = 1.2674\n",
            "Iteration 7, Batch 78/100: Batch Loss = 1.3446\n",
            "Iteration 7, Batch 79/100: Batch Loss = 1.2377\n",
            "Iteration 7, Batch 80/100: Batch Loss = 1.2979\n",
            "Iteration 7, Batch 81/100: Batch Loss = 1.3535\n",
            "Iteration 7, Batch 82/100: Batch Loss = 1.2389\n",
            "Iteration 7, Batch 83/100: Batch Loss = 1.2507\n",
            "Iteration 7, Batch 84/100: Batch Loss = 1.3737\n",
            "Iteration 7, Batch 85/100: Batch Loss = 1.2943\n",
            "Iteration 7, Batch 86/100: Batch Loss = 1.2996\n",
            "Iteration 7, Batch 87/100: Batch Loss = 1.2131\n",
            "Iteration 7, Batch 88/100: Batch Loss = 1.3334\n",
            "Iteration 7, Batch 89/100: Batch Loss = 1.3615\n",
            "Iteration 7, Batch 90/100: Batch Loss = 1.2447\n",
            "Iteration 7, Batch 91/100: Batch Loss = 1.4587\n",
            "Iteration 7, Batch 92/100: Batch Loss = 1.2776\n",
            "Iteration 7, Batch 93/100: Batch Loss = 1.2375\n",
            "Iteration 7, Batch 94/100: Batch Loss = 1.2883\n",
            "Iteration 7, Batch 95/100: Batch Loss = 1.3789\n",
            "Iteration 7, Batch 96/100: Batch Loss = 1.2672\n",
            "Iteration 7, Batch 97/100: Batch Loss = 1.2003\n",
            "Iteration 7, Batch 98/100: Batch Loss = 1.2739\n",
            "Iteration 7, Batch 99/100: Batch Loss = 1.3037\n",
            "Iteration 7, Batch 100/100: Batch Loss = 1.2965\n",
            "Iteration 7: Train Loss = 1.2866, Val Loss = 1.2838\n",
            "Iteration 8, Batch 1/100: Batch Loss = 1.2817\n",
            "Iteration 8, Batch 2/100: Batch Loss = 1.2052\n",
            "Iteration 8, Batch 3/100: Batch Loss = 1.3975\n",
            "Iteration 8, Batch 4/100: Batch Loss = 1.2216\n",
            "Iteration 8, Batch 5/100: Batch Loss = 1.2425\n",
            "Iteration 8, Batch 6/100: Batch Loss = 1.2790\n",
            "Iteration 8, Batch 7/100: Batch Loss = 1.2780\n",
            "Iteration 8, Batch 8/100: Batch Loss = 1.1698\n",
            "Iteration 8, Batch 9/100: Batch Loss = 1.3034\n",
            "Iteration 8, Batch 10/100: Batch Loss = 1.2922\n",
            "Iteration 8, Batch 11/100: Batch Loss = 1.3121\n",
            "Iteration 8, Batch 12/100: Batch Loss = 1.2402\n",
            "Iteration 8, Batch 13/100: Batch Loss = 1.3706\n",
            "Iteration 8, Batch 14/100: Batch Loss = 1.2816\n",
            "Iteration 8, Batch 15/100: Batch Loss = 1.2812\n",
            "Iteration 8, Batch 16/100: Batch Loss = 1.2899\n",
            "Iteration 8, Batch 17/100: Batch Loss = 1.3257\n",
            "Iteration 8, Batch 18/100: Batch Loss = 1.3297\n",
            "Iteration 8, Batch 19/100: Batch Loss = 1.3198\n",
            "Iteration 8, Batch 20/100: Batch Loss = 1.2731\n",
            "Iteration 8, Batch 21/100: Batch Loss = 1.2281\n",
            "Iteration 8, Batch 22/100: Batch Loss = 1.2585\n",
            "Iteration 8, Batch 23/100: Batch Loss = 1.3052\n",
            "Iteration 8, Batch 24/100: Batch Loss = 1.2495\n",
            "Iteration 8, Batch 25/100: Batch Loss = 1.3133\n",
            "Iteration 8, Batch 26/100: Batch Loss = 1.2798\n",
            "Iteration 8, Batch 27/100: Batch Loss = 1.3586\n",
            "Iteration 8, Batch 28/100: Batch Loss = 1.3604\n",
            "Iteration 8, Batch 29/100: Batch Loss = 1.3168\n",
            "Iteration 8, Batch 30/100: Batch Loss = 1.3153\n",
            "Iteration 8, Batch 31/100: Batch Loss = 1.2271\n",
            "Iteration 8, Batch 32/100: Batch Loss = 1.3929\n",
            "Iteration 8, Batch 33/100: Batch Loss = 1.2525\n",
            "Iteration 8, Batch 34/100: Batch Loss = 1.2496\n",
            "Iteration 8, Batch 35/100: Batch Loss = 1.1701\n",
            "Iteration 8, Batch 36/100: Batch Loss = 1.2585\n",
            "Iteration 8, Batch 37/100: Batch Loss = 1.2364\n",
            "Iteration 8, Batch 38/100: Batch Loss = 1.3293\n",
            "Iteration 8, Batch 39/100: Batch Loss = 1.3499\n",
            "Iteration 8, Batch 40/100: Batch Loss = 1.2974\n",
            "Iteration 8, Batch 41/100: Batch Loss = 1.2499\n",
            "Iteration 8, Batch 42/100: Batch Loss = 1.3319\n",
            "Iteration 8, Batch 43/100: Batch Loss = 1.3694\n",
            "Iteration 8, Batch 44/100: Batch Loss = 1.3131\n",
            "Iteration 8, Batch 45/100: Batch Loss = 1.2376\n",
            "Iteration 8, Batch 46/100: Batch Loss = 1.3018\n",
            "Iteration 8, Batch 47/100: Batch Loss = 1.2488\n",
            "Iteration 8, Batch 48/100: Batch Loss = 1.2318\n",
            "Iteration 8, Batch 49/100: Batch Loss = 1.2713\n",
            "Iteration 8, Batch 50/100: Batch Loss = 1.3918\n",
            "Iteration 8, Batch 51/100: Batch Loss = 1.1922\n",
            "Iteration 8, Batch 52/100: Batch Loss = 1.3784\n",
            "Iteration 8, Batch 53/100: Batch Loss = 1.3387\n",
            "Iteration 8, Batch 54/100: Batch Loss = 1.2378\n",
            "Iteration 8, Batch 55/100: Batch Loss = 1.1959\n",
            "Iteration 8, Batch 56/100: Batch Loss = 1.2392\n",
            "Iteration 8, Batch 57/100: Batch Loss = 1.2502\n",
            "Iteration 8, Batch 58/100: Batch Loss = 1.2786\n",
            "Iteration 8, Batch 59/100: Batch Loss = 1.3937\n",
            "Iteration 8, Batch 60/100: Batch Loss = 1.2489\n",
            "Iteration 8, Batch 61/100: Batch Loss = 1.1406\n",
            "Iteration 8, Batch 62/100: Batch Loss = 1.2006\n",
            "Iteration 8, Batch 63/100: Batch Loss = 1.3825\n",
            "Iteration 8, Batch 64/100: Batch Loss = 1.2310\n",
            "Iteration 8, Batch 65/100: Batch Loss = 1.2210\n",
            "Iteration 8, Batch 66/100: Batch Loss = 1.3129\n",
            "Iteration 8, Batch 67/100: Batch Loss = 1.3106\n",
            "Iteration 8, Batch 68/100: Batch Loss = 1.2494\n",
            "Iteration 8, Batch 69/100: Batch Loss = 1.2173\n",
            "Iteration 8, Batch 70/100: Batch Loss = 1.3038\n",
            "Iteration 8, Batch 71/100: Batch Loss = 1.3317\n",
            "Iteration 8, Batch 72/100: Batch Loss = 1.2841\n",
            "Iteration 8, Batch 73/100: Batch Loss = 1.3427\n",
            "Iteration 8, Batch 74/100: Batch Loss = 1.2329\n",
            "Iteration 8, Batch 75/100: Batch Loss = 1.3979\n",
            "Iteration 8, Batch 76/100: Batch Loss = 1.2833\n",
            "Iteration 8, Batch 77/100: Batch Loss = 1.2749\n",
            "Iteration 8, Batch 78/100: Batch Loss = 1.2974\n",
            "Iteration 8, Batch 79/100: Batch Loss = 1.3448\n",
            "Iteration 8, Batch 80/100: Batch Loss = 1.2670\n",
            "Iteration 8, Batch 81/100: Batch Loss = 1.3292\n",
            "Iteration 8, Batch 82/100: Batch Loss = 1.2700\n",
            "Iteration 8, Batch 83/100: Batch Loss = 1.2602\n",
            "Iteration 8, Batch 84/100: Batch Loss = 1.2312\n",
            "Iteration 8, Batch 85/100: Batch Loss = 1.3124\n",
            "Iteration 8, Batch 86/100: Batch Loss = 1.3146\n",
            "Iteration 8, Batch 87/100: Batch Loss = 1.3436\n",
            "Iteration 8, Batch 88/100: Batch Loss = 1.2133\n",
            "Iteration 8, Batch 89/100: Batch Loss = 1.4529\n",
            "Iteration 8, Batch 90/100: Batch Loss = 1.3869\n",
            "Iteration 8, Batch 91/100: Batch Loss = 1.2789\n",
            "Iteration 8, Batch 92/100: Batch Loss = 1.3061\n",
            "Iteration 8, Batch 93/100: Batch Loss = 1.2724\n",
            "Iteration 8, Batch 94/100: Batch Loss = 1.2156\n",
            "Iteration 8, Batch 95/100: Batch Loss = 1.2832\n",
            "Iteration 8, Batch 96/100: Batch Loss = 1.3144\n",
            "Iteration 8, Batch 97/100: Batch Loss = 1.2297\n",
            "Iteration 8, Batch 98/100: Batch Loss = 1.2262\n",
            "Iteration 8, Batch 99/100: Batch Loss = 1.2875\n",
            "Iteration 8, Batch 100/100: Batch Loss = 1.2132\n",
            "Iteration 8: Train Loss = 1.2851, Val Loss = 1.2825\n",
            "Iteration 9, Batch 1/100: Batch Loss = 1.2185\n",
            "Iteration 9, Batch 2/100: Batch Loss = 1.4118\n",
            "Iteration 9, Batch 3/100: Batch Loss = 1.2285\n",
            "Iteration 9, Batch 4/100: Batch Loss = 1.3694\n",
            "Iteration 9, Batch 5/100: Batch Loss = 1.3066\n",
            "Iteration 9, Batch 6/100: Batch Loss = 1.3066\n",
            "Iteration 9, Batch 7/100: Batch Loss = 1.3060\n",
            "Iteration 9, Batch 8/100: Batch Loss = 1.2721\n",
            "Iteration 9, Batch 9/100: Batch Loss = 1.3383\n",
            "Iteration 9, Batch 10/100: Batch Loss = 1.2495\n",
            "Iteration 9, Batch 11/100: Batch Loss = 1.2884\n",
            "Iteration 9, Batch 12/100: Batch Loss = 1.1938\n",
            "Iteration 9, Batch 13/100: Batch Loss = 1.2028\n",
            "Iteration 9, Batch 14/100: Batch Loss = 1.2763\n",
            "Iteration 9, Batch 15/100: Batch Loss = 1.4485\n",
            "Iteration 9, Batch 16/100: Batch Loss = 1.2524\n",
            "Iteration 9, Batch 17/100: Batch Loss = 1.2043\n",
            "Iteration 9, Batch 18/100: Batch Loss = 1.2964\n",
            "Iteration 9, Batch 19/100: Batch Loss = 1.2839\n",
            "Iteration 9, Batch 20/100: Batch Loss = 1.2702\n",
            "Iteration 9, Batch 21/100: Batch Loss = 1.2978\n",
            "Iteration 9, Batch 22/100: Batch Loss = 1.3097\n",
            "Iteration 9, Batch 23/100: Batch Loss = 1.2326\n",
            "Iteration 9, Batch 24/100: Batch Loss = 1.2658\n",
            "Iteration 9, Batch 25/100: Batch Loss = 1.3577\n",
            "Iteration 9, Batch 26/100: Batch Loss = 1.3092\n",
            "Iteration 9, Batch 27/100: Batch Loss = 1.3210\n",
            "Iteration 9, Batch 28/100: Batch Loss = 1.2671\n",
            "Iteration 9, Batch 29/100: Batch Loss = 1.2602\n",
            "Iteration 9, Batch 30/100: Batch Loss = 1.2029\n",
            "Iteration 9, Batch 31/100: Batch Loss = 1.2839\n",
            "Iteration 9, Batch 32/100: Batch Loss = 1.2182\n",
            "Iteration 9, Batch 33/100: Batch Loss = 1.3507\n",
            "Iteration 9, Batch 34/100: Batch Loss = 1.3347\n",
            "Iteration 9, Batch 35/100: Batch Loss = 1.2691\n",
            "Iteration 9, Batch 36/100: Batch Loss = 1.4164\n",
            "Iteration 9, Batch 37/100: Batch Loss = 1.1816\n",
            "Iteration 9, Batch 38/100: Batch Loss = 1.2418\n",
            "Iteration 9, Batch 39/100: Batch Loss = 1.3294\n",
            "Iteration 9, Batch 40/100: Batch Loss = 1.2925\n",
            "Iteration 9, Batch 41/100: Batch Loss = 1.2385\n",
            "Iteration 9, Batch 42/100: Batch Loss = 1.2309\n",
            "Iteration 9, Batch 43/100: Batch Loss = 1.2988\n",
            "Iteration 9, Batch 44/100: Batch Loss = 1.1807\n",
            "Iteration 9, Batch 45/100: Batch Loss = 1.1892\n",
            "Iteration 9, Batch 46/100: Batch Loss = 1.4521\n",
            "Iteration 9, Batch 47/100: Batch Loss = 1.3048\n",
            "Iteration 9, Batch 48/100: Batch Loss = 1.3208\n",
            "Iteration 9, Batch 49/100: Batch Loss = 1.2923\n",
            "Iteration 9, Batch 50/100: Batch Loss = 1.2837\n",
            "Iteration 9, Batch 51/100: Batch Loss = 1.3449\n",
            "Iteration 9, Batch 52/100: Batch Loss = 1.2364\n",
            "Iteration 9, Batch 53/100: Batch Loss = 1.3463\n",
            "Iteration 9, Batch 54/100: Batch Loss = 1.3011\n",
            "Iteration 9, Batch 55/100: Batch Loss = 1.3262\n",
            "Iteration 9, Batch 56/100: Batch Loss = 1.3273\n",
            "Iteration 9, Batch 57/100: Batch Loss = 1.3586\n",
            "Iteration 9, Batch 58/100: Batch Loss = 1.2645\n",
            "Iteration 9, Batch 59/100: Batch Loss = 1.1840\n",
            "Iteration 9, Batch 60/100: Batch Loss = 1.3042\n",
            "Iteration 9, Batch 61/100: Batch Loss = 1.2970\n",
            "Iteration 9, Batch 62/100: Batch Loss = 1.1585\n",
            "Iteration 9, Batch 63/100: Batch Loss = 1.2551\n",
            "Iteration 9, Batch 64/100: Batch Loss = 1.2572\n",
            "Iteration 9, Batch 65/100: Batch Loss = 1.3256\n",
            "Iteration 9, Batch 66/100: Batch Loss = 1.2360\n",
            "Iteration 9, Batch 67/100: Batch Loss = 1.2037\n",
            "Iteration 9, Batch 68/100: Batch Loss = 1.1947\n",
            "Iteration 9, Batch 69/100: Batch Loss = 1.3781\n",
            "Iteration 9, Batch 70/100: Batch Loss = 1.2055\n",
            "Iteration 9, Batch 71/100: Batch Loss = 1.3025\n",
            "Iteration 9, Batch 72/100: Batch Loss = 1.3185\n",
            "Iteration 9, Batch 73/100: Batch Loss = 1.3476\n",
            "Iteration 9, Batch 74/100: Batch Loss = 1.1792\n",
            "Iteration 9, Batch 75/100: Batch Loss = 1.2483\n",
            "Iteration 9, Batch 76/100: Batch Loss = 1.2573\n",
            "Iteration 9, Batch 77/100: Batch Loss = 1.3327\n",
            "Iteration 9, Batch 78/100: Batch Loss = 1.2878\n",
            "Iteration 9, Batch 79/100: Batch Loss = 1.3394\n",
            "Iteration 9, Batch 80/100: Batch Loss = 1.2991\n",
            "Iteration 9, Batch 81/100: Batch Loss = 1.1297\n",
            "Iteration 9, Batch 82/100: Batch Loss = 1.3956\n",
            "Iteration 9, Batch 83/100: Batch Loss = 1.2807\n",
            "Iteration 9, Batch 84/100: Batch Loss = 1.2023\n",
            "Iteration 9, Batch 85/100: Batch Loss = 1.3897\n",
            "Iteration 9, Batch 86/100: Batch Loss = 1.2423\n",
            "Iteration 9, Batch 87/100: Batch Loss = 1.3608\n",
            "Iteration 9, Batch 88/100: Batch Loss = 1.3568\n",
            "Iteration 9, Batch 89/100: Batch Loss = 1.2502\n",
            "Iteration 9, Batch 90/100: Batch Loss = 1.1903\n",
            "Iteration 9, Batch 91/100: Batch Loss = 1.3177\n",
            "Iteration 9, Batch 92/100: Batch Loss = 1.3306\n",
            "Iteration 9, Batch 93/100: Batch Loss = 1.3504\n",
            "Iteration 9, Batch 94/100: Batch Loss = 1.3258\n",
            "Iteration 9, Batch 95/100: Batch Loss = 1.2990\n",
            "Iteration 9, Batch 96/100: Batch Loss = 1.2049\n",
            "Iteration 9, Batch 97/100: Batch Loss = 1.2573\n",
            "Iteration 9, Batch 98/100: Batch Loss = 1.3043\n",
            "Iteration 9, Batch 99/100: Batch Loss = 1.2384\n",
            "Iteration 9, Batch 100/100: Batch Loss = 1.3068\n",
            "Iteration 9: Train Loss = 1.2841, Val Loss = 1.2816\n",
            "Iteration 10, Batch 1/100: Batch Loss = 1.2566\n",
            "Iteration 10, Batch 2/100: Batch Loss = 1.2018\n",
            "Iteration 10, Batch 3/100: Batch Loss = 1.3066\n",
            "Iteration 10, Batch 4/100: Batch Loss = 1.2635\n",
            "Iteration 10, Batch 5/100: Batch Loss = 1.4090\n",
            "Iteration 10, Batch 6/100: Batch Loss = 1.4003\n",
            "Iteration 10, Batch 7/100: Batch Loss = 1.3615\n",
            "Iteration 10, Batch 8/100: Batch Loss = 1.2920\n",
            "Iteration 10, Batch 9/100: Batch Loss = 1.2166\n",
            "Iteration 10, Batch 10/100: Batch Loss = 1.2542\n",
            "Iteration 10, Batch 11/100: Batch Loss = 1.3204\n",
            "Iteration 10, Batch 12/100: Batch Loss = 1.3950\n",
            "Iteration 10, Batch 13/100: Batch Loss = 1.3747\n",
            "Iteration 10, Batch 14/100: Batch Loss = 1.1505\n",
            "Iteration 10, Batch 15/100: Batch Loss = 1.2207\n",
            "Iteration 10, Batch 16/100: Batch Loss = 1.2801\n",
            "Iteration 10, Batch 17/100: Batch Loss = 1.4154\n",
            "Iteration 10, Batch 18/100: Batch Loss = 1.3329\n",
            "Iteration 10, Batch 19/100: Batch Loss = 1.2231\n",
            "Iteration 10, Batch 20/100: Batch Loss = 1.1886\n",
            "Iteration 10, Batch 21/100: Batch Loss = 1.3283\n",
            "Iteration 10, Batch 22/100: Batch Loss = 1.2628\n",
            "Iteration 10, Batch 23/100: Batch Loss = 1.2626\n",
            "Iteration 10, Batch 24/100: Batch Loss = 1.3035\n",
            "Iteration 10, Batch 25/100: Batch Loss = 1.3162\n",
            "Iteration 10, Batch 26/100: Batch Loss = 1.2320\n",
            "Iteration 10, Batch 27/100: Batch Loss = 1.3841\n",
            "Iteration 10, Batch 28/100: Batch Loss = 1.2706\n",
            "Iteration 10, Batch 29/100: Batch Loss = 1.3162\n",
            "Iteration 10, Batch 30/100: Batch Loss = 1.2864\n",
            "Iteration 10, Batch 31/100: Batch Loss = 1.3154\n",
            "Iteration 10, Batch 32/100: Batch Loss = 1.2706\n",
            "Iteration 10, Batch 33/100: Batch Loss = 1.2129\n",
            "Iteration 10, Batch 34/100: Batch Loss = 1.2828\n",
            "Iteration 10, Batch 35/100: Batch Loss = 1.2336\n",
            "Iteration 10, Batch 36/100: Batch Loss = 1.2282\n",
            "Iteration 10, Batch 37/100: Batch Loss = 1.2133\n",
            "Iteration 10, Batch 38/100: Batch Loss = 1.3142\n",
            "Iteration 10, Batch 39/100: Batch Loss = 1.4027\n",
            "Iteration 10, Batch 40/100: Batch Loss = 1.2918\n",
            "Iteration 10, Batch 41/100: Batch Loss = 1.1739\n",
            "Iteration 10, Batch 42/100: Batch Loss = 1.2589\n",
            "Iteration 10, Batch 43/100: Batch Loss = 1.2948\n",
            "Iteration 10, Batch 44/100: Batch Loss = 1.2948\n",
            "Iteration 10, Batch 45/100: Batch Loss = 1.1770\n",
            "Iteration 10, Batch 46/100: Batch Loss = 1.3676\n",
            "Iteration 10, Batch 47/100: Batch Loss = 1.2759\n",
            "Iteration 10, Batch 48/100: Batch Loss = 1.1642\n",
            "Iteration 10, Batch 49/100: Batch Loss = 1.2421\n",
            "Iteration 10, Batch 50/100: Batch Loss = 1.2680\n",
            "Iteration 10, Batch 51/100: Batch Loss = 1.3310\n",
            "Iteration 10, Batch 52/100: Batch Loss = 1.4553\n",
            "Iteration 10, Batch 53/100: Batch Loss = 1.1550\n",
            "Iteration 10, Batch 54/100: Batch Loss = 1.2952\n",
            "Iteration 10, Batch 55/100: Batch Loss = 1.2832\n",
            "Iteration 10, Batch 56/100: Batch Loss = 1.2880\n",
            "Iteration 10, Batch 57/100: Batch Loss = 1.4689\n",
            "Iteration 10, Batch 58/100: Batch Loss = 1.3027\n",
            "Iteration 10, Batch 59/100: Batch Loss = 1.2557\n",
            "Iteration 10, Batch 60/100: Batch Loss = 1.2257\n",
            "Iteration 10, Batch 61/100: Batch Loss = 1.2330\n",
            "Iteration 10, Batch 62/100: Batch Loss = 1.2518\n",
            "Iteration 10, Batch 63/100: Batch Loss = 1.4312\n",
            "Iteration 10, Batch 64/100: Batch Loss = 1.2648\n",
            "Iteration 10, Batch 65/100: Batch Loss = 1.3242\n",
            "Iteration 10, Batch 66/100: Batch Loss = 1.3193\n",
            "Iteration 10, Batch 67/100: Batch Loss = 1.2677\n",
            "Iteration 10, Batch 68/100: Batch Loss = 1.1330\n",
            "Iteration 10, Batch 69/100: Batch Loss = 1.3280\n",
            "Iteration 10, Batch 70/100: Batch Loss = 1.3630\n",
            "Iteration 10, Batch 71/100: Batch Loss = 1.3312\n",
            "Iteration 10, Batch 72/100: Batch Loss = 1.3803\n",
            "Iteration 10, Batch 73/100: Batch Loss = 1.1820\n",
            "Iteration 10, Batch 74/100: Batch Loss = 1.3011\n",
            "Iteration 10, Batch 75/100: Batch Loss = 1.3213\n",
            "Iteration 10, Batch 76/100: Batch Loss = 1.2581\n",
            "Iteration 10, Batch 77/100: Batch Loss = 1.2569\n",
            "Iteration 10, Batch 78/100: Batch Loss = 1.2487\n",
            "Iteration 10, Batch 79/100: Batch Loss = 1.1785\n",
            "Iteration 10, Batch 80/100: Batch Loss = 1.3193\n",
            "Iteration 10, Batch 81/100: Batch Loss = 1.3548\n",
            "Iteration 10, Batch 82/100: Batch Loss = 1.4154\n",
            "Iteration 10, Batch 83/100: Batch Loss = 1.1462\n",
            "Iteration 10, Batch 84/100: Batch Loss = 1.3620\n",
            "Iteration 10, Batch 85/100: Batch Loss = 1.2030\n",
            "Iteration 10, Batch 86/100: Batch Loss = 1.3541\n",
            "Iteration 10, Batch 87/100: Batch Loss = 1.2879\n",
            "Iteration 10, Batch 88/100: Batch Loss = 1.1692\n",
            "Iteration 10, Batch 89/100: Batch Loss = 1.2446\n",
            "Iteration 10, Batch 90/100: Batch Loss = 1.3405\n",
            "Iteration 10, Batch 91/100: Batch Loss = 1.2247\n",
            "Iteration 10, Batch 92/100: Batch Loss = 1.2149\n",
            "Iteration 10, Batch 93/100: Batch Loss = 1.3214\n",
            "Iteration 10, Batch 94/100: Batch Loss = 1.2147\n",
            "Iteration 10, Batch 95/100: Batch Loss = 1.3107\n",
            "Iteration 10, Batch 96/100: Batch Loss = 1.3824\n",
            "Iteration 10, Batch 97/100: Batch Loss = 1.2526\n",
            "Iteration 10, Batch 98/100: Batch Loss = 1.2200\n",
            "Iteration 10, Batch 99/100: Batch Loss = 1.2602\n",
            "Iteration 10, Batch 100/100: Batch Loss = 1.1947\n",
            "Iteration 10: Train Loss = 1.2834, Val Loss = 1.2809\n",
            "Iteration 11, Batch 1/100: Batch Loss = 1.2865\n",
            "Iteration 11, Batch 2/100: Batch Loss = 1.2754\n",
            "Iteration 11, Batch 3/100: Batch Loss = 1.1965\n",
            "Iteration 11, Batch 4/100: Batch Loss = 1.2223\n",
            "Iteration 11, Batch 5/100: Batch Loss = 1.4048\n",
            "Iteration 11, Batch 6/100: Batch Loss = 1.2330\n",
            "Iteration 11, Batch 7/100: Batch Loss = 1.2426\n",
            "Iteration 11, Batch 8/100: Batch Loss = 1.2698\n",
            "Iteration 11, Batch 9/100: Batch Loss = 1.2261\n",
            "Iteration 11, Batch 10/100: Batch Loss = 1.3698\n",
            "Iteration 11, Batch 11/100: Batch Loss = 1.3025\n",
            "Iteration 11, Batch 12/100: Batch Loss = 1.2942\n",
            "Iteration 11, Batch 13/100: Batch Loss = 1.2292\n",
            "Iteration 11, Batch 14/100: Batch Loss = 1.2984\n",
            "Iteration 11, Batch 15/100: Batch Loss = 1.2859\n",
            "Iteration 11, Batch 16/100: Batch Loss = 1.2473\n",
            "Iteration 11, Batch 17/100: Batch Loss = 1.2452\n",
            "Iteration 11, Batch 18/100: Batch Loss = 1.1742\n",
            "Iteration 11, Batch 19/100: Batch Loss = 1.2165\n",
            "Iteration 11, Batch 20/100: Batch Loss = 1.2971\n",
            "Iteration 11, Batch 21/100: Batch Loss = 1.2354\n",
            "Iteration 11, Batch 22/100: Batch Loss = 1.1601\n",
            "Iteration 11, Batch 23/100: Batch Loss = 1.3108\n",
            "Iteration 11, Batch 24/100: Batch Loss = 1.2560\n",
            "Iteration 11, Batch 25/100: Batch Loss = 1.3095\n",
            "Iteration 11, Batch 26/100: Batch Loss = 1.3232\n",
            "Iteration 11, Batch 27/100: Batch Loss = 1.2009\n",
            "Iteration 11, Batch 28/100: Batch Loss = 1.2734\n",
            "Iteration 11, Batch 29/100: Batch Loss = 1.2414\n",
            "Iteration 11, Batch 30/100: Batch Loss = 1.2789\n",
            "Iteration 11, Batch 31/100: Batch Loss = 1.2032\n",
            "Iteration 11, Batch 32/100: Batch Loss = 1.2069\n",
            "Iteration 11, Batch 33/100: Batch Loss = 1.2632\n",
            "Iteration 11, Batch 34/100: Batch Loss = 1.3121\n",
            "Iteration 11, Batch 35/100: Batch Loss = 1.2853\n",
            "Iteration 11, Batch 36/100: Batch Loss = 1.2140\n",
            "Iteration 11, Batch 37/100: Batch Loss = 1.2479\n",
            "Iteration 11, Batch 38/100: Batch Loss = 1.4627\n",
            "Iteration 11, Batch 39/100: Batch Loss = 1.1474\n",
            "Iteration 11, Batch 40/100: Batch Loss = 1.2753\n",
            "Iteration 11, Batch 41/100: Batch Loss = 1.3008\n",
            "Iteration 11, Batch 42/100: Batch Loss = 1.2878\n",
            "Iteration 11, Batch 43/100: Batch Loss = 1.4102\n",
            "Iteration 11, Batch 44/100: Batch Loss = 1.2042\n",
            "Iteration 11, Batch 45/100: Batch Loss = 1.2878\n",
            "Iteration 11, Batch 46/100: Batch Loss = 1.2519\n",
            "Iteration 11, Batch 47/100: Batch Loss = 1.3787\n",
            "Iteration 11, Batch 48/100: Batch Loss = 1.3214\n",
            "Iteration 11, Batch 49/100: Batch Loss = 1.3635\n",
            "Iteration 11, Batch 50/100: Batch Loss = 1.2620\n",
            "Iteration 11, Batch 51/100: Batch Loss = 1.4429\n",
            "Iteration 11, Batch 52/100: Batch Loss = 1.2383\n",
            "Iteration 11, Batch 53/100: Batch Loss = 1.2727\n",
            "Iteration 11, Batch 54/100: Batch Loss = 1.3239\n",
            "Iteration 11, Batch 55/100: Batch Loss = 1.2262\n",
            "Iteration 11, Batch 56/100: Batch Loss = 1.3264\n",
            "Iteration 11, Batch 57/100: Batch Loss = 1.1002\n",
            "Iteration 11, Batch 58/100: Batch Loss = 1.3527\n",
            "Iteration 11, Batch 59/100: Batch Loss = 1.1334\n",
            "Iteration 11, Batch 60/100: Batch Loss = 1.2072\n",
            "Iteration 11, Batch 61/100: Batch Loss = 1.3194\n",
            "Iteration 11, Batch 62/100: Batch Loss = 1.2087\n",
            "Iteration 11, Batch 63/100: Batch Loss = 1.2263\n",
            "Iteration 11, Batch 64/100: Batch Loss = 1.3524\n",
            "Iteration 11, Batch 65/100: Batch Loss = 1.4253\n",
            "Iteration 11, Batch 66/100: Batch Loss = 1.3417\n",
            "Iteration 11, Batch 67/100: Batch Loss = 1.2915\n",
            "Iteration 11, Batch 68/100: Batch Loss = 1.3067\n",
            "Iteration 11, Batch 69/100: Batch Loss = 1.2294\n",
            "Iteration 11, Batch 70/100: Batch Loss = 1.2550\n",
            "Iteration 11, Batch 71/100: Batch Loss = 1.3885\n",
            "Iteration 11, Batch 72/100: Batch Loss = 1.3922\n",
            "Iteration 11, Batch 73/100: Batch Loss = 1.4045\n",
            "Iteration 11, Batch 74/100: Batch Loss = 1.3133\n",
            "Iteration 11, Batch 75/100: Batch Loss = 1.3014\n",
            "Iteration 11, Batch 76/100: Batch Loss = 1.2724\n",
            "Iteration 11, Batch 77/100: Batch Loss = 1.2976\n",
            "Iteration 11, Batch 78/100: Batch Loss = 1.2163\n",
            "Iteration 11, Batch 79/100: Batch Loss = 1.3191\n",
            "Iteration 11, Batch 80/100: Batch Loss = 1.3156\n",
            "Iteration 11, Batch 81/100: Batch Loss = 1.2821\n",
            "Iteration 11, Batch 82/100: Batch Loss = 1.2877\n",
            "Iteration 11, Batch 83/100: Batch Loss = 1.2996\n",
            "Iteration 11, Batch 84/100: Batch Loss = 1.1673\n",
            "Iteration 11, Batch 85/100: Batch Loss = 1.3937\n",
            "Iteration 11, Batch 86/100: Batch Loss = 1.3352\n",
            "Iteration 11, Batch 87/100: Batch Loss = 1.1263\n",
            "Iteration 11, Batch 88/100: Batch Loss = 1.3347\n",
            "Iteration 11, Batch 89/100: Batch Loss = 1.4287\n",
            "Iteration 11, Batch 90/100: Batch Loss = 1.3595\n",
            "Iteration 11, Batch 91/100: Batch Loss = 1.2565\n",
            "Iteration 11, Batch 92/100: Batch Loss = 1.3220\n",
            "Iteration 11, Batch 93/100: Batch Loss = 1.4252\n",
            "Iteration 11, Batch 94/100: Batch Loss = 1.3660\n",
            "Iteration 11, Batch 95/100: Batch Loss = 1.2914\n",
            "Iteration 11, Batch 96/100: Batch Loss = 1.3314\n",
            "Iteration 11, Batch 97/100: Batch Loss = 1.1889\n",
            "Iteration 11, Batch 98/100: Batch Loss = 1.2164\n",
            "Iteration 11, Batch 99/100: Batch Loss = 1.2380\n",
            "Iteration 11, Batch 100/100: Batch Loss = 1.2406\n",
            "Iteration 11: Train Loss = 1.2829, Val Loss = 1.2805\n",
            "Iteration 12, Batch 1/100: Batch Loss = 1.3475\n",
            "Iteration 12, Batch 2/100: Batch Loss = 1.3004\n",
            "Iteration 12, Batch 3/100: Batch Loss = 1.2975\n",
            "Iteration 12, Batch 4/100: Batch Loss = 1.2943\n",
            "Iteration 12, Batch 5/100: Batch Loss = 1.2508\n",
            "Iteration 12, Batch 6/100: Batch Loss = 1.4034\n",
            "Iteration 12, Batch 7/100: Batch Loss = 1.2600\n",
            "Iteration 12, Batch 8/100: Batch Loss = 1.2227\n",
            "Iteration 12, Batch 9/100: Batch Loss = 1.2256\n",
            "Iteration 12, Batch 10/100: Batch Loss = 1.2036\n",
            "Iteration 12, Batch 11/100: Batch Loss = 1.3066\n",
            "Iteration 12, Batch 12/100: Batch Loss = 1.2005\n",
            "Iteration 12, Batch 13/100: Batch Loss = 1.2972\n",
            "Iteration 12, Batch 14/100: Batch Loss = 1.3222\n",
            "Iteration 12, Batch 15/100: Batch Loss = 1.2160\n",
            "Iteration 12, Batch 16/100: Batch Loss = 1.2001\n",
            "Iteration 12, Batch 17/100: Batch Loss = 1.3442\n",
            "Iteration 12, Batch 18/100: Batch Loss = 1.2220\n",
            "Iteration 12, Batch 19/100: Batch Loss = 1.2600\n",
            "Iteration 12, Batch 20/100: Batch Loss = 1.2533\n",
            "Iteration 12, Batch 21/100: Batch Loss = 1.2157\n",
            "Iteration 12, Batch 22/100: Batch Loss = 1.2533\n",
            "Iteration 12, Batch 23/100: Batch Loss = 1.1971\n",
            "Iteration 12, Batch 24/100: Batch Loss = 1.3539\n",
            "Iteration 12, Batch 25/100: Batch Loss = 1.2564\n",
            "Iteration 12, Batch 26/100: Batch Loss = 1.3380\n",
            "Iteration 12, Batch 27/100: Batch Loss = 1.3916\n",
            "Iteration 12, Batch 28/100: Batch Loss = 1.2973\n",
            "Iteration 12, Batch 29/100: Batch Loss = 1.2251\n",
            "Iteration 12, Batch 30/100: Batch Loss = 1.2846\n",
            "Iteration 12, Batch 31/100: Batch Loss = 1.2156\n",
            "Iteration 12, Batch 32/100: Batch Loss = 1.2943\n",
            "Iteration 12, Batch 33/100: Batch Loss = 1.1810\n",
            "Iteration 12, Batch 34/100: Batch Loss = 1.3350\n",
            "Iteration 12, Batch 35/100: Batch Loss = 1.2278\n",
            "Iteration 12, Batch 36/100: Batch Loss = 1.2818\n",
            "Iteration 12, Batch 37/100: Batch Loss = 1.2689\n",
            "Iteration 12, Batch 38/100: Batch Loss = 1.3387\n",
            "Iteration 12, Batch 39/100: Batch Loss = 1.2752\n",
            "Iteration 12, Batch 40/100: Batch Loss = 1.3066\n",
            "Iteration 12, Batch 41/100: Batch Loss = 1.3288\n",
            "Iteration 12, Batch 42/100: Batch Loss = 1.2469\n",
            "Iteration 12, Batch 43/100: Batch Loss = 1.3443\n",
            "Iteration 12, Batch 44/100: Batch Loss = 1.3194\n",
            "Iteration 12, Batch 45/100: Batch Loss = 1.3037\n",
            "Iteration 12, Batch 46/100: Batch Loss = 1.2750\n",
            "Iteration 12, Batch 47/100: Batch Loss = 1.3379\n",
            "Iteration 12, Batch 48/100: Batch Loss = 1.2628\n",
            "Iteration 12, Batch 49/100: Batch Loss = 1.2811\n",
            "Iteration 12, Batch 50/100: Batch Loss = 1.3792\n",
            "Iteration 12, Batch 51/100: Batch Loss = 1.3165\n",
            "Iteration 12, Batch 52/100: Batch Loss = 1.2785\n",
            "Iteration 12, Batch 53/100: Batch Loss = 1.2789\n",
            "Iteration 12, Batch 54/100: Batch Loss = 1.3132\n",
            "Iteration 12, Batch 55/100: Batch Loss = 1.3857\n",
            "Iteration 12, Batch 56/100: Batch Loss = 1.2723\n",
            "Iteration 12, Batch 57/100: Batch Loss = 1.1714\n",
            "Iteration 12, Batch 58/100: Batch Loss = 1.2058\n",
            "Iteration 12, Batch 59/100: Batch Loss = 1.2531\n",
            "Iteration 12, Batch 60/100: Batch Loss = 1.2215\n",
            "Iteration 12, Batch 61/100: Batch Loss = 1.2911\n",
            "Iteration 12, Batch 62/100: Batch Loss = 1.2626\n",
            "Iteration 12, Batch 63/100: Batch Loss = 1.3130\n",
            "Iteration 12, Batch 64/100: Batch Loss = 1.2752\n",
            "Iteration 12, Batch 65/100: Batch Loss = 1.4299\n",
            "Iteration 12, Batch 66/100: Batch Loss = 1.3823\n",
            "Iteration 12, Batch 67/100: Batch Loss = 1.2344\n",
            "Iteration 12, Batch 68/100: Batch Loss = 1.3224\n",
            "Iteration 12, Batch 69/100: Batch Loss = 1.3821\n",
            "Iteration 12, Batch 70/100: Batch Loss = 1.1178\n",
            "Iteration 12, Batch 71/100: Batch Loss = 1.3037\n",
            "Iteration 12, Batch 72/100: Batch Loss = 1.3662\n",
            "Iteration 12, Batch 73/100: Batch Loss = 1.1961\n",
            "Iteration 12, Batch 74/100: Batch Loss = 1.2244\n",
            "Iteration 12, Batch 75/100: Batch Loss = 1.3258\n",
            "Iteration 12, Batch 76/100: Batch Loss = 1.1082\n",
            "Iteration 12, Batch 77/100: Batch Loss = 1.1771\n",
            "Iteration 12, Batch 78/100: Batch Loss = 1.3825\n",
            "Iteration 12, Batch 79/100: Batch Loss = 1.1955\n",
            "Iteration 12, Batch 80/100: Batch Loss = 1.3795\n",
            "Iteration 12, Batch 81/100: Batch Loss = 1.1713\n",
            "Iteration 12, Batch 82/100: Batch Loss = 1.2808\n",
            "Iteration 12, Batch 83/100: Batch Loss = 1.3165\n",
            "Iteration 12, Batch 84/100: Batch Loss = 1.4145\n",
            "Iteration 12, Batch 85/100: Batch Loss = 1.3753\n",
            "Iteration 12, Batch 86/100: Batch Loss = 1.1895\n",
            "Iteration 12, Batch 87/100: Batch Loss = 1.2476\n",
            "Iteration 12, Batch 88/100: Batch Loss = 1.2718\n",
            "Iteration 12, Batch 89/100: Batch Loss = 1.3740\n",
            "Iteration 12, Batch 90/100: Batch Loss = 1.1714\n",
            "Iteration 12, Batch 91/100: Batch Loss = 1.3855\n",
            "Iteration 12, Batch 92/100: Batch Loss = 1.3062\n",
            "Iteration 12, Batch 93/100: Batch Loss = 1.2754\n",
            "Iteration 12, Batch 94/100: Batch Loss = 1.1771\n",
            "Iteration 12, Batch 95/100: Batch Loss = 1.2335\n",
            "Iteration 12, Batch 96/100: Batch Loss = 1.4141\n",
            "Iteration 12, Batch 97/100: Batch Loss = 1.3993\n",
            "Iteration 12, Batch 98/100: Batch Loss = 1.2947\n",
            "Iteration 12, Batch 99/100: Batch Loss = 1.2336\n",
            "Iteration 12, Batch 100/100: Batch Loss = 1.3040\n",
            "Iteration 12: Train Loss = 1.2825, Val Loss = 1.2801\n",
            "Iteration 13, Batch 1/100: Batch Loss = 1.1829\n",
            "Iteration 13, Batch 2/100: Batch Loss = 1.2687\n",
            "Iteration 13, Batch 3/100: Batch Loss = 1.3289\n",
            "Iteration 13, Batch 4/100: Batch Loss = 1.4148\n",
            "Iteration 13, Batch 5/100: Batch Loss = 1.1581\n",
            "Iteration 13, Batch 6/100: Batch Loss = 1.2533\n",
            "Iteration 13, Batch 7/100: Batch Loss = 1.1989\n",
            "Iteration 13, Batch 8/100: Batch Loss = 1.3158\n",
            "Iteration 13, Batch 9/100: Batch Loss = 1.2870\n",
            "Iteration 13, Batch 10/100: Batch Loss = 1.2832\n",
            "Iteration 13, Batch 11/100: Batch Loss = 1.3527\n",
            "Iteration 13, Batch 12/100: Batch Loss = 1.1838\n",
            "Iteration 13, Batch 13/100: Batch Loss = 1.2827\n",
            "Iteration 13, Batch 14/100: Batch Loss = 1.4466\n",
            "Iteration 13, Batch 15/100: Batch Loss = 1.3095\n",
            "Iteration 13, Batch 16/100: Batch Loss = 1.2386\n",
            "Iteration 13, Batch 17/100: Batch Loss = 1.2835\n",
            "Iteration 13, Batch 18/100: Batch Loss = 1.3098\n",
            "Iteration 13, Batch 19/100: Batch Loss = 1.2155\n",
            "Iteration 13, Batch 20/100: Batch Loss = 1.2139\n",
            "Iteration 13, Batch 21/100: Batch Loss = 1.2656\n",
            "Iteration 13, Batch 22/100: Batch Loss = 1.1983\n",
            "Iteration 13, Batch 23/100: Batch Loss = 1.1957\n",
            "Iteration 13, Batch 24/100: Batch Loss = 1.4220\n",
            "Iteration 13, Batch 25/100: Batch Loss = 1.3092\n",
            "Iteration 13, Batch 26/100: Batch Loss = 1.2756\n",
            "Iteration 13, Batch 27/100: Batch Loss = 1.2121\n",
            "Iteration 13, Batch 28/100: Batch Loss = 1.2624\n",
            "Iteration 13, Batch 29/100: Batch Loss = 1.1925\n",
            "Iteration 13, Batch 30/100: Batch Loss = 1.1034\n",
            "Iteration 13, Batch 31/100: Batch Loss = 1.3561\n",
            "Iteration 13, Batch 32/100: Batch Loss = 1.1756\n",
            "Iteration 13, Batch 33/100: Batch Loss = 1.3449\n",
            "Iteration 13, Batch 34/100: Batch Loss = 1.2623\n",
            "Iteration 13, Batch 35/100: Batch Loss = 1.3105\n",
            "Iteration 13, Batch 36/100: Batch Loss = 1.3160\n",
            "Iteration 13, Batch 37/100: Batch Loss = 1.2800\n",
            "Iteration 13, Batch 38/100: Batch Loss = 1.1885\n",
            "Iteration 13, Batch 39/100: Batch Loss = 1.2415\n",
            "Iteration 13, Batch 40/100: Batch Loss = 1.2333\n",
            "Iteration 13, Batch 41/100: Batch Loss = 1.2146\n",
            "Iteration 13, Batch 42/100: Batch Loss = 1.3642\n",
            "Iteration 13, Batch 43/100: Batch Loss = 1.3160\n",
            "Iteration 13, Batch 44/100: Batch Loss = 1.3253\n",
            "Iteration 13, Batch 45/100: Batch Loss = 1.1874\n",
            "Iteration 13, Batch 46/100: Batch Loss = 1.1816\n",
            "Iteration 13, Batch 47/100: Batch Loss = 1.2776\n",
            "Iteration 13, Batch 48/100: Batch Loss = 1.3290\n",
            "Iteration 13, Batch 49/100: Batch Loss = 1.3872\n",
            "Iteration 13, Batch 50/100: Batch Loss = 1.2440\n",
            "Iteration 13, Batch 51/100: Batch Loss = 1.2438\n",
            "Iteration 13, Batch 52/100: Batch Loss = 1.4262\n",
            "Iteration 13, Batch 53/100: Batch Loss = 1.2580\n",
            "Iteration 13, Batch 54/100: Batch Loss = 1.1925\n",
            "Iteration 13, Batch 55/100: Batch Loss = 1.3286\n",
            "Iteration 13, Batch 56/100: Batch Loss = 1.3007\n",
            "Iteration 13, Batch 57/100: Batch Loss = 1.4279\n",
            "Iteration 13, Batch 58/100: Batch Loss = 1.2115\n",
            "Iteration 13, Batch 59/100: Batch Loss = 1.2843\n",
            "Iteration 13, Batch 60/100: Batch Loss = 1.1831\n",
            "Iteration 13, Batch 61/100: Batch Loss = 1.1859\n",
            "Iteration 13, Batch 62/100: Batch Loss = 1.2479\n",
            "Iteration 13, Batch 63/100: Batch Loss = 1.2408\n",
            "Iteration 13, Batch 64/100: Batch Loss = 1.3005\n",
            "Iteration 13, Batch 65/100: Batch Loss = 1.2974\n",
            "Iteration 13, Batch 66/100: Batch Loss = 1.2874\n",
            "Iteration 13, Batch 67/100: Batch Loss = 1.2460\n",
            "Iteration 13, Batch 68/100: Batch Loss = 1.2577\n",
            "Iteration 13, Batch 69/100: Batch Loss = 1.3315\n",
            "Iteration 13, Batch 70/100: Batch Loss = 1.2831\n",
            "Iteration 13, Batch 71/100: Batch Loss = 1.4194\n",
            "Iteration 13, Batch 72/100: Batch Loss = 1.3729\n",
            "Iteration 13, Batch 73/100: Batch Loss = 1.4182\n",
            "Iteration 13, Batch 74/100: Batch Loss = 1.2011\n",
            "Iteration 13, Batch 75/100: Batch Loss = 1.3774\n",
            "Iteration 13, Batch 76/100: Batch Loss = 1.3624\n",
            "Iteration 13, Batch 77/100: Batch Loss = 1.3456\n",
            "Iteration 13, Batch 78/100: Batch Loss = 1.1659\n",
            "Iteration 13, Batch 79/100: Batch Loss = 1.3268\n",
            "Iteration 13, Batch 80/100: Batch Loss = 1.2421\n",
            "Iteration 13, Batch 81/100: Batch Loss = 1.3758\n",
            "Iteration 13, Batch 82/100: Batch Loss = 1.4880\n",
            "Iteration 13, Batch 83/100: Batch Loss = 1.2054\n",
            "Iteration 13, Batch 84/100: Batch Loss = 1.1147\n",
            "Iteration 13, Batch 85/100: Batch Loss = 1.2658\n",
            "Iteration 13, Batch 86/100: Batch Loss = 1.3844\n",
            "Iteration 13, Batch 87/100: Batch Loss = 1.2296\n",
            "Iteration 13, Batch 88/100: Batch Loss = 1.2461\n",
            "Iteration 13, Batch 89/100: Batch Loss = 1.3901\n",
            "Iteration 13, Batch 90/100: Batch Loss = 1.3850\n",
            "Iteration 13, Batch 91/100: Batch Loss = 1.2391\n",
            "Iteration 13, Batch 92/100: Batch Loss = 1.3024\n",
            "Iteration 13, Batch 93/100: Batch Loss = 1.3354\n",
            "Iteration 13, Batch 94/100: Batch Loss = 1.2486\n",
            "Iteration 13, Batch 95/100: Batch Loss = 1.2682\n",
            "Iteration 13, Batch 96/100: Batch Loss = 1.2256\n",
            "Iteration 13, Batch 97/100: Batch Loss = 1.2903\n",
            "Iteration 13, Batch 98/100: Batch Loss = 1.3853\n",
            "Iteration 13, Batch 99/100: Batch Loss = 1.2178\n",
            "Iteration 13, Batch 100/100: Batch Loss = 1.3553\n",
            "Iteration 13: Train Loss = 1.2822, Val Loss = 1.2798\n",
            "Iteration 14, Batch 1/100: Batch Loss = 1.3653\n",
            "Iteration 14, Batch 2/100: Batch Loss = 1.3134\n",
            "Iteration 14, Batch 3/100: Batch Loss = 1.1602\n",
            "Iteration 14, Batch 4/100: Batch Loss = 1.2313\n",
            "Iteration 14, Batch 5/100: Batch Loss = 1.3306\n",
            "Iteration 14, Batch 6/100: Batch Loss = 1.3025\n",
            "Iteration 14, Batch 7/100: Batch Loss = 1.2544\n",
            "Iteration 14, Batch 8/100: Batch Loss = 1.2666\n",
            "Iteration 14, Batch 9/100: Batch Loss = 1.2460\n",
            "Iteration 14, Batch 10/100: Batch Loss = 1.3929\n",
            "Iteration 14, Batch 11/100: Batch Loss = 1.3030\n",
            "Iteration 14, Batch 12/100: Batch Loss = 1.2034\n",
            "Iteration 14, Batch 13/100: Batch Loss = 1.2744\n",
            "Iteration 14, Batch 14/100: Batch Loss = 1.4249\n",
            "Iteration 14, Batch 15/100: Batch Loss = 1.1948\n",
            "Iteration 14, Batch 16/100: Batch Loss = 1.2390\n",
            "Iteration 14, Batch 17/100: Batch Loss = 1.3871\n",
            "Iteration 14, Batch 18/100: Batch Loss = 1.3021\n",
            "Iteration 14, Batch 19/100: Batch Loss = 1.3449\n",
            "Iteration 14, Batch 20/100: Batch Loss = 1.3322\n",
            "Iteration 14, Batch 21/100: Batch Loss = 1.2553\n",
            "Iteration 14, Batch 22/100: Batch Loss = 1.3385\n",
            "Iteration 14, Batch 23/100: Batch Loss = 1.1496\n",
            "Iteration 14, Batch 24/100: Batch Loss = 1.3488\n",
            "Iteration 14, Batch 25/100: Batch Loss = 1.3001\n",
            "Iteration 14, Batch 26/100: Batch Loss = 1.2484\n",
            "Iteration 14, Batch 27/100: Batch Loss = 1.3029\n",
            "Iteration 14, Batch 28/100: Batch Loss = 1.2896\n",
            "Iteration 14, Batch 29/100: Batch Loss = 1.2620\n",
            "Iteration 14, Batch 30/100: Batch Loss = 1.1740\n",
            "Iteration 14, Batch 31/100: Batch Loss = 1.1243\n",
            "Iteration 14, Batch 32/100: Batch Loss = 1.2145\n",
            "Iteration 14, Batch 33/100: Batch Loss = 1.2786\n",
            "Iteration 14, Batch 34/100: Batch Loss = 1.2664\n",
            "Iteration 14, Batch 35/100: Batch Loss = 1.2364\n",
            "Iteration 14, Batch 36/100: Batch Loss = 1.4524\n",
            "Iteration 14, Batch 37/100: Batch Loss = 1.2298\n",
            "Iteration 14, Batch 38/100: Batch Loss = 1.2528\n",
            "Iteration 14, Batch 39/100: Batch Loss = 1.1586\n",
            "Iteration 14, Batch 40/100: Batch Loss = 1.2986\n",
            "Iteration 14, Batch 41/100: Batch Loss = 1.2688\n",
            "Iteration 14, Batch 42/100: Batch Loss = 1.2231\n",
            "Iteration 14, Batch 43/100: Batch Loss = 1.3060\n",
            "Iteration 14, Batch 44/100: Batch Loss = 1.4232\n",
            "Iteration 14, Batch 45/100: Batch Loss = 1.2193\n",
            "Iteration 14, Batch 46/100: Batch Loss = 1.4413\n",
            "Iteration 14, Batch 47/100: Batch Loss = 1.1834\n",
            "Iteration 14, Batch 48/100: Batch Loss = 1.3210\n",
            "Iteration 14, Batch 49/100: Batch Loss = 1.3978\n",
            "Iteration 14, Batch 50/100: Batch Loss = 1.4089\n",
            "Iteration 14, Batch 51/100: Batch Loss = 1.3197\n",
            "Iteration 14, Batch 52/100: Batch Loss = 1.1907\n",
            "Iteration 14, Batch 53/100: Batch Loss = 1.3100\n",
            "Iteration 14, Batch 54/100: Batch Loss = 1.4338\n",
            "Iteration 14, Batch 55/100: Batch Loss = 1.1875\n",
            "Iteration 14, Batch 56/100: Batch Loss = 1.2266\n",
            "Iteration 14, Batch 57/100: Batch Loss = 1.2598\n",
            "Iteration 14, Batch 58/100: Batch Loss = 1.3391\n",
            "Iteration 14, Batch 59/100: Batch Loss = 1.1723\n",
            "Iteration 14, Batch 60/100: Batch Loss = 1.3126\n",
            "Iteration 14, Batch 61/100: Batch Loss = 1.4539\n",
            "Iteration 14, Batch 62/100: Batch Loss = 1.2694\n",
            "Iteration 14, Batch 63/100: Batch Loss = 1.4019\n",
            "Iteration 14, Batch 64/100: Batch Loss = 1.2915\n",
            "Iteration 14, Batch 65/100: Batch Loss = 1.2477\n",
            "Iteration 14, Batch 66/100: Batch Loss = 1.2885\n",
            "Iteration 14, Batch 67/100: Batch Loss = 1.3696\n",
            "Iteration 14, Batch 68/100: Batch Loss = 1.2947\n",
            "Iteration 14, Batch 69/100: Batch Loss = 1.3511\n",
            "Iteration 14, Batch 70/100: Batch Loss = 1.2697\n",
            "Iteration 14, Batch 71/100: Batch Loss = 1.3059\n",
            "Iteration 14, Batch 72/100: Batch Loss = 1.2369\n",
            "Iteration 14, Batch 73/100: Batch Loss = 1.2606\n",
            "Iteration 14, Batch 74/100: Batch Loss = 1.3871\n",
            "Iteration 14, Batch 75/100: Batch Loss = 1.1735\n",
            "Iteration 14, Batch 76/100: Batch Loss = 1.3397\n",
            "Iteration 14, Batch 77/100: Batch Loss = 1.2041\n",
            "Iteration 14, Batch 78/100: Batch Loss = 1.3021\n",
            "Iteration 14, Batch 79/100: Batch Loss = 1.2991\n",
            "Iteration 14, Batch 80/100: Batch Loss = 1.2392\n",
            "Iteration 14, Batch 81/100: Batch Loss = 1.2400\n",
            "Iteration 14, Batch 82/100: Batch Loss = 1.2837\n",
            "Iteration 14, Batch 83/100: Batch Loss = 1.1201\n",
            "Iteration 14, Batch 84/100: Batch Loss = 1.1950\n",
            "Iteration 14, Batch 85/100: Batch Loss = 1.2662\n",
            "Iteration 14, Batch 86/100: Batch Loss = 1.1753\n",
            "Iteration 14, Batch 87/100: Batch Loss = 1.2489\n",
            "Iteration 14, Batch 88/100: Batch Loss = 1.2628\n",
            "Iteration 14, Batch 89/100: Batch Loss = 1.3041\n",
            "Iteration 14, Batch 90/100: Batch Loss = 1.3598\n",
            "Iteration 14, Batch 91/100: Batch Loss = 1.3020\n",
            "Iteration 14, Batch 92/100: Batch Loss = 1.2711\n",
            "Iteration 14, Batch 93/100: Batch Loss = 1.2226\n",
            "Iteration 14, Batch 94/100: Batch Loss = 1.2698\n",
            "Iteration 14, Batch 95/100: Batch Loss = 1.2117\n",
            "Iteration 14, Batch 96/100: Batch Loss = 1.2501\n",
            "Iteration 14, Batch 97/100: Batch Loss = 1.2490\n",
            "Iteration 14, Batch 98/100: Batch Loss = 1.2987\n",
            "Iteration 14, Batch 99/100: Batch Loss = 1.3284\n",
            "Iteration 14, Batch 100/100: Batch Loss = 1.3590\n",
            "Iteration 14: Train Loss = 1.2820, Val Loss = 1.2796\n",
            "Iteration 15, Batch 1/100: Batch Loss = 1.4435\n",
            "Iteration 15, Batch 2/100: Batch Loss = 1.3436\n",
            "Iteration 15, Batch 3/100: Batch Loss = 1.3468\n",
            "Iteration 15, Batch 4/100: Batch Loss = 1.3333\n",
            "Iteration 15, Batch 5/100: Batch Loss = 1.4026\n",
            "Iteration 15, Batch 6/100: Batch Loss = 1.2772\n",
            "Iteration 15, Batch 7/100: Batch Loss = 1.3870\n",
            "Iteration 15, Batch 8/100: Batch Loss = 1.2500\n",
            "Iteration 15, Batch 9/100: Batch Loss = 1.1526\n",
            "Iteration 15, Batch 10/100: Batch Loss = 1.2806\n",
            "Iteration 15, Batch 11/100: Batch Loss = 1.1910\n",
            "Iteration 15, Batch 12/100: Batch Loss = 1.2602\n",
            "Iteration 15, Batch 13/100: Batch Loss = 1.2479\n",
            "Iteration 15, Batch 14/100: Batch Loss = 1.1739\n",
            "Iteration 15, Batch 15/100: Batch Loss = 1.1863\n",
            "Iteration 15, Batch 16/100: Batch Loss = 1.1970\n",
            "Iteration 15, Batch 17/100: Batch Loss = 1.3021\n",
            "Iteration 15, Batch 18/100: Batch Loss = 1.2413\n",
            "Iteration 15, Batch 19/100: Batch Loss = 1.2114\n",
            "Iteration 15, Batch 20/100: Batch Loss = 1.1871\n",
            "Iteration 15, Batch 21/100: Batch Loss = 1.1509\n",
            "Iteration 15, Batch 22/100: Batch Loss = 1.2561\n",
            "Iteration 15, Batch 23/100: Batch Loss = 1.3405\n",
            "Iteration 15, Batch 24/100: Batch Loss = 1.3243\n",
            "Iteration 15, Batch 25/100: Batch Loss = 1.4072\n",
            "Iteration 15, Batch 26/100: Batch Loss = 1.2487\n",
            "Iteration 15, Batch 27/100: Batch Loss = 1.2238\n",
            "Iteration 15, Batch 28/100: Batch Loss = 1.2339\n",
            "Iteration 15, Batch 29/100: Batch Loss = 1.3072\n",
            "Iteration 15, Batch 30/100: Batch Loss = 1.3629\n",
            "Iteration 15, Batch 31/100: Batch Loss = 1.3871\n",
            "Iteration 15, Batch 32/100: Batch Loss = 1.2028\n",
            "Iteration 15, Batch 33/100: Batch Loss = 1.1960\n",
            "Iteration 15, Batch 34/100: Batch Loss = 1.4354\n",
            "Iteration 15, Batch 35/100: Batch Loss = 1.3671\n",
            "Iteration 15, Batch 36/100: Batch Loss = 1.2850\n",
            "Iteration 15, Batch 37/100: Batch Loss = 1.2741\n",
            "Iteration 15, Batch 38/100: Batch Loss = 1.2444\n",
            "Iteration 15, Batch 39/100: Batch Loss = 1.3105\n",
            "Iteration 15, Batch 40/100: Batch Loss = 1.2767\n",
            "Iteration 15, Batch 41/100: Batch Loss = 1.3284\n",
            "Iteration 15, Batch 42/100: Batch Loss = 1.3356\n",
            "Iteration 15, Batch 43/100: Batch Loss = 1.2628\n",
            "Iteration 15, Batch 44/100: Batch Loss = 1.1619\n",
            "Iteration 15, Batch 45/100: Batch Loss = 1.2112\n",
            "Iteration 15, Batch 46/100: Batch Loss = 1.1794\n",
            "Iteration 15, Batch 47/100: Batch Loss = 1.2410\n",
            "Iteration 15, Batch 48/100: Batch Loss = 1.2520\n",
            "Iteration 15, Batch 49/100: Batch Loss = 1.2949\n",
            "Iteration 15, Batch 50/100: Batch Loss = 1.3021\n",
            "Iteration 15, Batch 51/100: Batch Loss = 1.2700\n",
            "Iteration 15, Batch 52/100: Batch Loss = 1.2790\n",
            "Iteration 15, Batch 53/100: Batch Loss = 1.2574\n",
            "Iteration 15, Batch 54/100: Batch Loss = 1.2841\n",
            "Iteration 15, Batch 55/100: Batch Loss = 1.2870\n",
            "Iteration 15, Batch 56/100: Batch Loss = 1.3925\n",
            "Iteration 15, Batch 57/100: Batch Loss = 1.1775\n",
            "Iteration 15, Batch 58/100: Batch Loss = 1.2936\n",
            "Iteration 15, Batch 59/100: Batch Loss = 1.2209\n",
            "Iteration 15, Batch 60/100: Batch Loss = 1.3277\n",
            "Iteration 15, Batch 61/100: Batch Loss = 1.2047\n",
            "Iteration 15, Batch 62/100: Batch Loss = 1.3610\n",
            "Iteration 15, Batch 63/100: Batch Loss = 1.3993\n",
            "Iteration 15, Batch 64/100: Batch Loss = 1.4035\n",
            "Iteration 15, Batch 65/100: Batch Loss = 1.3326\n",
            "Iteration 15, Batch 66/100: Batch Loss = 1.3182\n",
            "Iteration 15, Batch 67/100: Batch Loss = 1.2359\n",
            "Iteration 15, Batch 68/100: Batch Loss = 1.3775\n",
            "Iteration 15, Batch 69/100: Batch Loss = 1.4779\n",
            "Iteration 15, Batch 70/100: Batch Loss = 1.3694\n",
            "Iteration 15, Batch 71/100: Batch Loss = 1.2867\n",
            "Iteration 15, Batch 72/100: Batch Loss = 1.1386\n",
            "Iteration 15, Batch 73/100: Batch Loss = 1.2755\n",
            "Iteration 15, Batch 74/100: Batch Loss = 1.3040\n",
            "Iteration 15, Batch 75/100: Batch Loss = 1.2271\n",
            "Iteration 15, Batch 76/100: Batch Loss = 1.2995\n",
            "Iteration 15, Batch 77/100: Batch Loss = 1.1934\n",
            "Iteration 15, Batch 78/100: Batch Loss = 1.2741\n",
            "Iteration 15, Batch 79/100: Batch Loss = 1.1979\n",
            "Iteration 15, Batch 80/100: Batch Loss = 1.3468\n",
            "Iteration 15, Batch 81/100: Batch Loss = 1.3204\n",
            "Iteration 15, Batch 82/100: Batch Loss = 1.4068\n",
            "Iteration 15, Batch 83/100: Batch Loss = 1.2388\n",
            "Iteration 15, Batch 84/100: Batch Loss = 1.2138\n",
            "Iteration 15, Batch 85/100: Batch Loss = 1.2531\n",
            "Iteration 15, Batch 86/100: Batch Loss = 1.2454\n",
            "Iteration 15, Batch 87/100: Batch Loss = 1.3411\n",
            "Iteration 15, Batch 88/100: Batch Loss = 1.3213\n",
            "Iteration 15, Batch 89/100: Batch Loss = 1.2488\n",
            "Iteration 15, Batch 90/100: Batch Loss = 1.3876\n",
            "Iteration 15, Batch 91/100: Batch Loss = 1.2348\n",
            "Iteration 15, Batch 92/100: Batch Loss = 1.3215\n",
            "Iteration 15, Batch 93/100: Batch Loss = 1.2268\n",
            "Iteration 15, Batch 94/100: Batch Loss = 1.2815\n",
            "Iteration 15, Batch 95/100: Batch Loss = 1.2287\n",
            "Iteration 15, Batch 96/100: Batch Loss = 1.2340\n",
            "Iteration 15, Batch 97/100: Batch Loss = 1.2361\n",
            "Iteration 15, Batch 98/100: Batch Loss = 1.1978\n",
            "Iteration 15, Batch 99/100: Batch Loss = 1.2960\n",
            "Iteration 15, Batch 100/100: Batch Loss = 1.3167\n",
            "Iteration 15: Train Loss = 1.2818, Val Loss = 1.2794\n",
            "Iteration 16, Batch 1/100: Batch Loss = 1.2677\n",
            "Iteration 16, Batch 2/100: Batch Loss = 1.2421\n",
            "Iteration 16, Batch 3/100: Batch Loss = 1.2629\n",
            "Iteration 16, Batch 4/100: Batch Loss = 1.2186\n",
            "Iteration 16, Batch 5/100: Batch Loss = 1.2817\n",
            "Iteration 16, Batch 6/100: Batch Loss = 1.2313\n",
            "Iteration 16, Batch 7/100: Batch Loss = 1.3201\n",
            "Iteration 16, Batch 8/100: Batch Loss = 1.4068\n",
            "Iteration 16, Batch 9/100: Batch Loss = 1.3094\n",
            "Iteration 16, Batch 10/100: Batch Loss = 1.3006\n",
            "Iteration 16, Batch 11/100: Batch Loss = 1.2636\n",
            "Iteration 16, Batch 12/100: Batch Loss = 1.2500\n",
            "Iteration 16, Batch 13/100: Batch Loss = 1.2765\n",
            "Iteration 16, Batch 14/100: Batch Loss = 1.2986\n",
            "Iteration 16, Batch 15/100: Batch Loss = 1.2674\n",
            "Iteration 16, Batch 16/100: Batch Loss = 1.2584\n",
            "Iteration 16, Batch 17/100: Batch Loss = 1.2674\n",
            "Iteration 16, Batch 18/100: Batch Loss = 1.3331\n",
            "Iteration 16, Batch 19/100: Batch Loss = 1.1922\n",
            "Iteration 16, Batch 20/100: Batch Loss = 1.3739\n",
            "Iteration 16, Batch 21/100: Batch Loss = 1.2841\n",
            "Iteration 16, Batch 22/100: Batch Loss = 1.3323\n",
            "Iteration 16, Batch 23/100: Batch Loss = 1.2457\n",
            "Iteration 16, Batch 24/100: Batch Loss = 1.2789\n",
            "Iteration 16, Batch 25/100: Batch Loss = 1.2545\n",
            "Iteration 16, Batch 26/100: Batch Loss = 1.1421\n",
            "Iteration 16, Batch 27/100: Batch Loss = 1.2090\n",
            "Iteration 16, Batch 28/100: Batch Loss = 1.2415\n",
            "Iteration 16, Batch 29/100: Batch Loss = 1.3072\n",
            "Iteration 16, Batch 30/100: Batch Loss = 1.1996\n",
            "Iteration 16, Batch 31/100: Batch Loss = 1.2916\n",
            "Iteration 16, Batch 32/100: Batch Loss = 1.2933\n",
            "Iteration 16, Batch 33/100: Batch Loss = 1.3846\n",
            "Iteration 16, Batch 34/100: Batch Loss = 1.3221\n",
            "Iteration 16, Batch 35/100: Batch Loss = 1.2895\n",
            "Iteration 16, Batch 36/100: Batch Loss = 1.1787\n",
            "Iteration 16, Batch 37/100: Batch Loss = 1.2591\n",
            "Iteration 16, Batch 38/100: Batch Loss = 1.2268\n",
            "Iteration 16, Batch 39/100: Batch Loss = 1.3185\n",
            "Iteration 16, Batch 40/100: Batch Loss = 1.2498\n",
            "Iteration 16, Batch 41/100: Batch Loss = 1.3165\n",
            "Iteration 16, Batch 42/100: Batch Loss = 1.3768\n",
            "Iteration 16, Batch 43/100: Batch Loss = 1.2969\n",
            "Iteration 16, Batch 44/100: Batch Loss = 1.2803\n",
            "Iteration 16, Batch 45/100: Batch Loss = 1.3765\n",
            "Iteration 16, Batch 46/100: Batch Loss = 1.2440\n",
            "Iteration 16, Batch 47/100: Batch Loss = 1.3810\n",
            "Iteration 16, Batch 48/100: Batch Loss = 1.2492\n",
            "Iteration 16, Batch 49/100: Batch Loss = 1.3552\n",
            "Iteration 16, Batch 50/100: Batch Loss = 1.2545\n",
            "Iteration 16, Batch 51/100: Batch Loss = 1.3145\n",
            "Iteration 16, Batch 52/100: Batch Loss = 1.1504\n",
            "Iteration 16, Batch 53/100: Batch Loss = 1.2453\n",
            "Iteration 16, Batch 54/100: Batch Loss = 1.3073\n",
            "Iteration 16, Batch 55/100: Batch Loss = 1.3105\n",
            "Iteration 16, Batch 56/100: Batch Loss = 1.2884\n",
            "Iteration 16, Batch 57/100: Batch Loss = 1.2560\n",
            "Iteration 16, Batch 58/100: Batch Loss = 1.2794\n",
            "Iteration 16, Batch 59/100: Batch Loss = 1.2372\n",
            "Iteration 16, Batch 60/100: Batch Loss = 1.2931\n",
            "Iteration 16, Batch 61/100: Batch Loss = 1.2925\n",
            "Iteration 16, Batch 62/100: Batch Loss = 1.1219\n",
            "Iteration 16, Batch 63/100: Batch Loss = 1.2486\n",
            "Iteration 16, Batch 64/100: Batch Loss = 1.3499\n",
            "Iteration 16, Batch 65/100: Batch Loss = 1.3295\n",
            "Iteration 16, Batch 66/100: Batch Loss = 1.2157\n",
            "Iteration 16, Batch 67/100: Batch Loss = 1.4669\n",
            "Iteration 16, Batch 68/100: Batch Loss = 1.3962\n",
            "Iteration 16, Batch 69/100: Batch Loss = 1.2727\n",
            "Iteration 16, Batch 70/100: Batch Loss = 1.3332\n",
            "Iteration 16, Batch 71/100: Batch Loss = 1.3201\n",
            "Iteration 16, Batch 72/100: Batch Loss = 1.2229\n",
            "Iteration 16, Batch 73/100: Batch Loss = 1.3546\n",
            "Iteration 16, Batch 74/100: Batch Loss = 1.3078\n",
            "Iteration 16, Batch 75/100: Batch Loss = 1.4351\n",
            "Iteration 16, Batch 76/100: Batch Loss = 1.2148\n",
            "Iteration 16, Batch 77/100: Batch Loss = 1.3112\n",
            "Iteration 16, Batch 78/100: Batch Loss = 1.2309\n",
            "Iteration 16, Batch 79/100: Batch Loss = 1.1297\n",
            "Iteration 16, Batch 80/100: Batch Loss = 1.3028\n",
            "Iteration 16, Batch 81/100: Batch Loss = 1.2275\n",
            "Iteration 16, Batch 82/100: Batch Loss = 1.2921\n",
            "Iteration 16, Batch 83/100: Batch Loss = 1.2860\n",
            "Iteration 16, Batch 84/100: Batch Loss = 1.2887\n",
            "Iteration 16, Batch 85/100: Batch Loss = 1.1526\n",
            "Iteration 16, Batch 86/100: Batch Loss = 1.2860\n",
            "Iteration 16, Batch 87/100: Batch Loss = 1.2629\n",
            "Iteration 16, Batch 88/100: Batch Loss = 1.3230\n",
            "Iteration 16, Batch 89/100: Batch Loss = 1.2499\n",
            "Iteration 16, Batch 90/100: Batch Loss = 1.1865\n",
            "Iteration 16, Batch 91/100: Batch Loss = 1.2449\n",
            "Iteration 16, Batch 92/100: Batch Loss = 1.2260\n",
            "Iteration 16, Batch 93/100: Batch Loss = 1.2239\n",
            "Iteration 16, Batch 94/100: Batch Loss = 1.2945\n",
            "Iteration 16, Batch 95/100: Batch Loss = 1.2957\n",
            "Iteration 16, Batch 96/100: Batch Loss = 1.3299\n",
            "Iteration 16, Batch 97/100: Batch Loss = 1.3522\n",
            "Iteration 16, Batch 98/100: Batch Loss = 1.3440\n",
            "Iteration 16, Batch 99/100: Batch Loss = 1.3026\n",
            "Iteration 16, Batch 100/100: Batch Loss = 1.3990\n",
            "Iteration 16: Train Loss = 1.2817, Val Loss = 1.2793\n",
            "Iteration 17, Batch 1/100: Batch Loss = 1.2708\n",
            "Iteration 17, Batch 2/100: Batch Loss = 1.3241\n",
            "Iteration 17, Batch 3/100: Batch Loss = 1.3791\n",
            "Iteration 17, Batch 4/100: Batch Loss = 1.2176\n",
            "Iteration 17, Batch 5/100: Batch Loss = 1.3235\n",
            "Iteration 17, Batch 6/100: Batch Loss = 1.2474\n",
            "Iteration 17, Batch 7/100: Batch Loss = 1.1524\n",
            "Iteration 17, Batch 8/100: Batch Loss = 1.1949\n",
            "Iteration 17, Batch 9/100: Batch Loss = 1.3106\n",
            "Iteration 17, Batch 10/100: Batch Loss = 1.2626\n",
            "Iteration 17, Batch 11/100: Batch Loss = 1.2852\n",
            "Iteration 17, Batch 12/100: Batch Loss = 1.3692\n",
            "Iteration 17, Batch 13/100: Batch Loss = 1.2703\n",
            "Iteration 17, Batch 14/100: Batch Loss = 1.3621\n",
            "Iteration 17, Batch 15/100: Batch Loss = 1.3323\n",
            "Iteration 17, Batch 16/100: Batch Loss = 1.1575\n",
            "Iteration 17, Batch 17/100: Batch Loss = 1.3237\n",
            "Iteration 17, Batch 18/100: Batch Loss = 1.3236\n",
            "Iteration 17, Batch 19/100: Batch Loss = 1.2494\n",
            "Iteration 17, Batch 20/100: Batch Loss = 1.3091\n",
            "Iteration 17, Batch 21/100: Batch Loss = 1.3424\n",
            "Iteration 17, Batch 22/100: Batch Loss = 1.1735\n",
            "Iteration 17, Batch 23/100: Batch Loss = 1.3433\n",
            "Iteration 17, Batch 24/100: Batch Loss = 1.2463\n",
            "Iteration 17, Batch 25/100: Batch Loss = 1.2495\n",
            "Iteration 17, Batch 26/100: Batch Loss = 1.2903\n",
            "Iteration 17, Batch 27/100: Batch Loss = 1.4016\n",
            "Iteration 17, Batch 28/100: Batch Loss = 1.4081\n",
            "Iteration 17, Batch 29/100: Batch Loss = 1.3389\n",
            "Iteration 17, Batch 30/100: Batch Loss = 1.2546\n",
            "Iteration 17, Batch 31/100: Batch Loss = 1.1628\n",
            "Iteration 17, Batch 32/100: Batch Loss = 1.2791\n",
            "Iteration 17, Batch 33/100: Batch Loss = 1.3348\n",
            "Iteration 17, Batch 34/100: Batch Loss = 1.2452\n",
            "Iteration 17, Batch 35/100: Batch Loss = 1.3686\n",
            "Iteration 17, Batch 36/100: Batch Loss = 1.2958\n",
            "Iteration 17, Batch 37/100: Batch Loss = 1.3208\n",
            "Iteration 17, Batch 38/100: Batch Loss = 1.1769\n",
            "Iteration 17, Batch 39/100: Batch Loss = 1.2099\n",
            "Iteration 17, Batch 40/100: Batch Loss = 1.2319\n",
            "Iteration 17, Batch 41/100: Batch Loss = 1.3286\n",
            "Iteration 17, Batch 42/100: Batch Loss = 1.2879\n",
            "Iteration 17, Batch 43/100: Batch Loss = 1.3168\n",
            "Iteration 17, Batch 44/100: Batch Loss = 1.3134\n",
            "Iteration 17, Batch 45/100: Batch Loss = 1.3412\n",
            "Iteration 17, Batch 46/100: Batch Loss = 1.3215\n",
            "Iteration 17, Batch 47/100: Batch Loss = 1.2289\n",
            "Iteration 17, Batch 48/100: Batch Loss = 1.2447\n",
            "Iteration 17, Batch 49/100: Batch Loss = 1.3124\n",
            "Iteration 17, Batch 50/100: Batch Loss = 1.4523\n",
            "Iteration 17, Batch 51/100: Batch Loss = 1.3677\n",
            "Iteration 17, Batch 52/100: Batch Loss = 1.3907\n",
            "Iteration 17, Batch 53/100: Batch Loss = 1.3154\n",
            "Iteration 17, Batch 54/100: Batch Loss = 1.2310\n",
            "Iteration 17, Batch 55/100: Batch Loss = 1.2354\n",
            "Iteration 17, Batch 56/100: Batch Loss = 1.2664\n",
            "Iteration 17, Batch 57/100: Batch Loss = 1.2896\n",
            "Iteration 17, Batch 58/100: Batch Loss = 1.1935\n",
            "Iteration 17, Batch 59/100: Batch Loss = 1.1913\n",
            "Iteration 17, Batch 60/100: Batch Loss = 1.3088\n",
            "Iteration 17, Batch 61/100: Batch Loss = 1.2954\n",
            "Iteration 17, Batch 62/100: Batch Loss = 1.2757\n",
            "Iteration 17, Batch 63/100: Batch Loss = 1.3686\n",
            "Iteration 17, Batch 64/100: Batch Loss = 1.0713\n",
            "Iteration 17, Batch 65/100: Batch Loss = 1.2342\n",
            "Iteration 17, Batch 66/100: Batch Loss = 1.2850\n",
            "Iteration 17, Batch 67/100: Batch Loss = 1.4079\n",
            "Iteration 17, Batch 68/100: Batch Loss = 1.3426\n",
            "Iteration 17, Batch 69/100: Batch Loss = 1.2834\n",
            "Iteration 17, Batch 70/100: Batch Loss = 1.1855\n",
            "Iteration 17, Batch 71/100: Batch Loss = 1.3247\n",
            "Iteration 17, Batch 72/100: Batch Loss = 1.2824\n",
            "Iteration 17, Batch 73/100: Batch Loss = 1.2153\n",
            "Iteration 17, Batch 74/100: Batch Loss = 1.2537\n",
            "Iteration 17, Batch 75/100: Batch Loss = 1.3733\n",
            "Iteration 17, Batch 76/100: Batch Loss = 1.2255\n",
            "Iteration 17, Batch 77/100: Batch Loss = 1.3966\n",
            "Iteration 17, Batch 78/100: Batch Loss = 1.2758\n",
            "Iteration 17, Batch 79/100: Batch Loss = 1.2886\n",
            "Iteration 17, Batch 80/100: Batch Loss = 1.1904\n",
            "Iteration 17, Batch 81/100: Batch Loss = 1.3382\n",
            "Iteration 17, Batch 82/100: Batch Loss = 1.2323\n",
            "Iteration 17, Batch 83/100: Batch Loss = 1.2260\n",
            "Iteration 17, Batch 84/100: Batch Loss = 1.3349\n",
            "Iteration 17, Batch 85/100: Batch Loss = 1.2027\n",
            "Iteration 17, Batch 86/100: Batch Loss = 1.3243\n",
            "Iteration 17, Batch 87/100: Batch Loss = 1.3355\n",
            "Iteration 17, Batch 88/100: Batch Loss = 1.3178\n",
            "Iteration 17, Batch 89/100: Batch Loss = 1.2516\n",
            "Iteration 17, Batch 90/100: Batch Loss = 1.2265\n",
            "Iteration 17, Batch 91/100: Batch Loss = 1.2474\n",
            "Iteration 17, Batch 92/100: Batch Loss = 1.2391\n",
            "Iteration 17, Batch 93/100: Batch Loss = 1.2176\n",
            "Iteration 17, Batch 94/100: Batch Loss = 1.1867\n",
            "Iteration 17, Batch 95/100: Batch Loss = 1.2302\n",
            "Iteration 17, Batch 96/100: Batch Loss = 1.2668\n",
            "Iteration 17, Batch 97/100: Batch Loss = 1.3163\n",
            "Iteration 17, Batch 98/100: Batch Loss = 1.3138\n",
            "Iteration 17, Batch 99/100: Batch Loss = 1.2962\n",
            "Iteration 17, Batch 100/100: Batch Loss = 1.2042\n",
            "Iteration 17: Train Loss = 1.2816, Val Loss = 1.2792\n",
            "Iteration 18, Batch 1/100: Batch Loss = 1.2491\n",
            "Iteration 18, Batch 2/100: Batch Loss = 1.2923\n",
            "Iteration 18, Batch 3/100: Batch Loss = 1.3513\n",
            "Iteration 18, Batch 4/100: Batch Loss = 1.4142\n",
            "Iteration 18, Batch 5/100: Batch Loss = 1.2810\n",
            "Iteration 18, Batch 6/100: Batch Loss = 1.2073\n",
            "Iteration 18, Batch 7/100: Batch Loss = 1.2140\n",
            "Iteration 18, Batch 8/100: Batch Loss = 1.3161\n",
            "Iteration 18, Batch 9/100: Batch Loss = 1.2664\n",
            "Iteration 18, Batch 10/100: Batch Loss = 1.3081\n",
            "Iteration 18, Batch 11/100: Batch Loss = 1.2046\n",
            "Iteration 18, Batch 12/100: Batch Loss = 1.2584\n",
            "Iteration 18, Batch 13/100: Batch Loss = 1.3038\n",
            "Iteration 18, Batch 14/100: Batch Loss = 1.3292\n",
            "Iteration 18, Batch 15/100: Batch Loss = 1.2101\n",
            "Iteration 18, Batch 16/100: Batch Loss = 1.2927\n",
            "Iteration 18, Batch 17/100: Batch Loss = 1.1636\n",
            "Iteration 18, Batch 18/100: Batch Loss = 1.1963\n",
            "Iteration 18, Batch 19/100: Batch Loss = 1.4103\n",
            "Iteration 18, Batch 20/100: Batch Loss = 1.1987\n",
            "Iteration 18, Batch 21/100: Batch Loss = 1.3147\n",
            "Iteration 18, Batch 22/100: Batch Loss = 1.2534\n",
            "Iteration 18, Batch 23/100: Batch Loss = 1.4057\n",
            "Iteration 18, Batch 24/100: Batch Loss = 1.3414\n",
            "Iteration 18, Batch 25/100: Batch Loss = 1.5011\n",
            "Iteration 18, Batch 26/100: Batch Loss = 1.2946\n",
            "Iteration 18, Batch 27/100: Batch Loss = 1.1843\n",
            "Iteration 18, Batch 28/100: Batch Loss = 1.1512\n",
            "Iteration 18, Batch 29/100: Batch Loss = 1.3090\n",
            "Iteration 18, Batch 30/100: Batch Loss = 1.2435\n",
            "Iteration 18, Batch 31/100: Batch Loss = 1.2286\n",
            "Iteration 18, Batch 32/100: Batch Loss = 1.3627\n",
            "Iteration 18, Batch 33/100: Batch Loss = 1.1603\n",
            "Iteration 18, Batch 34/100: Batch Loss = 1.3251\n",
            "Iteration 18, Batch 35/100: Batch Loss = 1.3806\n",
            "Iteration 18, Batch 36/100: Batch Loss = 1.4013\n",
            "Iteration 18, Batch 37/100: Batch Loss = 1.3040\n",
            "Iteration 18, Batch 38/100: Batch Loss = 1.2714\n",
            "Iteration 18, Batch 39/100: Batch Loss = 1.3514\n",
            "Iteration 18, Batch 40/100: Batch Loss = 1.2652\n",
            "Iteration 18, Batch 41/100: Batch Loss = 1.3241\n",
            "Iteration 18, Batch 42/100: Batch Loss = 1.1995\n",
            "Iteration 18, Batch 43/100: Batch Loss = 1.2765\n",
            "Iteration 18, Batch 44/100: Batch Loss = 1.3075\n",
            "Iteration 18, Batch 45/100: Batch Loss = 1.3534\n",
            "Iteration 18, Batch 46/100: Batch Loss = 1.2764\n",
            "Iteration 18, Batch 47/100: Batch Loss = 1.3953\n",
            "Iteration 18, Batch 48/100: Batch Loss = 1.3244\n",
            "Iteration 18, Batch 49/100: Batch Loss = 1.3004\n",
            "Iteration 18, Batch 50/100: Batch Loss = 1.1115\n",
            "Iteration 18, Batch 51/100: Batch Loss = 1.4135\n",
            "Iteration 18, Batch 52/100: Batch Loss = 1.2352\n",
            "Iteration 18, Batch 53/100: Batch Loss = 1.2147\n",
            "Iteration 18, Batch 54/100: Batch Loss = 1.3496\n",
            "Iteration 18, Batch 55/100: Batch Loss = 1.1388\n",
            "Iteration 18, Batch 56/100: Batch Loss = 1.3708\n",
            "Iteration 18, Batch 57/100: Batch Loss = 1.3292\n",
            "Iteration 18, Batch 58/100: Batch Loss = 1.1859\n",
            "Iteration 18, Batch 59/100: Batch Loss = 1.2879\n",
            "Iteration 18, Batch 60/100: Batch Loss = 1.2707\n",
            "Iteration 18, Batch 61/100: Batch Loss = 1.2613\n",
            "Iteration 18, Batch 62/100: Batch Loss = 1.1318\n",
            "Iteration 18, Batch 63/100: Batch Loss = 1.2654\n",
            "Iteration 18, Batch 64/100: Batch Loss = 1.2724\n",
            "Iteration 18, Batch 65/100: Batch Loss = 1.1765\n",
            "Iteration 18, Batch 66/100: Batch Loss = 1.2273\n",
            "Iteration 18, Batch 67/100: Batch Loss = 1.2896\n",
            "Iteration 18, Batch 68/100: Batch Loss = 1.2406\n",
            "Iteration 18, Batch 69/100: Batch Loss = 1.2326\n",
            "Iteration 18, Batch 70/100: Batch Loss = 1.2550\n",
            "Iteration 18, Batch 71/100: Batch Loss = 1.2178\n",
            "Iteration 18, Batch 72/100: Batch Loss = 1.2162\n",
            "Iteration 18, Batch 73/100: Batch Loss = 1.3425\n",
            "Iteration 18, Batch 74/100: Batch Loss = 1.2316\n",
            "Iteration 18, Batch 75/100: Batch Loss = 1.4064\n",
            "Iteration 18, Batch 76/100: Batch Loss = 1.3076\n",
            "Iteration 18, Batch 77/100: Batch Loss = 1.1833\n",
            "Iteration 18, Batch 78/100: Batch Loss = 1.2004\n",
            "Iteration 18, Batch 79/100: Batch Loss = 1.3727\n",
            "Iteration 18, Batch 80/100: Batch Loss = 1.3815\n",
            "Iteration 18, Batch 81/100: Batch Loss = 1.3049\n",
            "Iteration 18, Batch 82/100: Batch Loss = 1.3475\n",
            "Iteration 18, Batch 83/100: Batch Loss = 1.3516\n",
            "Iteration 18, Batch 84/100: Batch Loss = 1.1901\n",
            "Iteration 18, Batch 85/100: Batch Loss = 1.3579\n",
            "Iteration 18, Batch 86/100: Batch Loss = 1.2194\n",
            "Iteration 18, Batch 87/100: Batch Loss = 1.3453\n",
            "Iteration 18, Batch 88/100: Batch Loss = 1.2789\n",
            "Iteration 18, Batch 89/100: Batch Loss = 1.2603\n",
            "Iteration 18, Batch 90/100: Batch Loss = 1.2254\n",
            "Iteration 18, Batch 91/100: Batch Loss = 1.2900\n",
            "Iteration 18, Batch 92/100: Batch Loss = 1.3018\n",
            "Iteration 18, Batch 93/100: Batch Loss = 1.3091\n",
            "Iteration 18, Batch 94/100: Batch Loss = 1.2671\n",
            "Iteration 18, Batch 95/100: Batch Loss = 1.2198\n",
            "Iteration 18, Batch 96/100: Batch Loss = 1.3578\n",
            "Iteration 18, Batch 97/100: Batch Loss = 1.2631\n",
            "Iteration 18, Batch 98/100: Batch Loss = 1.2452\n",
            "Iteration 18, Batch 99/100: Batch Loss = 1.2861\n",
            "Iteration 18, Batch 100/100: Batch Loss = 1.3373\n",
            "Iteration 18: Train Loss = 1.2815, Val Loss = 1.2791\n",
            "Iteration 19, Batch 1/100: Batch Loss = 1.2225\n",
            "Iteration 19, Batch 2/100: Batch Loss = 1.3648\n",
            "Iteration 19, Batch 3/100: Batch Loss = 1.2911\n",
            "Iteration 19, Batch 4/100: Batch Loss = 1.1340\n",
            "Iteration 19, Batch 5/100: Batch Loss = 1.1596\n",
            "Iteration 19, Batch 6/100: Batch Loss = 1.1684\n",
            "Iteration 19, Batch 7/100: Batch Loss = 1.2040\n",
            "Iteration 19, Batch 8/100: Batch Loss = 1.2493\n",
            "Iteration 19, Batch 9/100: Batch Loss = 1.3418\n",
            "Iteration 19, Batch 10/100: Batch Loss = 1.1582\n",
            "Iteration 19, Batch 11/100: Batch Loss = 1.4966\n",
            "Iteration 19, Batch 12/100: Batch Loss = 1.2983\n",
            "Iteration 19, Batch 13/100: Batch Loss = 1.2742\n",
            "Iteration 19, Batch 14/100: Batch Loss = 1.1759\n",
            "Iteration 19, Batch 15/100: Batch Loss = 1.2824\n",
            "Iteration 19, Batch 16/100: Batch Loss = 1.2717\n",
            "Iteration 19, Batch 17/100: Batch Loss = 1.2747\n",
            "Iteration 19, Batch 18/100: Batch Loss = 1.2947\n",
            "Iteration 19, Batch 19/100: Batch Loss = 1.3016\n",
            "Iteration 19, Batch 20/100: Batch Loss = 1.4509\n",
            "Iteration 19, Batch 21/100: Batch Loss = 1.2232\n",
            "Iteration 19, Batch 22/100: Batch Loss = 1.1944\n",
            "Iteration 19, Batch 23/100: Batch Loss = 1.3234\n",
            "Iteration 19, Batch 24/100: Batch Loss = 1.2616\n",
            "Iteration 19, Batch 25/100: Batch Loss = 1.2538\n",
            "Iteration 19, Batch 26/100: Batch Loss = 1.3155\n",
            "Iteration 19, Batch 27/100: Batch Loss = 1.3614\n",
            "Iteration 19, Batch 28/100: Batch Loss = 1.2799\n",
            "Iteration 19, Batch 29/100: Batch Loss = 1.3430\n",
            "Iteration 19, Batch 30/100: Batch Loss = 1.2245\n",
            "Iteration 19, Batch 31/100: Batch Loss = 1.1668\n",
            "Iteration 19, Batch 32/100: Batch Loss = 1.2669\n",
            "Iteration 19, Batch 33/100: Batch Loss = 1.3409\n",
            "Iteration 19, Batch 34/100: Batch Loss = 1.2670\n",
            "Iteration 19, Batch 35/100: Batch Loss = 1.3242\n",
            "Iteration 19, Batch 36/100: Batch Loss = 1.2518\n",
            "Iteration 19, Batch 37/100: Batch Loss = 1.2949\n",
            "Iteration 19, Batch 38/100: Batch Loss = 1.1818\n",
            "Iteration 19, Batch 39/100: Batch Loss = 1.3767\n",
            "Iteration 19, Batch 40/100: Batch Loss = 1.2597\n",
            "Iteration 19, Batch 41/100: Batch Loss = 1.3558\n",
            "Iteration 19, Batch 42/100: Batch Loss = 1.1688\n",
            "Iteration 19, Batch 43/100: Batch Loss = 1.2317\n",
            "Iteration 19, Batch 44/100: Batch Loss = 1.2947\n",
            "Iteration 19, Batch 45/100: Batch Loss = 1.2577\n",
            "Iteration 19, Batch 46/100: Batch Loss = 1.2446\n",
            "Iteration 19, Batch 47/100: Batch Loss = 1.3523\n",
            "Iteration 19, Batch 48/100: Batch Loss = 1.3485\n",
            "Iteration 19, Batch 49/100: Batch Loss = 1.3022\n",
            "Iteration 19, Batch 50/100: Batch Loss = 1.2501\n",
            "Iteration 19, Batch 51/100: Batch Loss = 1.2036\n",
            "Iteration 19, Batch 52/100: Batch Loss = 1.3080\n",
            "Iteration 19, Batch 53/100: Batch Loss = 1.3746\n",
            "Iteration 19, Batch 54/100: Batch Loss = 1.2931\n",
            "Iteration 19, Batch 55/100: Batch Loss = 1.2375\n",
            "Iteration 19, Batch 56/100: Batch Loss = 1.2054\n",
            "Iteration 19, Batch 57/100: Batch Loss = 1.3840\n",
            "Iteration 19, Batch 58/100: Batch Loss = 1.2431\n",
            "Iteration 19, Batch 59/100: Batch Loss = 1.3986\n",
            "Iteration 19, Batch 60/100: Batch Loss = 1.2630\n",
            "Iteration 19, Batch 61/100: Batch Loss = 1.2879\n",
            "Iteration 19, Batch 62/100: Batch Loss = 1.3077\n",
            "Iteration 19, Batch 63/100: Batch Loss = 1.3023\n",
            "Iteration 19, Batch 64/100: Batch Loss = 1.2206\n",
            "Iteration 19, Batch 65/100: Batch Loss = 1.2185\n",
            "Iteration 19, Batch 66/100: Batch Loss = 1.3761\n",
            "Iteration 19, Batch 67/100: Batch Loss = 1.2757\n",
            "Iteration 19, Batch 68/100: Batch Loss = 1.3693\n",
            "Iteration 19, Batch 69/100: Batch Loss = 1.3636\n",
            "Iteration 19, Batch 70/100: Batch Loss = 1.3297\n",
            "Iteration 19, Batch 71/100: Batch Loss = 1.2443\n",
            "Iteration 19, Batch 72/100: Batch Loss = 1.3262\n",
            "Iteration 19, Batch 73/100: Batch Loss = 1.2860\n",
            "Iteration 19, Batch 74/100: Batch Loss = 1.2045\n",
            "Iteration 19, Batch 75/100: Batch Loss = 1.2985\n",
            "Iteration 19, Batch 76/100: Batch Loss = 1.2520\n",
            "Iteration 19, Batch 77/100: Batch Loss = 1.1818\n",
            "Iteration 19, Batch 78/100: Batch Loss = 1.3749\n",
            "Iteration 19, Batch 79/100: Batch Loss = 1.2801\n",
            "Iteration 19, Batch 80/100: Batch Loss = 1.2223\n",
            "Iteration 19, Batch 81/100: Batch Loss = 1.5027\n",
            "Iteration 19, Batch 82/100: Batch Loss = 1.2392\n",
            "Iteration 19, Batch 83/100: Batch Loss = 1.3708\n",
            "Iteration 19, Batch 84/100: Batch Loss = 1.1317\n",
            "Iteration 19, Batch 85/100: Batch Loss = 1.2854\n",
            "Iteration 19, Batch 86/100: Batch Loss = 1.3246\n",
            "Iteration 19, Batch 87/100: Batch Loss = 1.2689\n",
            "Iteration 19, Batch 88/100: Batch Loss = 1.2225\n",
            "Iteration 19, Batch 89/100: Batch Loss = 1.3263\n",
            "Iteration 19, Batch 90/100: Batch Loss = 1.2131\n",
            "Iteration 19, Batch 91/100: Batch Loss = 1.2112\n",
            "Iteration 19, Batch 92/100: Batch Loss = 1.3336\n",
            "Iteration 19, Batch 93/100: Batch Loss = 1.3837\n",
            "Iteration 19, Batch 94/100: Batch Loss = 1.2855\n",
            "Iteration 19, Batch 95/100: Batch Loss = 1.2763\n",
            "Iteration 19, Batch 96/100: Batch Loss = 1.2042\n",
            "Iteration 19, Batch 97/100: Batch Loss = 1.3875\n",
            "Iteration 19, Batch 98/100: Batch Loss = 1.3150\n",
            "Iteration 19, Batch 99/100: Batch Loss = 1.2540\n",
            "Iteration 19, Batch 100/100: Batch Loss = 1.2448\n",
            "Iteration 19: Train Loss = 1.2815, Val Loss = 1.2790\n",
            "Iteration 20, Batch 1/100: Batch Loss = 1.1408\n",
            "Iteration 20, Batch 2/100: Batch Loss = 1.2984\n",
            "Iteration 20, Batch 3/100: Batch Loss = 1.1591\n",
            "Iteration 20, Batch 4/100: Batch Loss = 1.1792\n",
            "Iteration 20, Batch 5/100: Batch Loss = 1.2704\n",
            "Iteration 20, Batch 6/100: Batch Loss = 1.2890\n",
            "Iteration 20, Batch 7/100: Batch Loss = 1.1210\n",
            "Iteration 20, Batch 8/100: Batch Loss = 1.2262\n",
            "Iteration 20, Batch 9/100: Batch Loss = 1.2440\n",
            "Iteration 20, Batch 10/100: Batch Loss = 1.2898\n",
            "Iteration 20, Batch 11/100: Batch Loss = 1.2728\n",
            "Iteration 20, Batch 12/100: Batch Loss = 1.2514\n",
            "Iteration 20, Batch 13/100: Batch Loss = 1.2282\n",
            "Iteration 20, Batch 14/100: Batch Loss = 1.3680\n",
            "Iteration 20, Batch 15/100: Batch Loss = 1.3583\n",
            "Iteration 20, Batch 16/100: Batch Loss = 1.4217\n",
            "Iteration 20, Batch 17/100: Batch Loss = 1.3340\n",
            "Iteration 20, Batch 18/100: Batch Loss = 1.1214\n",
            "Iteration 20, Batch 19/100: Batch Loss = 1.3042\n",
            "Iteration 20, Batch 20/100: Batch Loss = 1.2518\n",
            "Iteration 20, Batch 21/100: Batch Loss = 1.3114\n",
            "Iteration 20, Batch 22/100: Batch Loss = 1.3019\n",
            "Iteration 20, Batch 23/100: Batch Loss = 1.3871\n",
            "Iteration 20, Batch 24/100: Batch Loss = 1.2516\n",
            "Iteration 20, Batch 25/100: Batch Loss = 1.3243\n",
            "Iteration 20, Batch 26/100: Batch Loss = 1.2600\n",
            "Iteration 20, Batch 27/100: Batch Loss = 1.3338\n",
            "Iteration 20, Batch 28/100: Batch Loss = 1.2093\n",
            "Iteration 20, Batch 29/100: Batch Loss = 1.3374\n",
            "Iteration 20, Batch 30/100: Batch Loss = 1.2691\n",
            "Iteration 20, Batch 31/100: Batch Loss = 1.3583\n",
            "Iteration 20, Batch 32/100: Batch Loss = 1.2798\n",
            "Iteration 20, Batch 33/100: Batch Loss = 1.2984\n",
            "Iteration 20, Batch 34/100: Batch Loss = 1.1941\n",
            "Iteration 20, Batch 35/100: Batch Loss = 1.3019\n",
            "Iteration 20, Batch 36/100: Batch Loss = 1.3655\n",
            "Iteration 20, Batch 37/100: Batch Loss = 1.3059\n",
            "Iteration 20, Batch 38/100: Batch Loss = 1.2985\n",
            "Iteration 20, Batch 39/100: Batch Loss = 1.2446\n",
            "Iteration 20, Batch 40/100: Batch Loss = 1.2668\n",
            "Iteration 20, Batch 41/100: Batch Loss = 1.2329\n",
            "Iteration 20, Batch 42/100: Batch Loss = 1.4405\n",
            "Iteration 20, Batch 43/100: Batch Loss = 1.3113\n",
            "Iteration 20, Batch 44/100: Batch Loss = 1.3182\n",
            "Iteration 20, Batch 45/100: Batch Loss = 1.2318\n",
            "Iteration 20, Batch 46/100: Batch Loss = 1.3363\n",
            "Iteration 20, Batch 47/100: Batch Loss = 1.2446\n",
            "Iteration 20, Batch 48/100: Batch Loss = 1.2258\n",
            "Iteration 20, Batch 49/100: Batch Loss = 1.3466\n",
            "Iteration 20, Batch 50/100: Batch Loss = 1.2635\n",
            "Iteration 20, Batch 51/100: Batch Loss = 1.2353\n",
            "Iteration 20, Batch 52/100: Batch Loss = 1.2797\n",
            "Iteration 20, Batch 53/100: Batch Loss = 1.2950\n",
            "Iteration 20, Batch 54/100: Batch Loss = 1.3290\n",
            "Iteration 20, Batch 55/100: Batch Loss = 1.3969\n",
            "Iteration 20, Batch 56/100: Batch Loss = 1.2946\n",
            "Iteration 20, Batch 57/100: Batch Loss = 1.2668\n",
            "Iteration 20, Batch 58/100: Batch Loss = 1.2225\n",
            "Iteration 20, Batch 59/100: Batch Loss = 1.2389\n",
            "Iteration 20, Batch 60/100: Batch Loss = 1.3557\n",
            "Iteration 20, Batch 61/100: Batch Loss = 1.2886\n",
            "Iteration 20, Batch 62/100: Batch Loss = 1.4091\n",
            "Iteration 20, Batch 63/100: Batch Loss = 1.2819\n",
            "Iteration 20, Batch 64/100: Batch Loss = 1.1553\n",
            "Iteration 20, Batch 65/100: Batch Loss = 1.1948\n",
            "Iteration 20, Batch 66/100: Batch Loss = 1.2342\n",
            "Iteration 20, Batch 67/100: Batch Loss = 1.2169\n",
            "Iteration 20, Batch 68/100: Batch Loss = 1.2441\n",
            "Iteration 20, Batch 69/100: Batch Loss = 1.2167\n",
            "Iteration 20, Batch 70/100: Batch Loss = 1.4242\n",
            "Iteration 20, Batch 71/100: Batch Loss = 1.2626\n",
            "Iteration 20, Batch 72/100: Batch Loss = 1.2440\n",
            "Iteration 20, Batch 73/100: Batch Loss = 1.2346\n",
            "Iteration 20, Batch 74/100: Batch Loss = 1.4035\n",
            "Iteration 20, Batch 75/100: Batch Loss = 1.2263\n",
            "Iteration 20, Batch 76/100: Batch Loss = 1.4213\n",
            "Iteration 20, Batch 77/100: Batch Loss = 1.1998\n",
            "Iteration 20, Batch 78/100: Batch Loss = 1.1978\n",
            "Iteration 20, Batch 79/100: Batch Loss = 1.3638\n",
            "Iteration 20, Batch 80/100: Batch Loss = 1.4477\n",
            "Iteration 20, Batch 81/100: Batch Loss = 1.2804\n",
            "Iteration 20, Batch 82/100: Batch Loss = 1.2889\n",
            "Iteration 20, Batch 83/100: Batch Loss = 1.3213\n",
            "Iteration 20, Batch 84/100: Batch Loss = 1.1999\n",
            "Iteration 20, Batch 85/100: Batch Loss = 1.3506\n",
            "Iteration 20, Batch 86/100: Batch Loss = 1.1815\n",
            "Iteration 20, Batch 87/100: Batch Loss = 1.2874\n",
            "Iteration 20, Batch 88/100: Batch Loss = 1.2620\n",
            "Iteration 20, Batch 89/100: Batch Loss = 1.1348\n",
            "Iteration 20, Batch 90/100: Batch Loss = 1.3136\n",
            "Iteration 20, Batch 91/100: Batch Loss = 1.3211\n",
            "Iteration 20, Batch 92/100: Batch Loss = 1.3078\n",
            "Iteration 20, Batch 93/100: Batch Loss = 1.3188\n",
            "Iteration 20, Batch 94/100: Batch Loss = 1.2350\n",
            "Iteration 20, Batch 95/100: Batch Loss = 1.3280\n",
            "Iteration 20, Batch 96/100: Batch Loss = 1.3176\n",
            "Iteration 20, Batch 97/100: Batch Loss = 1.2985\n",
            "Iteration 20, Batch 98/100: Batch Loss = 1.2422\n",
            "Iteration 20, Batch 99/100: Batch Loss = 1.2154\n",
            "Iteration 20, Batch 100/100: Batch Loss = 1.3748\n",
            "Iteration 20: Train Loss = 1.2814, Val Loss = 1.2790\n",
            "Iteration 21, Batch 1/100: Batch Loss = 1.2336\n",
            "Iteration 21, Batch 2/100: Batch Loss = 1.3221\n",
            "Iteration 21, Batch 3/100: Batch Loss = 1.3021\n",
            "Iteration 21, Batch 4/100: Batch Loss = 1.2025\n",
            "Iteration 21, Batch 5/100: Batch Loss = 1.2092\n",
            "Iteration 21, Batch 6/100: Batch Loss = 1.4476\n",
            "Iteration 21, Batch 7/100: Batch Loss = 1.4415\n",
            "Iteration 21, Batch 8/100: Batch Loss = 1.3235\n",
            "Iteration 21, Batch 9/100: Batch Loss = 1.2506\n",
            "Iteration 21, Batch 10/100: Batch Loss = 1.1805\n",
            "Iteration 21, Batch 11/100: Batch Loss = 1.2543\n",
            "Iteration 21, Batch 12/100: Batch Loss = 1.2165\n",
            "Iteration 21, Batch 13/100: Batch Loss = 1.2479\n",
            "Iteration 21, Batch 14/100: Batch Loss = 1.3120\n",
            "Iteration 21, Batch 15/100: Batch Loss = 1.1646\n",
            "Iteration 21, Batch 16/100: Batch Loss = 1.2627\n",
            "Iteration 21, Batch 17/100: Batch Loss = 1.2761\n",
            "Iteration 21, Batch 18/100: Batch Loss = 1.3775\n",
            "Iteration 21, Batch 19/100: Batch Loss = 1.2818\n",
            "Iteration 21, Batch 20/100: Batch Loss = 1.1889\n",
            "Iteration 21, Batch 21/100: Batch Loss = 1.3678\n",
            "Iteration 21, Batch 22/100: Batch Loss = 1.2515\n",
            "Iteration 21, Batch 23/100: Batch Loss = 1.2763\n",
            "Iteration 21, Batch 24/100: Batch Loss = 1.2797\n",
            "Iteration 21, Batch 25/100: Batch Loss = 1.2316\n",
            "Iteration 21, Batch 26/100: Batch Loss = 1.2033\n",
            "Iteration 21, Batch 27/100: Batch Loss = 1.3455\n",
            "Iteration 21, Batch 28/100: Batch Loss = 1.1864\n",
            "Iteration 21, Batch 29/100: Batch Loss = 1.3173\n",
            "Iteration 21, Batch 30/100: Batch Loss = 1.4127\n",
            "Iteration 21, Batch 31/100: Batch Loss = 1.3115\n",
            "Iteration 21, Batch 32/100: Batch Loss = 1.2652\n",
            "Iteration 21, Batch 33/100: Batch Loss = 1.3059\n",
            "Iteration 21, Batch 34/100: Batch Loss = 1.2516\n",
            "Iteration 21, Batch 35/100: Batch Loss = 1.3750\n",
            "Iteration 21, Batch 36/100: Batch Loss = 1.1849\n",
            "Iteration 21, Batch 37/100: Batch Loss = 1.3339\n",
            "Iteration 21, Batch 38/100: Batch Loss = 1.2739\n",
            "Iteration 21, Batch 39/100: Batch Loss = 1.2384\n",
            "Iteration 21, Batch 40/100: Batch Loss = 1.3620\n",
            "Iteration 21, Batch 41/100: Batch Loss = 1.2094\n",
            "Iteration 21, Batch 42/100: Batch Loss = 1.1551\n",
            "Iteration 21, Batch 43/100: Batch Loss = 1.2291\n",
            "Iteration 21, Batch 44/100: Batch Loss = 1.1584\n",
            "Iteration 21, Batch 45/100: Batch Loss = 1.2277\n",
            "Iteration 21, Batch 46/100: Batch Loss = 1.3529\n",
            "Iteration 21, Batch 47/100: Batch Loss = 1.1428\n",
            "Iteration 21, Batch 48/100: Batch Loss = 1.3907\n",
            "Iteration 21, Batch 49/100: Batch Loss = 1.1841\n",
            "Iteration 21, Batch 50/100: Batch Loss = 1.2534\n",
            "Iteration 21, Batch 51/100: Batch Loss = 1.2669\n",
            "Iteration 21, Batch 52/100: Batch Loss = 1.2796\n",
            "Iteration 21, Batch 53/100: Batch Loss = 1.2928\n",
            "Iteration 21, Batch 54/100: Batch Loss = 1.2573\n",
            "Iteration 21, Batch 55/100: Batch Loss = 1.2577\n",
            "Iteration 21, Batch 56/100: Batch Loss = 1.3566\n",
            "Iteration 21, Batch 57/100: Batch Loss = 1.3021\n",
            "Iteration 21, Batch 58/100: Batch Loss = 1.4582\n",
            "Iteration 21, Batch 59/100: Batch Loss = 1.1368\n",
            "Iteration 21, Batch 60/100: Batch Loss = 1.2796\n",
            "Iteration 21, Batch 61/100: Batch Loss = 1.2444\n",
            "Iteration 21, Batch 62/100: Batch Loss = 1.2667\n",
            "Iteration 21, Batch 63/100: Batch Loss = 1.3184\n",
            "Iteration 21, Batch 64/100: Batch Loss = 1.2923\n",
            "Iteration 21, Batch 65/100: Batch Loss = 1.2766\n",
            "Iteration 21, Batch 66/100: Batch Loss = 1.3019\n",
            "Iteration 21, Batch 67/100: Batch Loss = 1.3942\n",
            "Iteration 21, Batch 68/100: Batch Loss = 1.2161\n",
            "Iteration 21, Batch 69/100: Batch Loss = 1.2857\n",
            "Iteration 21, Batch 70/100: Batch Loss = 1.2640\n",
            "Iteration 21, Batch 71/100: Batch Loss = 1.2030\n",
            "Iteration 21, Batch 72/100: Batch Loss = 1.2858\n",
            "Iteration 21, Batch 73/100: Batch Loss = 1.3653\n",
            "Iteration 21, Batch 74/100: Batch Loss = 1.2704\n",
            "Iteration 21, Batch 75/100: Batch Loss = 1.2092\n",
            "Iteration 21, Batch 76/100: Batch Loss = 1.2762\n",
            "Iteration 21, Batch 77/100: Batch Loss = 1.3556\n",
            "Iteration 21, Batch 78/100: Batch Loss = 1.3624\n",
            "Iteration 21, Batch 79/100: Batch Loss = 1.2892\n",
            "Iteration 21, Batch 80/100: Batch Loss = 1.2951\n",
            "Iteration 21, Batch 81/100: Batch Loss = 1.4095\n",
            "Iteration 21, Batch 82/100: Batch Loss = 1.3949\n",
            "Iteration 21, Batch 83/100: Batch Loss = 1.3224\n",
            "Iteration 21, Batch 84/100: Batch Loss = 1.2163\n",
            "Iteration 21, Batch 85/100: Batch Loss = 1.3111\n",
            "Iteration 21, Batch 86/100: Batch Loss = 1.1713\n",
            "Iteration 21, Batch 87/100: Batch Loss = 1.3493\n",
            "Iteration 21, Batch 88/100: Batch Loss = 1.2631\n",
            "Iteration 21, Batch 89/100: Batch Loss = 1.2549\n",
            "Iteration 21, Batch 90/100: Batch Loss = 1.2892\n",
            "Iteration 21, Batch 91/100: Batch Loss = 1.3245\n",
            "Iteration 21, Batch 92/100: Batch Loss = 1.3778\n",
            "Iteration 21, Batch 93/100: Batch Loss = 1.3370\n",
            "Iteration 21, Batch 94/100: Batch Loss = 1.2543\n",
            "Iteration 21, Batch 95/100: Batch Loss = 1.3495\n",
            "Iteration 21, Batch 96/100: Batch Loss = 1.1778\n",
            "Iteration 21, Batch 97/100: Batch Loss = 1.2666\n",
            "Iteration 21, Batch 98/100: Batch Loss = 1.3965\n",
            "Iteration 21, Batch 99/100: Batch Loss = 1.1644\n",
            "Iteration 21, Batch 100/100: Batch Loss = 1.2765\n",
            "Iteration 21: Train Loss = 1.2814, Val Loss = 1.2789\n",
            "Iteration 22, Batch 1/100: Batch Loss = 1.2440\n",
            "Iteration 22, Batch 2/100: Batch Loss = 1.2704\n",
            "Iteration 22, Batch 3/100: Batch Loss = 1.3209\n",
            "Iteration 22, Batch 4/100: Batch Loss = 1.3533\n",
            "Iteration 22, Batch 5/100: Batch Loss = 1.2291\n",
            "Iteration 22, Batch 6/100: Batch Loss = 1.3054\n",
            "Iteration 22, Batch 7/100: Batch Loss = 1.3018\n",
            "Iteration 22, Batch 8/100: Batch Loss = 1.2730\n",
            "Iteration 22, Batch 9/100: Batch Loss = 1.3206\n",
            "Iteration 22, Batch 10/100: Batch Loss = 1.3269\n",
            "Iteration 22, Batch 11/100: Batch Loss = 1.2128\n",
            "Iteration 22, Batch 12/100: Batch Loss = 1.3339\n",
            "Iteration 22, Batch 13/100: Batch Loss = 1.3080\n",
            "Iteration 22, Batch 14/100: Batch Loss = 1.2960\n",
            "Iteration 22, Batch 15/100: Batch Loss = 1.2550\n",
            "Iteration 22, Batch 16/100: Batch Loss = 1.2891\n",
            "Iteration 22, Batch 17/100: Batch Loss = 1.2194\n",
            "Iteration 22, Batch 18/100: Batch Loss = 1.2573\n",
            "Iteration 22, Batch 19/100: Batch Loss = 1.3369\n",
            "Iteration 22, Batch 20/100: Batch Loss = 1.2859\n",
            "Iteration 22, Batch 21/100: Batch Loss = 1.2096\n",
            "Iteration 22, Batch 22/100: Batch Loss = 1.2413\n",
            "Iteration 22, Batch 23/100: Batch Loss = 1.1837\n",
            "Iteration 22, Batch 24/100: Batch Loss = 1.3302\n",
            "Iteration 22, Batch 25/100: Batch Loss = 1.2254\n",
            "Iteration 22, Batch 26/100: Batch Loss = 1.4006\n",
            "Iteration 22, Batch 27/100: Batch Loss = 1.2604\n",
            "Iteration 22, Batch 28/100: Batch Loss = 1.2859\n",
            "Iteration 22, Batch 29/100: Batch Loss = 1.2446\n",
            "Iteration 22, Batch 30/100: Batch Loss = 1.2920\n",
            "Iteration 22, Batch 31/100: Batch Loss = 1.2736\n",
            "Iteration 22, Batch 32/100: Batch Loss = 1.2796\n",
            "Iteration 22, Batch 33/100: Batch Loss = 1.2121\n",
            "Iteration 22, Batch 34/100: Batch Loss = 1.2440\n",
            "Iteration 22, Batch 35/100: Batch Loss = 1.2636\n",
            "Iteration 22, Batch 36/100: Batch Loss = 1.3147\n",
            "Iteration 22, Batch 37/100: Batch Loss = 1.2445\n",
            "Iteration 22, Batch 38/100: Batch Loss = 1.2513\n",
            "Iteration 22, Batch 39/100: Batch Loss = 1.3121\n",
            "Iteration 22, Batch 40/100: Batch Loss = 1.3312\n",
            "Iteration 22, Batch 41/100: Batch Loss = 1.3591\n",
            "Iteration 22, Batch 42/100: Batch Loss = 1.2923\n",
            "Iteration 22, Batch 43/100: Batch Loss = 1.1937\n",
            "Iteration 22, Batch 44/100: Batch Loss = 1.2573\n",
            "Iteration 22, Batch 45/100: Batch Loss = 1.4834\n",
            "Iteration 22, Batch 46/100: Batch Loss = 1.3847\n",
            "Iteration 22, Batch 47/100: Batch Loss = 1.4031\n",
            "Iteration 22, Batch 48/100: Batch Loss = 1.2893\n",
            "Iteration 22, Batch 49/100: Batch Loss = 1.2953\n",
            "Iteration 22, Batch 50/100: Batch Loss = 1.3331\n",
            "Iteration 22, Batch 51/100: Batch Loss = 1.3724\n",
            "Iteration 22, Batch 52/100: Batch Loss = 1.2258\n",
            "Iteration 22, Batch 53/100: Batch Loss = 1.2666\n",
            "Iteration 22, Batch 54/100: Batch Loss = 1.2705\n",
            "Iteration 22, Batch 55/100: Batch Loss = 1.2259\n",
            "Iteration 22, Batch 56/100: Batch Loss = 1.2443\n",
            "Iteration 22, Batch 57/100: Batch Loss = 1.3687\n",
            "Iteration 22, Batch 58/100: Batch Loss = 1.3081\n",
            "Iteration 22, Batch 59/100: Batch Loss = 1.2288\n",
            "Iteration 22, Batch 60/100: Batch Loss = 1.2860\n",
            "Iteration 22, Batch 61/100: Batch Loss = 1.4100\n",
            "Iteration 22, Batch 62/100: Batch Loss = 1.1781\n",
            "Iteration 22, Batch 63/100: Batch Loss = 1.2577\n",
            "Iteration 22, Batch 64/100: Batch Loss = 1.2764\n",
            "Iteration 22, Batch 65/100: Batch Loss = 1.1589\n",
            "Iteration 22, Batch 66/100: Batch Loss = 1.3149\n",
            "Iteration 22, Batch 67/100: Batch Loss = 1.2572\n",
            "Iteration 22, Batch 68/100: Batch Loss = 1.3268\n",
            "Iteration 22, Batch 69/100: Batch Loss = 1.2278\n",
            "Iteration 22, Batch 70/100: Batch Loss = 1.2259\n",
            "Iteration 22, Batch 71/100: Batch Loss = 1.2627\n",
            "Iteration 22, Batch 72/100: Batch Loss = 1.2532\n",
            "Iteration 22, Batch 73/100: Batch Loss = 1.2085\n",
            "Iteration 22, Batch 74/100: Batch Loss = 1.1304\n",
            "Iteration 22, Batch 75/100: Batch Loss = 1.1431\n",
            "Iteration 22, Batch 76/100: Batch Loss = 1.3871\n",
            "Iteration 22, Batch 77/100: Batch Loss = 1.2982\n",
            "Iteration 22, Batch 78/100: Batch Loss = 1.2166\n",
            "Iteration 22, Batch 79/100: Batch Loss = 1.2213\n",
            "Iteration 22, Batch 80/100: Batch Loss = 1.2442\n",
            "Iteration 22, Batch 81/100: Batch Loss = 1.3456\n",
            "Iteration 22, Batch 82/100: Batch Loss = 1.2513\n",
            "Iteration 22, Batch 83/100: Batch Loss = 1.1930\n",
            "Iteration 22, Batch 84/100: Batch Loss = 1.3072\n",
            "Iteration 22, Batch 85/100: Batch Loss = 1.3049\n",
            "Iteration 22, Batch 86/100: Batch Loss = 1.1351\n",
            "Iteration 22, Batch 87/100: Batch Loss = 1.3397\n",
            "Iteration 22, Batch 88/100: Batch Loss = 1.3836\n",
            "Iteration 22, Batch 89/100: Batch Loss = 1.3939\n",
            "Iteration 22, Batch 90/100: Batch Loss = 1.3048\n",
            "Iteration 22, Batch 91/100: Batch Loss = 1.4020\n",
            "Iteration 22, Batch 92/100: Batch Loss = 1.3340\n",
            "Iteration 22, Batch 93/100: Batch Loss = 1.2466\n",
            "Iteration 22, Batch 94/100: Batch Loss = 1.3087\n",
            "Iteration 22, Batch 95/100: Batch Loss = 1.3020\n",
            "Iteration 22, Batch 96/100: Batch Loss = 1.3118\n",
            "Iteration 22, Batch 97/100: Batch Loss = 1.3140\n",
            "Iteration 22, Batch 98/100: Batch Loss = 1.2836\n",
            "Iteration 22, Batch 99/100: Batch Loss = 1.3181\n",
            "Iteration 22, Batch 100/100: Batch Loss = 1.1297\n",
            "Iteration 22: Train Loss = 1.2814, Val Loss = 1.2789\n",
            "Iteration 23, Batch 1/100: Batch Loss = 1.3586\n",
            "Iteration 23, Batch 2/100: Batch Loss = 1.3034\n",
            "Iteration 23, Batch 3/100: Batch Loss = 1.3261\n",
            "Iteration 23, Batch 4/100: Batch Loss = 1.3985\n",
            "Iteration 23, Batch 5/100: Batch Loss = 1.3179\n",
            "Iteration 23, Batch 6/100: Batch Loss = 1.3278\n",
            "Iteration 23, Batch 7/100: Batch Loss = 1.2484\n",
            "Iteration 23, Batch 8/100: Batch Loss = 1.3544\n",
            "Iteration 23, Batch 9/100: Batch Loss = 1.1803\n",
            "Iteration 23, Batch 10/100: Batch Loss = 1.2656\n",
            "Iteration 23, Batch 11/100: Batch Loss = 1.2063\n",
            "Iteration 23, Batch 12/100: Batch Loss = 1.2140\n",
            "Iteration 23, Batch 13/100: Batch Loss = 1.2643\n",
            "Iteration 23, Batch 14/100: Batch Loss = 1.2005\n",
            "Iteration 23, Batch 15/100: Batch Loss = 1.2793\n",
            "Iteration 23, Batch 16/100: Batch Loss = 1.1449\n",
            "Iteration 23, Batch 17/100: Batch Loss = 1.2307\n",
            "Iteration 23, Batch 18/100: Batch Loss = 1.2244\n",
            "Iteration 23, Batch 19/100: Batch Loss = 1.1298\n",
            "Iteration 23, Batch 20/100: Batch Loss = 1.3081\n",
            "Iteration 23, Batch 21/100: Batch Loss = 1.2119\n",
            "Iteration 23, Batch 22/100: Batch Loss = 1.4022\n",
            "Iteration 23, Batch 23/100: Batch Loss = 1.2318\n",
            "Iteration 23, Batch 24/100: Batch Loss = 1.3086\n",
            "Iteration 23, Batch 25/100: Batch Loss = 1.2474\n",
            "Iteration 23, Batch 26/100: Batch Loss = 1.3049\n",
            "Iteration 23, Batch 27/100: Batch Loss = 1.3304\n",
            "Iteration 23, Batch 28/100: Batch Loss = 1.1372\n",
            "Iteration 23, Batch 29/100: Batch Loss = 1.2088\n",
            "Iteration 23, Batch 30/100: Batch Loss = 1.1291\n",
            "Iteration 23, Batch 31/100: Batch Loss = 1.4851\n",
            "Iteration 23, Batch 32/100: Batch Loss = 1.2285\n",
            "Iteration 23, Batch 33/100: Batch Loss = 1.3212\n",
            "Iteration 23, Batch 34/100: Batch Loss = 1.1742\n",
            "Iteration 23, Batch 35/100: Batch Loss = 1.2855\n",
            "Iteration 23, Batch 36/100: Batch Loss = 1.3370\n",
            "Iteration 23, Batch 37/100: Batch Loss = 1.2739\n",
            "Iteration 23, Batch 38/100: Batch Loss = 1.3309\n",
            "Iteration 23, Batch 39/100: Batch Loss = 1.2891\n",
            "Iteration 23, Batch 40/100: Batch Loss = 1.3464\n",
            "Iteration 23, Batch 41/100: Batch Loss = 1.2498\n",
            "Iteration 23, Batch 42/100: Batch Loss = 1.2098\n",
            "Iteration 23, Batch 43/100: Batch Loss = 1.3497\n",
            "Iteration 23, Batch 44/100: Batch Loss = 1.2591\n",
            "Iteration 23, Batch 45/100: Batch Loss = 1.3213\n",
            "Iteration 23, Batch 46/100: Batch Loss = 1.2967\n",
            "Iteration 23, Batch 47/100: Batch Loss = 1.4078\n",
            "Iteration 23, Batch 48/100: Batch Loss = 1.3345\n",
            "Iteration 23, Batch 49/100: Batch Loss = 1.2290\n",
            "Iteration 23, Batch 50/100: Batch Loss = 1.2137\n",
            "Iteration 23, Batch 51/100: Batch Loss = 1.2758\n",
            "Iteration 23, Batch 52/100: Batch Loss = 1.3732\n",
            "Iteration 23, Batch 53/100: Batch Loss = 1.4525\n",
            "Iteration 23, Batch 54/100: Batch Loss = 1.3049\n",
            "Iteration 23, Batch 55/100: Batch Loss = 1.2387\n",
            "Iteration 23, Batch 56/100: Batch Loss = 1.1746\n",
            "Iteration 23, Batch 57/100: Batch Loss = 1.2286\n",
            "Iteration 23, Batch 58/100: Batch Loss = 1.2661\n",
            "Iteration 23, Batch 59/100: Batch Loss = 1.3240\n",
            "Iteration 23, Batch 60/100: Batch Loss = 1.2207\n",
            "Iteration 23, Batch 61/100: Batch Loss = 1.1957\n",
            "Iteration 23, Batch 62/100: Batch Loss = 1.2342\n",
            "Iteration 23, Batch 63/100: Batch Loss = 1.3700\n",
            "Iteration 23, Batch 64/100: Batch Loss = 1.3589\n",
            "Iteration 23, Batch 65/100: Batch Loss = 1.4296\n",
            "Iteration 23, Batch 66/100: Batch Loss = 1.3211\n",
            "Iteration 23, Batch 67/100: Batch Loss = 1.2343\n",
            "Iteration 23, Batch 68/100: Batch Loss = 1.2090\n",
            "Iteration 23, Batch 69/100: Batch Loss = 1.2757\n",
            "Iteration 23, Batch 70/100: Batch Loss = 1.3233\n",
            "Iteration 23, Batch 71/100: Batch Loss = 1.3131\n",
            "Iteration 23, Batch 72/100: Batch Loss = 1.3049\n",
            "Iteration 23, Batch 73/100: Batch Loss = 1.2242\n",
            "Iteration 23, Batch 74/100: Batch Loss = 1.2413\n",
            "Iteration 23, Batch 75/100: Batch Loss = 1.2716\n",
            "Iteration 23, Batch 76/100: Batch Loss = 1.2483\n",
            "Iteration 23, Batch 77/100: Batch Loss = 1.3253\n",
            "Iteration 23, Batch 78/100: Batch Loss = 1.4244\n",
            "Iteration 23, Batch 79/100: Batch Loss = 1.1898\n",
            "Iteration 23, Batch 80/100: Batch Loss = 1.3342\n",
            "Iteration 23, Batch 81/100: Batch Loss = 1.3534\n",
            "Iteration 23, Batch 82/100: Batch Loss = 1.3525\n",
            "Iteration 23, Batch 83/100: Batch Loss = 1.2473\n",
            "Iteration 23, Batch 84/100: Batch Loss = 1.1474\n",
            "Iteration 23, Batch 85/100: Batch Loss = 1.2746\n",
            "Iteration 23, Batch 86/100: Batch Loss = 1.2722\n",
            "Iteration 23, Batch 87/100: Batch Loss = 1.2390\n",
            "Iteration 23, Batch 88/100: Batch Loss = 1.3820\n",
            "Iteration 23, Batch 89/100: Batch Loss = 1.2873\n",
            "Iteration 23, Batch 90/100: Batch Loss = 1.4040\n",
            "Iteration 23, Batch 91/100: Batch Loss = 1.2589\n",
            "Iteration 23, Batch 92/100: Batch Loss = 1.2251\n",
            "Iteration 23, Batch 93/100: Batch Loss = 1.2222\n",
            "Iteration 23, Batch 94/100: Batch Loss = 1.3252\n",
            "Iteration 23, Batch 95/100: Batch Loss = 1.4285\n",
            "Iteration 23, Batch 96/100: Batch Loss = 1.2757\n",
            "Iteration 23, Batch 97/100: Batch Loss = 1.2422\n",
            "Iteration 23, Batch 98/100: Batch Loss = 1.2589\n",
            "Iteration 23, Batch 99/100: Batch Loss = 1.2851\n",
            "Iteration 23, Batch 100/100: Batch Loss = 1.2260\n",
            "Iteration 23: Train Loss = 1.2813, Val Loss = 1.2789\n",
            "Iteration 24, Batch 1/100: Batch Loss = 1.3488\n",
            "Iteration 24, Batch 2/100: Batch Loss = 1.1393\n",
            "Iteration 24, Batch 3/100: Batch Loss = 1.3559\n",
            "Iteration 24, Batch 4/100: Batch Loss = 1.2796\n",
            "Iteration 24, Batch 5/100: Batch Loss = 1.2333\n",
            "Iteration 24, Batch 6/100: Batch Loss = 1.2741\n",
            "Iteration 24, Batch 7/100: Batch Loss = 1.3272\n",
            "Iteration 24, Batch 8/100: Batch Loss = 1.4406\n",
            "Iteration 24, Batch 9/100: Batch Loss = 1.2867\n",
            "Iteration 24, Batch 10/100: Batch Loss = 1.3868\n",
            "Iteration 24, Batch 11/100: Batch Loss = 1.1058\n",
            "Iteration 24, Batch 12/100: Batch Loss = 1.3678\n",
            "Iteration 24, Batch 13/100: Batch Loss = 1.4207\n",
            "Iteration 24, Batch 14/100: Batch Loss = 1.1838\n",
            "Iteration 24, Batch 15/100: Batch Loss = 1.3035\n",
            "Iteration 24, Batch 16/100: Batch Loss = 1.3181\n",
            "Iteration 24, Batch 17/100: Batch Loss = 1.3567\n",
            "Iteration 24, Batch 18/100: Batch Loss = 1.3405\n",
            "Iteration 24, Batch 19/100: Batch Loss = 1.2953\n",
            "Iteration 24, Batch 20/100: Batch Loss = 1.2481\n",
            "Iteration 24, Batch 21/100: Batch Loss = 1.1394\n",
            "Iteration 24, Batch 22/100: Batch Loss = 1.1737\n",
            "Iteration 24, Batch 23/100: Batch Loss = 1.1894\n",
            "Iteration 24, Batch 24/100: Batch Loss = 1.2063\n",
            "Iteration 24, Batch 25/100: Batch Loss = 1.4073\n",
            "Iteration 24, Batch 26/100: Batch Loss = 1.2888\n",
            "Iteration 24, Batch 27/100: Batch Loss = 1.3075\n",
            "Iteration 24, Batch 28/100: Batch Loss = 1.5073\n",
            "Iteration 24, Batch 29/100: Batch Loss = 1.1172\n",
            "Iteration 24, Batch 30/100: Batch Loss = 1.2588\n",
            "Iteration 24, Batch 31/100: Batch Loss = 1.2841\n",
            "Iteration 24, Batch 32/100: Batch Loss = 1.1348\n",
            "Iteration 24, Batch 33/100: Batch Loss = 1.2646\n",
            "Iteration 24, Batch 34/100: Batch Loss = 1.4042\n",
            "Iteration 24, Batch 35/100: Batch Loss = 1.2272\n",
            "Iteration 24, Batch 36/100: Batch Loss = 1.4234\n",
            "Iteration 24, Batch 37/100: Batch Loss = 1.4130\n",
            "Iteration 24, Batch 38/100: Batch Loss = 1.2910\n",
            "Iteration 24, Batch 39/100: Batch Loss = 1.3392\n",
            "Iteration 24, Batch 40/100: Batch Loss = 1.2836\n",
            "Iteration 24, Batch 41/100: Batch Loss = 1.3255\n",
            "Iteration 24, Batch 42/100: Batch Loss = 1.2680\n",
            "Iteration 24, Batch 43/100: Batch Loss = 1.2693\n",
            "Iteration 24, Batch 44/100: Batch Loss = 1.3471\n",
            "Iteration 24, Batch 45/100: Batch Loss = 1.4579\n",
            "Iteration 24, Batch 46/100: Batch Loss = 1.3038\n",
            "Iteration 24, Batch 47/100: Batch Loss = 1.2154\n",
            "Iteration 24, Batch 48/100: Batch Loss = 1.2989\n",
            "Iteration 24, Batch 49/100: Batch Loss = 1.1593\n",
            "Iteration 24, Batch 50/100: Batch Loss = 1.2552\n",
            "Iteration 24, Batch 51/100: Batch Loss = 1.4379\n",
            "Iteration 24, Batch 52/100: Batch Loss = 1.2782\n",
            "Iteration 24, Batch 53/100: Batch Loss = 1.2485\n",
            "Iteration 24, Batch 54/100: Batch Loss = 1.1619\n",
            "Iteration 24, Batch 55/100: Batch Loss = 1.2748\n",
            "Iteration 24, Batch 56/100: Batch Loss = 1.3923\n",
            "Iteration 24, Batch 57/100: Batch Loss = 1.2333\n",
            "Iteration 24, Batch 58/100: Batch Loss = 1.2369\n",
            "Iteration 24, Batch 59/100: Batch Loss = 1.3170\n",
            "Iteration 24, Batch 60/100: Batch Loss = 1.2720\n",
            "Iteration 24, Batch 61/100: Batch Loss = 1.1788\n",
            "Iteration 24, Batch 62/100: Batch Loss = 1.3262\n",
            "Iteration 24, Batch 63/100: Batch Loss = 1.2007\n",
            "Iteration 24, Batch 64/100: Batch Loss = 1.2784\n",
            "Iteration 24, Batch 65/100: Batch Loss = 1.3054\n",
            "Iteration 24, Batch 66/100: Batch Loss = 1.3598\n",
            "Iteration 24, Batch 67/100: Batch Loss = 1.2032\n",
            "Iteration 24, Batch 68/100: Batch Loss = 1.3856\n",
            "Iteration 24, Batch 69/100: Batch Loss = 1.3111\n",
            "Iteration 24, Batch 70/100: Batch Loss = 1.2951\n",
            "Iteration 24, Batch 71/100: Batch Loss = 1.1607\n",
            "Iteration 24, Batch 72/100: Batch Loss = 1.1799\n",
            "Iteration 24, Batch 73/100: Batch Loss = 1.3003\n",
            "Iteration 24, Batch 74/100: Batch Loss = 1.3027\n",
            "Iteration 24, Batch 75/100: Batch Loss = 1.1577\n",
            "Iteration 24, Batch 76/100: Batch Loss = 1.1462\n",
            "Iteration 24, Batch 77/100: Batch Loss = 1.2456\n",
            "Iteration 24, Batch 78/100: Batch Loss = 1.2250\n",
            "Iteration 24, Batch 79/100: Batch Loss = 1.2680\n",
            "Iteration 24, Batch 80/100: Batch Loss = 1.3811\n",
            "Iteration 24, Batch 81/100: Batch Loss = 1.2226\n",
            "Iteration 24, Batch 82/100: Batch Loss = 1.4172\n",
            "Iteration 24, Batch 83/100: Batch Loss = 1.2810\n",
            "Iteration 24, Batch 84/100: Batch Loss = 1.2330\n",
            "Iteration 24, Batch 85/100: Batch Loss = 1.1925\n",
            "Iteration 24, Batch 86/100: Batch Loss = 1.3029\n",
            "Iteration 24, Batch 87/100: Batch Loss = 1.1574\n",
            "Iteration 24, Batch 88/100: Batch Loss = 1.3394\n",
            "Iteration 24, Batch 89/100: Batch Loss = 1.3953\n",
            "Iteration 24, Batch 90/100: Batch Loss = 1.4537\n",
            "Iteration 24, Batch 91/100: Batch Loss = 1.3055\n",
            "Iteration 24, Batch 92/100: Batch Loss = 1.1732\n",
            "Iteration 24, Batch 93/100: Batch Loss = 1.4003\n",
            "Iteration 24, Batch 94/100: Batch Loss = 1.2990\n",
            "Iteration 24, Batch 95/100: Batch Loss = 1.2070\n",
            "Iteration 24, Batch 96/100: Batch Loss = 1.2574\n",
            "Iteration 24, Batch 97/100: Batch Loss = 1.1950\n",
            "Iteration 24, Batch 98/100: Batch Loss = 1.2406\n",
            "Iteration 24, Batch 99/100: Batch Loss = 1.2309\n",
            "Iteration 24, Batch 100/100: Batch Loss = 1.1925\n",
            "Iteration 24: Train Loss = 1.2814, Val Loss = 1.2789\n",
            "Iteration 25, Batch 1/100: Batch Loss = 1.3916\n",
            "Iteration 25, Batch 2/100: Batch Loss = 1.3202\n",
            "Iteration 25, Batch 3/100: Batch Loss = 1.2396\n",
            "Iteration 25, Batch 4/100: Batch Loss = 1.2183\n",
            "Iteration 25, Batch 5/100: Batch Loss = 1.2599\n",
            "Iteration 25, Batch 6/100: Batch Loss = 1.4757\n",
            "Iteration 25, Batch 7/100: Batch Loss = 1.3082\n",
            "Iteration 25, Batch 8/100: Batch Loss = 1.2434\n",
            "Iteration 25, Batch 9/100: Batch Loss = 1.2935\n",
            "Iteration 25, Batch 10/100: Batch Loss = 1.2414\n",
            "Iteration 25, Batch 11/100: Batch Loss = 1.1381\n",
            "Iteration 25, Batch 12/100: Batch Loss = 1.3093\n",
            "Iteration 25, Batch 13/100: Batch Loss = 1.3839\n",
            "Iteration 25, Batch 14/100: Batch Loss = 1.3314\n",
            "Iteration 25, Batch 15/100: Batch Loss = 1.2260\n",
            "Iteration 25, Batch 16/100: Batch Loss = 1.2920\n",
            "Iteration 25, Batch 17/100: Batch Loss = 1.1673\n",
            "Iteration 25, Batch 18/100: Batch Loss = 1.2759\n",
            "Iteration 25, Batch 19/100: Batch Loss = 1.2188\n",
            "Iteration 25, Batch 20/100: Batch Loss = 1.1362\n",
            "Iteration 25, Batch 21/100: Batch Loss = 1.3573\n",
            "Iteration 25, Batch 22/100: Batch Loss = 1.2287\n",
            "Iteration 25, Batch 23/100: Batch Loss = 1.2830\n",
            "Iteration 25, Batch 24/100: Batch Loss = 1.1630\n",
            "Iteration 25, Batch 25/100: Batch Loss = 1.2839\n",
            "Iteration 25, Batch 26/100: Batch Loss = 1.3077\n",
            "Iteration 25, Batch 27/100: Batch Loss = 1.3229\n",
            "Iteration 25, Batch 28/100: Batch Loss = 1.3984\n",
            "Iteration 25, Batch 29/100: Batch Loss = 1.2954\n",
            "Iteration 25, Batch 30/100: Batch Loss = 1.3543\n",
            "Iteration 25, Batch 31/100: Batch Loss = 1.2024\n",
            "Iteration 25, Batch 32/100: Batch Loss = 1.4664\n",
            "Iteration 25, Batch 33/100: Batch Loss = 1.2538\n",
            "Iteration 25, Batch 34/100: Batch Loss = 1.2855\n",
            "Iteration 25, Batch 35/100: Batch Loss = 1.3372\n",
            "Iteration 25, Batch 36/100: Batch Loss = 1.1355\n",
            "Iteration 25, Batch 37/100: Batch Loss = 1.3273\n",
            "Iteration 25, Batch 38/100: Batch Loss = 1.1290\n",
            "Iteration 25, Batch 39/100: Batch Loss = 1.3118\n",
            "Iteration 25, Batch 40/100: Batch Loss = 1.2053\n",
            "Iteration 25, Batch 41/100: Batch Loss = 1.1448\n",
            "Iteration 25, Batch 42/100: Batch Loss = 1.2729\n",
            "Iteration 25, Batch 43/100: Batch Loss = 1.2403\n",
            "Iteration 25, Batch 44/100: Batch Loss = 1.3762\n",
            "Iteration 25, Batch 45/100: Batch Loss = 1.3398\n",
            "Iteration 25, Batch 46/100: Batch Loss = 1.1835\n",
            "Iteration 25, Batch 47/100: Batch Loss = 1.4824\n",
            "Iteration 25, Batch 48/100: Batch Loss = 1.3920\n",
            "Iteration 25, Batch 49/100: Batch Loss = 1.1583\n",
            "Iteration 25, Batch 50/100: Batch Loss = 1.2218\n",
            "Iteration 25, Batch 51/100: Batch Loss = 1.1823\n",
            "Iteration 25, Batch 52/100: Batch Loss = 1.1866\n",
            "Iteration 25, Batch 53/100: Batch Loss = 1.2345\n",
            "Iteration 25, Batch 54/100: Batch Loss = 1.1214\n",
            "Iteration 25, Batch 55/100: Batch Loss = 1.2661\n",
            "Iteration 25, Batch 56/100: Batch Loss = 1.3115\n",
            "Iteration 25, Batch 57/100: Batch Loss = 1.2374\n",
            "Iteration 25, Batch 58/100: Batch Loss = 1.3549\n",
            "Iteration 25, Batch 59/100: Batch Loss = 1.3795\n",
            "Iteration 25, Batch 60/100: Batch Loss = 1.2898\n",
            "Iteration 25, Batch 61/100: Batch Loss = 1.1891\n",
            "Iteration 25, Batch 62/100: Batch Loss = 1.1922\n",
            "Iteration 25, Batch 63/100: Batch Loss = 1.1983\n",
            "Iteration 25, Batch 64/100: Batch Loss = 1.3154\n",
            "Iteration 25, Batch 65/100: Batch Loss = 1.3698\n",
            "Iteration 25, Batch 66/100: Batch Loss = 1.3440\n",
            "Iteration 25, Batch 67/100: Batch Loss = 1.2438\n",
            "Iteration 25, Batch 68/100: Batch Loss = 1.2989\n",
            "Iteration 25, Batch 69/100: Batch Loss = 1.2083\n",
            "Iteration 25, Batch 70/100: Batch Loss = 1.3114\n",
            "Iteration 25, Batch 71/100: Batch Loss = 1.2889\n",
            "Iteration 25, Batch 72/100: Batch Loss = 1.2726\n",
            "Iteration 25, Batch 73/100: Batch Loss = 1.2692\n",
            "Iteration 25, Batch 74/100: Batch Loss = 1.2664\n",
            "Iteration 25, Batch 75/100: Batch Loss = 1.4441\n",
            "Iteration 25, Batch 76/100: Batch Loss = 1.2786\n",
            "Iteration 25, Batch 77/100: Batch Loss = 1.4731\n",
            "Iteration 25, Batch 78/100: Batch Loss = 1.4785\n",
            "Iteration 25, Batch 79/100: Batch Loss = 1.3150\n",
            "Iteration 25, Batch 80/100: Batch Loss = 1.2278\n",
            "Iteration 25, Batch 81/100: Batch Loss = 1.4725\n",
            "Iteration 25, Batch 82/100: Batch Loss = 1.2768\n",
            "Iteration 25, Batch 83/100: Batch Loss = 1.2145\n",
            "Iteration 25, Batch 84/100: Batch Loss = 1.2898\n",
            "Iteration 25, Batch 85/100: Batch Loss = 1.2396\n",
            "Iteration 25, Batch 86/100: Batch Loss = 1.2978\n",
            "Iteration 25, Batch 87/100: Batch Loss = 1.2313\n",
            "Iteration 25, Batch 88/100: Batch Loss = 1.3825\n",
            "Iteration 25, Batch 89/100: Batch Loss = 1.3082\n",
            "Iteration 25, Batch 90/100: Batch Loss = 1.2547\n",
            "Iteration 25, Batch 91/100: Batch Loss = 1.1831\n",
            "Iteration 25, Batch 92/100: Batch Loss = 1.2626\n",
            "Iteration 25, Batch 93/100: Batch Loss = 1.2899\n",
            "Iteration 25, Batch 94/100: Batch Loss = 1.3288\n",
            "Iteration 25, Batch 95/100: Batch Loss = 1.2063\n",
            "Iteration 25, Batch 96/100: Batch Loss = 1.2693\n",
            "Iteration 25, Batch 97/100: Batch Loss = 1.2388\n",
            "Iteration 25, Batch 98/100: Batch Loss = 1.3381\n",
            "Iteration 25, Batch 99/100: Batch Loss = 1.2721\n",
            "Iteration 25, Batch 100/100: Batch Loss = 1.2944\n",
            "Iteration 25: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 26, Batch 1/100: Batch Loss = 1.2766\n",
            "Iteration 26, Batch 2/100: Batch Loss = 1.2639\n",
            "Iteration 26, Batch 3/100: Batch Loss = 1.3359\n",
            "Iteration 26, Batch 4/100: Batch Loss = 1.3826\n",
            "Iteration 26, Batch 5/100: Batch Loss = 1.2444\n",
            "Iteration 26, Batch 6/100: Batch Loss = 1.3640\n",
            "Iteration 26, Batch 7/100: Batch Loss = 1.2165\n",
            "Iteration 26, Batch 8/100: Batch Loss = 1.3240\n",
            "Iteration 26, Batch 9/100: Batch Loss = 1.2248\n",
            "Iteration 26, Batch 10/100: Batch Loss = 1.0786\n",
            "Iteration 26, Batch 11/100: Batch Loss = 1.3555\n",
            "Iteration 26, Batch 12/100: Batch Loss = 1.1999\n",
            "Iteration 26, Batch 13/100: Batch Loss = 1.2523\n",
            "Iteration 26, Batch 14/100: Batch Loss = 1.1828\n",
            "Iteration 26, Batch 15/100: Batch Loss = 1.2797\n",
            "Iteration 26, Batch 16/100: Batch Loss = 1.3562\n",
            "Iteration 26, Batch 17/100: Batch Loss = 1.4639\n",
            "Iteration 26, Batch 18/100: Batch Loss = 1.2099\n",
            "Iteration 26, Batch 19/100: Batch Loss = 1.3676\n",
            "Iteration 26, Batch 20/100: Batch Loss = 1.1939\n",
            "Iteration 26, Batch 21/100: Batch Loss = 1.2306\n",
            "Iteration 26, Batch 22/100: Batch Loss = 1.1975\n",
            "Iteration 26, Batch 23/100: Batch Loss = 1.3075\n",
            "Iteration 26, Batch 24/100: Batch Loss = 1.0759\n",
            "Iteration 26, Batch 25/100: Batch Loss = 1.2038\n",
            "Iteration 26, Batch 26/100: Batch Loss = 1.2572\n",
            "Iteration 26, Batch 27/100: Batch Loss = 1.2611\n",
            "Iteration 26, Batch 28/100: Batch Loss = 1.3753\n",
            "Iteration 26, Batch 29/100: Batch Loss = 1.2363\n",
            "Iteration 26, Batch 30/100: Batch Loss = 1.4289\n",
            "Iteration 26, Batch 31/100: Batch Loss = 1.2849\n",
            "Iteration 26, Batch 32/100: Batch Loss = 1.3645\n",
            "Iteration 26, Batch 33/100: Batch Loss = 1.2625\n",
            "Iteration 26, Batch 34/100: Batch Loss = 1.3095\n",
            "Iteration 26, Batch 35/100: Batch Loss = 1.2675\n",
            "Iteration 26, Batch 36/100: Batch Loss = 1.2598\n",
            "Iteration 26, Batch 37/100: Batch Loss = 1.3317\n",
            "Iteration 26, Batch 38/100: Batch Loss = 1.2607\n",
            "Iteration 26, Batch 39/100: Batch Loss = 1.3459\n",
            "Iteration 26, Batch 40/100: Batch Loss = 1.2690\n",
            "Iteration 26, Batch 41/100: Batch Loss = 1.1722\n",
            "Iteration 26, Batch 42/100: Batch Loss = 1.2806\n",
            "Iteration 26, Batch 43/100: Batch Loss = 1.2183\n",
            "Iteration 26, Batch 44/100: Batch Loss = 1.3370\n",
            "Iteration 26, Batch 45/100: Batch Loss = 1.2560\n",
            "Iteration 26, Batch 46/100: Batch Loss = 1.1485\n",
            "Iteration 26, Batch 47/100: Batch Loss = 1.3214\n",
            "Iteration 26, Batch 48/100: Batch Loss = 1.1767\n",
            "Iteration 26, Batch 49/100: Batch Loss = 1.2149\n",
            "Iteration 26, Batch 50/100: Batch Loss = 1.3155\n",
            "Iteration 26, Batch 51/100: Batch Loss = 1.2183\n",
            "Iteration 26, Batch 52/100: Batch Loss = 1.3310\n",
            "Iteration 26, Batch 53/100: Batch Loss = 1.3507\n",
            "Iteration 26, Batch 54/100: Batch Loss = 1.2243\n",
            "Iteration 26, Batch 55/100: Batch Loss = 1.2759\n",
            "Iteration 26, Batch 56/100: Batch Loss = 1.3455\n",
            "Iteration 26, Batch 57/100: Batch Loss = 1.3530\n",
            "Iteration 26, Batch 58/100: Batch Loss = 1.2628\n",
            "Iteration 26, Batch 59/100: Batch Loss = 1.2479\n",
            "Iteration 26, Batch 60/100: Batch Loss = 1.2818\n",
            "Iteration 26, Batch 61/100: Batch Loss = 1.3448\n",
            "Iteration 26, Batch 62/100: Batch Loss = 1.2280\n",
            "Iteration 26, Batch 63/100: Batch Loss = 1.2536\n",
            "Iteration 26, Batch 64/100: Batch Loss = 1.3974\n",
            "Iteration 26, Batch 65/100: Batch Loss = 1.3262\n",
            "Iteration 26, Batch 66/100: Batch Loss = 1.3202\n",
            "Iteration 26, Batch 67/100: Batch Loss = 1.2186\n",
            "Iteration 26, Batch 68/100: Batch Loss = 1.3028\n",
            "Iteration 26, Batch 69/100: Batch Loss = 1.3354\n",
            "Iteration 26, Batch 70/100: Batch Loss = 1.3199\n",
            "Iteration 26, Batch 71/100: Batch Loss = 1.2753\n",
            "Iteration 26, Batch 72/100: Batch Loss = 1.2815\n",
            "Iteration 26, Batch 73/100: Batch Loss = 1.3028\n",
            "Iteration 26, Batch 74/100: Batch Loss = 1.3447\n",
            "Iteration 26, Batch 75/100: Batch Loss = 1.3040\n",
            "Iteration 26, Batch 76/100: Batch Loss = 1.2688\n",
            "Iteration 26, Batch 77/100: Batch Loss = 1.3399\n",
            "Iteration 26, Batch 78/100: Batch Loss = 1.2176\n",
            "Iteration 26, Batch 79/100: Batch Loss = 1.3139\n",
            "Iteration 26, Batch 80/100: Batch Loss = 1.2581\n",
            "Iteration 26, Batch 81/100: Batch Loss = 1.3257\n",
            "Iteration 26, Batch 82/100: Batch Loss = 1.2152\n",
            "Iteration 26, Batch 83/100: Batch Loss = 1.3495\n",
            "Iteration 26, Batch 84/100: Batch Loss = 1.2442\n",
            "Iteration 26, Batch 85/100: Batch Loss = 1.2602\n",
            "Iteration 26, Batch 86/100: Batch Loss = 1.2517\n",
            "Iteration 26, Batch 87/100: Batch Loss = 1.2973\n",
            "Iteration 26, Batch 88/100: Batch Loss = 1.2915\n",
            "Iteration 26, Batch 89/100: Batch Loss = 1.3856\n",
            "Iteration 26, Batch 90/100: Batch Loss = 1.3670\n",
            "Iteration 26, Batch 91/100: Batch Loss = 1.2561\n",
            "Iteration 26, Batch 92/100: Batch Loss = 1.2639\n",
            "Iteration 26, Batch 93/100: Batch Loss = 1.3094\n",
            "Iteration 26, Batch 94/100: Batch Loss = 1.1549\n",
            "Iteration 26, Batch 95/100: Batch Loss = 1.3328\n",
            "Iteration 26, Batch 96/100: Batch Loss = 1.2262\n",
            "Iteration 26, Batch 97/100: Batch Loss = 1.3385\n",
            "Iteration 26, Batch 98/100: Batch Loss = 1.2664\n",
            "Iteration 26, Batch 99/100: Batch Loss = 1.2914\n",
            "Iteration 26, Batch 100/100: Batch Loss = 1.3160\n",
            "Iteration 26: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 27, Batch 1/100: Batch Loss = 1.2806\n",
            "Iteration 27, Batch 2/100: Batch Loss = 1.3947\n",
            "Iteration 27, Batch 3/100: Batch Loss = 1.2376\n",
            "Iteration 27, Batch 4/100: Batch Loss = 1.2609\n",
            "Iteration 27, Batch 5/100: Batch Loss = 1.2257\n",
            "Iteration 27, Batch 6/100: Batch Loss = 1.2353\n",
            "Iteration 27, Batch 7/100: Batch Loss = 1.2922\n",
            "Iteration 27, Batch 8/100: Batch Loss = 1.3600\n",
            "Iteration 27, Batch 9/100: Batch Loss = 1.2306\n",
            "Iteration 27, Batch 10/100: Batch Loss = 1.3974\n",
            "Iteration 27, Batch 11/100: Batch Loss = 1.1699\n",
            "Iteration 27, Batch 12/100: Batch Loss = 1.2769\n",
            "Iteration 27, Batch 13/100: Batch Loss = 1.2312\n",
            "Iteration 27, Batch 14/100: Batch Loss = 1.1481\n",
            "Iteration 27, Batch 15/100: Batch Loss = 1.3730\n",
            "Iteration 27, Batch 16/100: Batch Loss = 1.2400\n",
            "Iteration 27, Batch 17/100: Batch Loss = 1.3575\n",
            "Iteration 27, Batch 18/100: Batch Loss = 1.2184\n",
            "Iteration 27, Batch 19/100: Batch Loss = 1.2589\n",
            "Iteration 27, Batch 20/100: Batch Loss = 1.1864\n",
            "Iteration 27, Batch 21/100: Batch Loss = 1.3055\n",
            "Iteration 27, Batch 22/100: Batch Loss = 1.3591\n",
            "Iteration 27, Batch 23/100: Batch Loss = 1.2952\n",
            "Iteration 27, Batch 24/100: Batch Loss = 1.3633\n",
            "Iteration 27, Batch 25/100: Batch Loss = 1.2297\n",
            "Iteration 27, Batch 26/100: Batch Loss = 1.2887\n",
            "Iteration 27, Batch 27/100: Batch Loss = 1.2721\n",
            "Iteration 27, Batch 28/100: Batch Loss = 1.3750\n",
            "Iteration 27, Batch 29/100: Batch Loss = 1.2695\n",
            "Iteration 27, Batch 30/100: Batch Loss = 1.1621\n",
            "Iteration 27, Batch 31/100: Batch Loss = 1.1706\n",
            "Iteration 27, Batch 32/100: Batch Loss = 1.3437\n",
            "Iteration 27, Batch 33/100: Batch Loss = 1.2584\n",
            "Iteration 27, Batch 34/100: Batch Loss = 1.2742\n",
            "Iteration 27, Batch 35/100: Batch Loss = 1.3041\n",
            "Iteration 27, Batch 36/100: Batch Loss = 1.2182\n",
            "Iteration 27, Batch 37/100: Batch Loss = 1.2901\n",
            "Iteration 27, Batch 38/100: Batch Loss = 1.2349\n",
            "Iteration 27, Batch 39/100: Batch Loss = 1.3900\n",
            "Iteration 27, Batch 40/100: Batch Loss = 1.1879\n",
            "Iteration 27, Batch 41/100: Batch Loss = 1.3243\n",
            "Iteration 27, Batch 42/100: Batch Loss = 1.2572\n",
            "Iteration 27, Batch 43/100: Batch Loss = 1.2581\n",
            "Iteration 27, Batch 44/100: Batch Loss = 1.1930\n",
            "Iteration 27, Batch 45/100: Batch Loss = 1.3502\n",
            "Iteration 27, Batch 46/100: Batch Loss = 1.3444\n",
            "Iteration 27, Batch 47/100: Batch Loss = 1.2992\n",
            "Iteration 27, Batch 48/100: Batch Loss = 1.2769\n",
            "Iteration 27, Batch 49/100: Batch Loss = 1.3559\n",
            "Iteration 27, Batch 50/100: Batch Loss = 1.3660\n",
            "Iteration 27, Batch 51/100: Batch Loss = 1.3454\n",
            "Iteration 27, Batch 52/100: Batch Loss = 1.3786\n",
            "Iteration 27, Batch 53/100: Batch Loss = 1.3374\n",
            "Iteration 27, Batch 54/100: Batch Loss = 1.2973\n",
            "Iteration 27, Batch 55/100: Batch Loss = 1.3233\n",
            "Iteration 27, Batch 56/100: Batch Loss = 1.2594\n",
            "Iteration 27, Batch 57/100: Batch Loss = 1.1866\n",
            "Iteration 27, Batch 58/100: Batch Loss = 1.2903\n",
            "Iteration 27, Batch 59/100: Batch Loss = 1.2860\n",
            "Iteration 27, Batch 60/100: Batch Loss = 1.2200\n",
            "Iteration 27, Batch 61/100: Batch Loss = 1.2319\n",
            "Iteration 27, Batch 62/100: Batch Loss = 1.4763\n",
            "Iteration 27, Batch 63/100: Batch Loss = 1.2239\n",
            "Iteration 27, Batch 64/100: Batch Loss = 1.2913\n",
            "Iteration 27, Batch 65/100: Batch Loss = 1.3099\n",
            "Iteration 27, Batch 66/100: Batch Loss = 1.3099\n",
            "Iteration 27, Batch 67/100: Batch Loss = 1.2976\n",
            "Iteration 27, Batch 68/100: Batch Loss = 1.3197\n",
            "Iteration 27, Batch 69/100: Batch Loss = 1.3151\n",
            "Iteration 27, Batch 70/100: Batch Loss = 1.1962\n",
            "Iteration 27, Batch 71/100: Batch Loss = 1.3939\n",
            "Iteration 27, Batch 72/100: Batch Loss = 1.2639\n",
            "Iteration 27, Batch 73/100: Batch Loss = 1.2468\n",
            "Iteration 27, Batch 74/100: Batch Loss = 1.3847\n",
            "Iteration 27, Batch 75/100: Batch Loss = 1.3605\n",
            "Iteration 27, Batch 76/100: Batch Loss = 1.1936\n",
            "Iteration 27, Batch 77/100: Batch Loss = 1.2910\n",
            "Iteration 27, Batch 78/100: Batch Loss = 1.3509\n",
            "Iteration 27, Batch 79/100: Batch Loss = 1.2600\n",
            "Iteration 27, Batch 80/100: Batch Loss = 1.3148\n",
            "Iteration 27, Batch 81/100: Batch Loss = 1.2158\n",
            "Iteration 27, Batch 82/100: Batch Loss = 1.2354\n",
            "Iteration 27, Batch 83/100: Batch Loss = 1.2501\n",
            "Iteration 27, Batch 84/100: Batch Loss = 1.3521\n",
            "Iteration 27, Batch 85/100: Batch Loss = 1.3569\n",
            "Iteration 27, Batch 86/100: Batch Loss = 1.2727\n",
            "Iteration 27, Batch 87/100: Batch Loss = 1.3066\n",
            "Iteration 27, Batch 88/100: Batch Loss = 1.2247\n",
            "Iteration 27, Batch 89/100: Batch Loss = 1.2088\n",
            "Iteration 27, Batch 90/100: Batch Loss = 1.2810\n",
            "Iteration 27, Batch 91/100: Batch Loss = 1.2278\n",
            "Iteration 27, Batch 92/100: Batch Loss = 1.1299\n",
            "Iteration 27, Batch 93/100: Batch Loss = 1.3083\n",
            "Iteration 27, Batch 94/100: Batch Loss = 1.1986\n",
            "Iteration 27, Batch 95/100: Batch Loss = 1.2284\n",
            "Iteration 27, Batch 96/100: Batch Loss = 1.2900\n",
            "Iteration 27, Batch 97/100: Batch Loss = 1.2832\n",
            "Iteration 27, Batch 98/100: Batch Loss = 1.3789\n",
            "Iteration 27, Batch 99/100: Batch Loss = 1.2431\n",
            "Iteration 27, Batch 100/100: Batch Loss = 1.1894\n",
            "Iteration 27: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 28, Batch 1/100: Batch Loss = 1.3670\n",
            "Iteration 28, Batch 2/100: Batch Loss = 1.3399\n",
            "Iteration 28, Batch 3/100: Batch Loss = 1.3236\n",
            "Iteration 28, Batch 4/100: Batch Loss = 1.2954\n",
            "Iteration 28, Batch 5/100: Batch Loss = 1.2079\n",
            "Iteration 28, Batch 6/100: Batch Loss = 1.2849\n",
            "Iteration 28, Batch 7/100: Batch Loss = 1.3738\n",
            "Iteration 28, Batch 8/100: Batch Loss = 1.3935\n",
            "Iteration 28, Batch 9/100: Batch Loss = 1.3015\n",
            "Iteration 28, Batch 10/100: Batch Loss = 1.2688\n",
            "Iteration 28, Batch 11/100: Batch Loss = 1.2132\n",
            "Iteration 28, Batch 12/100: Batch Loss = 1.1988\n",
            "Iteration 28, Batch 13/100: Batch Loss = 1.2982\n",
            "Iteration 28, Batch 14/100: Batch Loss = 1.2929\n",
            "Iteration 28, Batch 15/100: Batch Loss = 1.3531\n",
            "Iteration 28, Batch 16/100: Batch Loss = 1.1383\n",
            "Iteration 28, Batch 17/100: Batch Loss = 1.3660\n",
            "Iteration 28, Batch 18/100: Batch Loss = 1.3620\n",
            "Iteration 28, Batch 19/100: Batch Loss = 1.2337\n",
            "Iteration 28, Batch 20/100: Batch Loss = 1.2313\n",
            "Iteration 28, Batch 21/100: Batch Loss = 1.2991\n",
            "Iteration 28, Batch 22/100: Batch Loss = 1.2468\n",
            "Iteration 28, Batch 23/100: Batch Loss = 1.2350\n",
            "Iteration 28, Batch 24/100: Batch Loss = 1.3816\n",
            "Iteration 28, Batch 25/100: Batch Loss = 1.3213\n",
            "Iteration 28, Batch 26/100: Batch Loss = 1.3669\n",
            "Iteration 28, Batch 27/100: Batch Loss = 1.2614\n",
            "Iteration 28, Batch 28/100: Batch Loss = 1.3079\n",
            "Iteration 28, Batch 29/100: Batch Loss = 1.2808\n",
            "Iteration 28, Batch 30/100: Batch Loss = 1.4008\n",
            "Iteration 28, Batch 31/100: Batch Loss = 1.3145\n",
            "Iteration 28, Batch 32/100: Batch Loss = 1.3517\n",
            "Iteration 28, Batch 33/100: Batch Loss = 1.3696\n",
            "Iteration 28, Batch 34/100: Batch Loss = 1.1921\n",
            "Iteration 28, Batch 35/100: Batch Loss = 1.1938\n",
            "Iteration 28, Batch 36/100: Batch Loss = 1.1192\n",
            "Iteration 28, Batch 37/100: Batch Loss = 1.1985\n",
            "Iteration 28, Batch 38/100: Batch Loss = 1.3189\n",
            "Iteration 28, Batch 39/100: Batch Loss = 1.2885\n",
            "Iteration 28, Batch 40/100: Batch Loss = 1.2746\n",
            "Iteration 28, Batch 41/100: Batch Loss = 1.2024\n",
            "Iteration 28, Batch 42/100: Batch Loss = 1.3764\n",
            "Iteration 28, Batch 43/100: Batch Loss = 1.3123\n",
            "Iteration 28, Batch 44/100: Batch Loss = 1.3017\n",
            "Iteration 28, Batch 45/100: Batch Loss = 1.3657\n",
            "Iteration 28, Batch 46/100: Batch Loss = 1.2328\n",
            "Iteration 28, Batch 47/100: Batch Loss = 1.2050\n",
            "Iteration 28, Batch 48/100: Batch Loss = 1.1645\n",
            "Iteration 28, Batch 49/100: Batch Loss = 1.2689\n",
            "Iteration 28, Batch 50/100: Batch Loss = 1.2861\n",
            "Iteration 28, Batch 51/100: Batch Loss = 1.3542\n",
            "Iteration 28, Batch 52/100: Batch Loss = 1.2458\n",
            "Iteration 28, Batch 53/100: Batch Loss = 1.2624\n",
            "Iteration 28, Batch 54/100: Batch Loss = 1.3289\n",
            "Iteration 28, Batch 55/100: Batch Loss = 1.2058\n",
            "Iteration 28, Batch 56/100: Batch Loss = 1.2780\n",
            "Iteration 28, Batch 57/100: Batch Loss = 1.0915\n",
            "Iteration 28, Batch 58/100: Batch Loss = 1.3438\n",
            "Iteration 28, Batch 59/100: Batch Loss = 1.3660\n",
            "Iteration 28, Batch 60/100: Batch Loss = 1.2375\n",
            "Iteration 28, Batch 61/100: Batch Loss = 1.2520\n",
            "Iteration 28, Batch 62/100: Batch Loss = 1.3055\n",
            "Iteration 28, Batch 63/100: Batch Loss = 1.1065\n",
            "Iteration 28, Batch 64/100: Batch Loss = 1.3127\n",
            "Iteration 28, Batch 65/100: Batch Loss = 1.3487\n",
            "Iteration 28, Batch 66/100: Batch Loss = 1.3121\n",
            "Iteration 28, Batch 67/100: Batch Loss = 1.4408\n",
            "Iteration 28, Batch 68/100: Batch Loss = 1.2048\n",
            "Iteration 28, Batch 69/100: Batch Loss = 1.1669\n",
            "Iteration 28, Batch 70/100: Batch Loss = 1.2856\n",
            "Iteration 28, Batch 71/100: Batch Loss = 1.2580\n",
            "Iteration 28, Batch 72/100: Batch Loss = 1.1667\n",
            "Iteration 28, Batch 73/100: Batch Loss = 1.3344\n",
            "Iteration 28, Batch 74/100: Batch Loss = 1.3790\n",
            "Iteration 28, Batch 75/100: Batch Loss = 1.1693\n",
            "Iteration 28, Batch 76/100: Batch Loss = 1.4011\n",
            "Iteration 28, Batch 77/100: Batch Loss = 1.2608\n",
            "Iteration 28, Batch 78/100: Batch Loss = 1.1719\n",
            "Iteration 28, Batch 79/100: Batch Loss = 1.2949\n",
            "Iteration 28, Batch 80/100: Batch Loss = 1.2113\n",
            "Iteration 28, Batch 81/100: Batch Loss = 1.3859\n",
            "Iteration 28, Batch 82/100: Batch Loss = 1.2992\n",
            "Iteration 28, Batch 83/100: Batch Loss = 1.3420\n",
            "Iteration 28, Batch 84/100: Batch Loss = 1.2664\n",
            "Iteration 28, Batch 85/100: Batch Loss = 1.2924\n",
            "Iteration 28, Batch 86/100: Batch Loss = 1.3225\n",
            "Iteration 28, Batch 87/100: Batch Loss = 1.3048\n",
            "Iteration 28, Batch 88/100: Batch Loss = 1.2374\n",
            "Iteration 28, Batch 89/100: Batch Loss = 1.3624\n",
            "Iteration 28, Batch 90/100: Batch Loss = 1.2114\n",
            "Iteration 28, Batch 91/100: Batch Loss = 1.2360\n",
            "Iteration 28, Batch 92/100: Batch Loss = 1.2233\n",
            "Iteration 28, Batch 93/100: Batch Loss = 1.2701\n",
            "Iteration 28, Batch 94/100: Batch Loss = 1.2701\n",
            "Iteration 28, Batch 95/100: Batch Loss = 1.2206\n",
            "Iteration 28, Batch 96/100: Batch Loss = 1.3397\n",
            "Iteration 28, Batch 97/100: Batch Loss = 1.2887\n",
            "Iteration 28, Batch 98/100: Batch Loss = 1.3085\n",
            "Iteration 28, Batch 99/100: Batch Loss = 1.2992\n",
            "Iteration 28, Batch 100/100: Batch Loss = 1.2688\n",
            "Iteration 28: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 29, Batch 1/100: Batch Loss = 1.3296\n",
            "Iteration 29, Batch 2/100: Batch Loss = 1.3505\n",
            "Iteration 29, Batch 3/100: Batch Loss = 1.3412\n",
            "Iteration 29, Batch 4/100: Batch Loss = 1.2466\n",
            "Iteration 29, Batch 5/100: Batch Loss = 1.2300\n",
            "Iteration 29, Batch 6/100: Batch Loss = 1.3634\n",
            "Iteration 29, Batch 7/100: Batch Loss = 1.3406\n",
            "Iteration 29, Batch 8/100: Batch Loss = 1.2817\n",
            "Iteration 29, Batch 9/100: Batch Loss = 1.4017\n",
            "Iteration 29, Batch 10/100: Batch Loss = 1.3442\n",
            "Iteration 29, Batch 11/100: Batch Loss = 1.2176\n",
            "Iteration 29, Batch 12/100: Batch Loss = 1.2904\n",
            "Iteration 29, Batch 13/100: Batch Loss = 1.2806\n",
            "Iteration 29, Batch 14/100: Batch Loss = 1.2724\n",
            "Iteration 29, Batch 15/100: Batch Loss = 1.2065\n",
            "Iteration 29, Batch 16/100: Batch Loss = 1.2043\n",
            "Iteration 29, Batch 17/100: Batch Loss = 1.3181\n",
            "Iteration 29, Batch 18/100: Batch Loss = 1.4089\n",
            "Iteration 29, Batch 19/100: Batch Loss = 1.3903\n",
            "Iteration 29, Batch 20/100: Batch Loss = 1.3228\n",
            "Iteration 29, Batch 21/100: Batch Loss = 1.2919\n",
            "Iteration 29, Batch 22/100: Batch Loss = 1.3105\n",
            "Iteration 29, Batch 23/100: Batch Loss = 1.2375\n",
            "Iteration 29, Batch 24/100: Batch Loss = 1.2971\n",
            "Iteration 29, Batch 25/100: Batch Loss = 1.3217\n",
            "Iteration 29, Batch 26/100: Batch Loss = 1.1516\n",
            "Iteration 29, Batch 27/100: Batch Loss = 1.2182\n",
            "Iteration 29, Batch 28/100: Batch Loss = 1.1879\n",
            "Iteration 29, Batch 29/100: Batch Loss = 1.1854\n",
            "Iteration 29, Batch 30/100: Batch Loss = 1.3183\n",
            "Iteration 29, Batch 31/100: Batch Loss = 1.3552\n",
            "Iteration 29, Batch 32/100: Batch Loss = 1.2655\n",
            "Iteration 29, Batch 33/100: Batch Loss = 1.1624\n",
            "Iteration 29, Batch 34/100: Batch Loss = 1.0955\n",
            "Iteration 29, Batch 35/100: Batch Loss = 1.2958\n",
            "Iteration 29, Batch 36/100: Batch Loss = 1.2312\n",
            "Iteration 29, Batch 37/100: Batch Loss = 1.2026\n",
            "Iteration 29, Batch 38/100: Batch Loss = 1.1852\n",
            "Iteration 29, Batch 39/100: Batch Loss = 1.4110\n",
            "Iteration 29, Batch 40/100: Batch Loss = 1.2756\n",
            "Iteration 29, Batch 41/100: Batch Loss = 1.2603\n",
            "Iteration 29, Batch 42/100: Batch Loss = 1.2365\n",
            "Iteration 29, Batch 43/100: Batch Loss = 1.2793\n",
            "Iteration 29, Batch 44/100: Batch Loss = 1.3018\n",
            "Iteration 29, Batch 45/100: Batch Loss = 1.2032\n",
            "Iteration 29, Batch 46/100: Batch Loss = 1.3115\n",
            "Iteration 29, Batch 47/100: Batch Loss = 1.2376\n",
            "Iteration 29, Batch 48/100: Batch Loss = 1.3731\n",
            "Iteration 29, Batch 49/100: Batch Loss = 1.3495\n",
            "Iteration 29, Batch 50/100: Batch Loss = 1.3802\n",
            "Iteration 29, Batch 51/100: Batch Loss = 1.3369\n",
            "Iteration 29, Batch 52/100: Batch Loss = 1.2759\n",
            "Iteration 29, Batch 53/100: Batch Loss = 1.3086\n",
            "Iteration 29, Batch 54/100: Batch Loss = 1.2639\n",
            "Iteration 29, Batch 55/100: Batch Loss = 1.2011\n",
            "Iteration 29, Batch 56/100: Batch Loss = 1.4316\n",
            "Iteration 29, Batch 57/100: Batch Loss = 1.2415\n",
            "Iteration 29, Batch 58/100: Batch Loss = 1.1832\n",
            "Iteration 29, Batch 59/100: Batch Loss = 1.2507\n",
            "Iteration 29, Batch 60/100: Batch Loss = 1.2403\n",
            "Iteration 29, Batch 61/100: Batch Loss = 1.3338\n",
            "Iteration 29, Batch 62/100: Batch Loss = 1.2954\n",
            "Iteration 29, Batch 63/100: Batch Loss = 1.2281\n",
            "Iteration 29, Batch 64/100: Batch Loss = 1.2467\n",
            "Iteration 29, Batch 65/100: Batch Loss = 1.4705\n",
            "Iteration 29, Batch 66/100: Batch Loss = 1.3020\n",
            "Iteration 29, Batch 67/100: Batch Loss = 1.1877\n",
            "Iteration 29, Batch 68/100: Batch Loss = 1.1359\n",
            "Iteration 29, Batch 69/100: Batch Loss = 1.2438\n",
            "Iteration 29, Batch 70/100: Batch Loss = 1.1951\n",
            "Iteration 29, Batch 71/100: Batch Loss = 1.2446\n",
            "Iteration 29, Batch 72/100: Batch Loss = 1.2731\n",
            "Iteration 29, Batch 73/100: Batch Loss = 1.3020\n",
            "Iteration 29, Batch 74/100: Batch Loss = 1.3093\n",
            "Iteration 29, Batch 75/100: Batch Loss = 1.2824\n",
            "Iteration 29, Batch 76/100: Batch Loss = 1.3478\n",
            "Iteration 29, Batch 77/100: Batch Loss = 1.3403\n",
            "Iteration 29, Batch 78/100: Batch Loss = 1.4198\n",
            "Iteration 29, Batch 79/100: Batch Loss = 1.2847\n",
            "Iteration 29, Batch 80/100: Batch Loss = 1.4496\n",
            "Iteration 29, Batch 81/100: Batch Loss = 1.1693\n",
            "Iteration 29, Batch 82/100: Batch Loss = 1.3031\n",
            "Iteration 29, Batch 83/100: Batch Loss = 1.2793\n",
            "Iteration 29, Batch 84/100: Batch Loss = 1.2768\n",
            "Iteration 29, Batch 85/100: Batch Loss = 1.4263\n",
            "Iteration 29, Batch 86/100: Batch Loss = 1.2852\n",
            "Iteration 29, Batch 87/100: Batch Loss = 1.2003\n",
            "Iteration 29, Batch 88/100: Batch Loss = 1.3250\n",
            "Iteration 29, Batch 89/100: Batch Loss = 1.3156\n",
            "Iteration 29, Batch 90/100: Batch Loss = 1.2977\n",
            "Iteration 29, Batch 91/100: Batch Loss = 1.3502\n",
            "Iteration 29, Batch 92/100: Batch Loss = 1.2700\n",
            "Iteration 29, Batch 93/100: Batch Loss = 1.1019\n",
            "Iteration 29, Batch 94/100: Batch Loss = 1.2593\n",
            "Iteration 29, Batch 95/100: Batch Loss = 1.2114\n",
            "Iteration 29, Batch 96/100: Batch Loss = 1.2278\n",
            "Iteration 29, Batch 97/100: Batch Loss = 1.4136\n",
            "Iteration 29, Batch 98/100: Batch Loss = 1.0959\n",
            "Iteration 29, Batch 99/100: Batch Loss = 1.3238\n",
            "Iteration 29, Batch 100/100: Batch Loss = 1.2971\n",
            "Iteration 29: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 30, Batch 1/100: Batch Loss = 1.1631\n",
            "Iteration 30, Batch 2/100: Batch Loss = 1.2488\n",
            "Iteration 30, Batch 3/100: Batch Loss = 1.3698\n",
            "Iteration 30, Batch 4/100: Batch Loss = 1.3144\n",
            "Iteration 30, Batch 5/100: Batch Loss = 1.2796\n",
            "Iteration 30, Batch 6/100: Batch Loss = 1.3629\n",
            "Iteration 30, Batch 7/100: Batch Loss = 1.3494\n",
            "Iteration 30, Batch 8/100: Batch Loss = 1.1836\n",
            "Iteration 30, Batch 9/100: Batch Loss = 1.2860\n",
            "Iteration 30, Batch 10/100: Batch Loss = 1.1179\n",
            "Iteration 30, Batch 11/100: Batch Loss = 1.2738\n",
            "Iteration 30, Batch 12/100: Batch Loss = 1.3551\n",
            "Iteration 30, Batch 13/100: Batch Loss = 1.2572\n",
            "Iteration 30, Batch 14/100: Batch Loss = 1.3306\n",
            "Iteration 30, Batch 15/100: Batch Loss = 1.2523\n",
            "Iteration 30, Batch 16/100: Batch Loss = 1.2936\n",
            "Iteration 30, Batch 17/100: Batch Loss = 1.1376\n",
            "Iteration 30, Batch 18/100: Batch Loss = 1.3084\n",
            "Iteration 30, Batch 19/100: Batch Loss = 1.3042\n",
            "Iteration 30, Batch 20/100: Batch Loss = 1.2454\n",
            "Iteration 30, Batch 21/100: Batch Loss = 1.3083\n",
            "Iteration 30, Batch 22/100: Batch Loss = 1.2594\n",
            "Iteration 30, Batch 23/100: Batch Loss = 1.3368\n",
            "Iteration 30, Batch 24/100: Batch Loss = 1.3421\n",
            "Iteration 30, Batch 25/100: Batch Loss = 1.3613\n",
            "Iteration 30, Batch 26/100: Batch Loss = 1.1554\n",
            "Iteration 30, Batch 27/100: Batch Loss = 1.2867\n",
            "Iteration 30, Batch 28/100: Batch Loss = 1.2374\n",
            "Iteration 30, Batch 29/100: Batch Loss = 1.1645\n",
            "Iteration 30, Batch 30/100: Batch Loss = 1.2701\n",
            "Iteration 30, Batch 31/100: Batch Loss = 1.2481\n",
            "Iteration 30, Batch 32/100: Batch Loss = 1.2529\n",
            "Iteration 30, Batch 33/100: Batch Loss = 1.3466\n",
            "Iteration 30, Batch 34/100: Batch Loss = 1.1415\n",
            "Iteration 30, Batch 35/100: Batch Loss = 1.2041\n",
            "Iteration 30, Batch 36/100: Batch Loss = 1.2985\n",
            "Iteration 30, Batch 37/100: Batch Loss = 1.2239\n",
            "Iteration 30, Batch 38/100: Batch Loss = 1.1949\n",
            "Iteration 30, Batch 39/100: Batch Loss = 1.3236\n",
            "Iteration 30, Batch 40/100: Batch Loss = 1.2712\n",
            "Iteration 30, Batch 41/100: Batch Loss = 1.3394\n",
            "Iteration 30, Batch 42/100: Batch Loss = 1.1701\n",
            "Iteration 30, Batch 43/100: Batch Loss = 1.2878\n",
            "Iteration 30, Batch 44/100: Batch Loss = 1.4505\n",
            "Iteration 30, Batch 45/100: Batch Loss = 1.3548\n",
            "Iteration 30, Batch 46/100: Batch Loss = 1.2518\n",
            "Iteration 30, Batch 47/100: Batch Loss = 1.3995\n",
            "Iteration 30, Batch 48/100: Batch Loss = 1.2610\n",
            "Iteration 30, Batch 49/100: Batch Loss = 1.2766\n",
            "Iteration 30, Batch 50/100: Batch Loss = 1.2478\n",
            "Iteration 30, Batch 51/100: Batch Loss = 1.2955\n",
            "Iteration 30, Batch 52/100: Batch Loss = 1.3202\n",
            "Iteration 30, Batch 53/100: Batch Loss = 1.2387\n",
            "Iteration 30, Batch 54/100: Batch Loss = 1.3755\n",
            "Iteration 30, Batch 55/100: Batch Loss = 1.2670\n",
            "Iteration 30, Batch 56/100: Batch Loss = 1.4721\n",
            "Iteration 30, Batch 57/100: Batch Loss = 1.2783\n",
            "Iteration 30, Batch 58/100: Batch Loss = 1.3689\n",
            "Iteration 30, Batch 59/100: Batch Loss = 1.2372\n",
            "Iteration 30, Batch 60/100: Batch Loss = 1.3636\n",
            "Iteration 30, Batch 61/100: Batch Loss = 1.3485\n",
            "Iteration 30, Batch 62/100: Batch Loss = 1.1692\n",
            "Iteration 30, Batch 63/100: Batch Loss = 1.3945\n",
            "Iteration 30, Batch 64/100: Batch Loss = 1.2174\n",
            "Iteration 30, Batch 65/100: Batch Loss = 1.2790\n",
            "Iteration 30, Batch 66/100: Batch Loss = 1.2134\n",
            "Iteration 30, Batch 67/100: Batch Loss = 1.2555\n",
            "Iteration 30, Batch 68/100: Batch Loss = 1.1450\n",
            "Iteration 30, Batch 69/100: Batch Loss = 1.3237\n",
            "Iteration 30, Batch 70/100: Batch Loss = 1.2129\n",
            "Iteration 30, Batch 71/100: Batch Loss = 1.4634\n",
            "Iteration 30, Batch 72/100: Batch Loss = 1.3122\n",
            "Iteration 30, Batch 73/100: Batch Loss = 1.4226\n",
            "Iteration 30, Batch 74/100: Batch Loss = 1.2367\n",
            "Iteration 30, Batch 75/100: Batch Loss = 1.0919\n",
            "Iteration 30, Batch 76/100: Batch Loss = 1.3660\n",
            "Iteration 30, Batch 77/100: Batch Loss = 1.3659\n",
            "Iteration 30, Batch 78/100: Batch Loss = 1.2258\n",
            "Iteration 30, Batch 79/100: Batch Loss = 1.2793\n",
            "Iteration 30, Batch 80/100: Batch Loss = 1.1727\n",
            "Iteration 30, Batch 81/100: Batch Loss = 1.2356\n",
            "Iteration 30, Batch 82/100: Batch Loss = 1.2849\n",
            "Iteration 30, Batch 83/100: Batch Loss = 1.2058\n",
            "Iteration 30, Batch 84/100: Batch Loss = 1.3215\n",
            "Iteration 30, Batch 85/100: Batch Loss = 1.2846\n",
            "Iteration 30, Batch 86/100: Batch Loss = 1.1686\n",
            "Iteration 30, Batch 87/100: Batch Loss = 1.3060\n",
            "Iteration 30, Batch 88/100: Batch Loss = 1.2663\n",
            "Iteration 30, Batch 89/100: Batch Loss = 1.3346\n",
            "Iteration 30, Batch 90/100: Batch Loss = 1.2187\n",
            "Iteration 30, Batch 91/100: Batch Loss = 1.2755\n",
            "Iteration 30, Batch 92/100: Batch Loss = 1.2759\n",
            "Iteration 30, Batch 93/100: Batch Loss = 1.3490\n",
            "Iteration 30, Batch 94/100: Batch Loss = 1.3120\n",
            "Iteration 30, Batch 95/100: Batch Loss = 1.2520\n",
            "Iteration 30, Batch 96/100: Batch Loss = 1.2787\n",
            "Iteration 30, Batch 97/100: Batch Loss = 1.3660\n",
            "Iteration 30, Batch 98/100: Batch Loss = 1.2809\n",
            "Iteration 30, Batch 99/100: Batch Loss = 1.4025\n",
            "Iteration 30, Batch 100/100: Batch Loss = 1.2972\n",
            "Iteration 30: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 31, Batch 1/100: Batch Loss = 1.1983\n",
            "Iteration 31, Batch 2/100: Batch Loss = 1.2849\n",
            "Iteration 31, Batch 3/100: Batch Loss = 1.3682\n",
            "Iteration 31, Batch 4/100: Batch Loss = 1.3264\n",
            "Iteration 31, Batch 5/100: Batch Loss = 1.3399\n",
            "Iteration 31, Batch 6/100: Batch Loss = 1.2815\n",
            "Iteration 31, Batch 7/100: Batch Loss = 1.2257\n",
            "Iteration 31, Batch 8/100: Batch Loss = 1.3973\n",
            "Iteration 31, Batch 9/100: Batch Loss = 1.2500\n",
            "Iteration 31, Batch 10/100: Batch Loss = 1.2406\n",
            "Iteration 31, Batch 11/100: Batch Loss = 1.1859\n",
            "Iteration 31, Batch 12/100: Batch Loss = 1.1519\n",
            "Iteration 31, Batch 13/100: Batch Loss = 1.2018\n",
            "Iteration 31, Batch 14/100: Batch Loss = 1.4438\n",
            "Iteration 31, Batch 15/100: Batch Loss = 1.2170\n",
            "Iteration 31, Batch 16/100: Batch Loss = 1.3401\n",
            "Iteration 31, Batch 17/100: Batch Loss = 1.2035\n",
            "Iteration 31, Batch 18/100: Batch Loss = 1.3953\n",
            "Iteration 31, Batch 19/100: Batch Loss = 1.2978\n",
            "Iteration 31, Batch 20/100: Batch Loss = 1.2221\n",
            "Iteration 31, Batch 21/100: Batch Loss = 1.2828\n",
            "Iteration 31, Batch 22/100: Batch Loss = 1.2318\n",
            "Iteration 31, Batch 23/100: Batch Loss = 1.3156\n",
            "Iteration 31, Batch 24/100: Batch Loss = 1.2593\n",
            "Iteration 31, Batch 25/100: Batch Loss = 1.2734\n",
            "Iteration 31, Batch 26/100: Batch Loss = 1.3551\n",
            "Iteration 31, Batch 27/100: Batch Loss = 1.3288\n",
            "Iteration 31, Batch 28/100: Batch Loss = 1.3416\n",
            "Iteration 31, Batch 29/100: Batch Loss = 1.2544\n",
            "Iteration 31, Batch 30/100: Batch Loss = 1.2200\n",
            "Iteration 31, Batch 31/100: Batch Loss = 1.2358\n",
            "Iteration 31, Batch 32/100: Batch Loss = 1.2815\n",
            "Iteration 31, Batch 33/100: Batch Loss = 1.3217\n",
            "Iteration 31, Batch 34/100: Batch Loss = 1.2463\n",
            "Iteration 31, Batch 35/100: Batch Loss = 1.2381\n",
            "Iteration 31, Batch 36/100: Batch Loss = 1.3813\n",
            "Iteration 31, Batch 37/100: Batch Loss = 1.3240\n",
            "Iteration 31, Batch 38/100: Batch Loss = 1.4080\n",
            "Iteration 31, Batch 39/100: Batch Loss = 1.3171\n",
            "Iteration 31, Batch 40/100: Batch Loss = 1.2575\n",
            "Iteration 31, Batch 41/100: Batch Loss = 1.2092\n",
            "Iteration 31, Batch 42/100: Batch Loss = 1.3150\n",
            "Iteration 31, Batch 43/100: Batch Loss = 1.3395\n",
            "Iteration 31, Batch 44/100: Batch Loss = 1.2795\n",
            "Iteration 31, Batch 45/100: Batch Loss = 1.3767\n",
            "Iteration 31, Batch 46/100: Batch Loss = 1.3153\n",
            "Iteration 31, Batch 47/100: Batch Loss = 1.3678\n",
            "Iteration 31, Batch 48/100: Batch Loss = 1.2067\n",
            "Iteration 31, Batch 49/100: Batch Loss = 1.3613\n",
            "Iteration 31, Batch 50/100: Batch Loss = 1.2485\n",
            "Iteration 31, Batch 51/100: Batch Loss = 1.4258\n",
            "Iteration 31, Batch 52/100: Batch Loss = 1.1583\n",
            "Iteration 31, Batch 53/100: Batch Loss = 1.2066\n",
            "Iteration 31, Batch 54/100: Batch Loss = 1.2535\n",
            "Iteration 31, Batch 55/100: Batch Loss = 1.1802\n",
            "Iteration 31, Batch 56/100: Batch Loss = 1.2795\n",
            "Iteration 31, Batch 57/100: Batch Loss = 1.2884\n",
            "Iteration 31, Batch 58/100: Batch Loss = 1.1955\n",
            "Iteration 31, Batch 59/100: Batch Loss = 1.1756\n",
            "Iteration 31, Batch 60/100: Batch Loss = 1.2906\n",
            "Iteration 31, Batch 61/100: Batch Loss = 1.2729\n",
            "Iteration 31, Batch 62/100: Batch Loss = 1.2862\n",
            "Iteration 31, Batch 63/100: Batch Loss = 1.2574\n",
            "Iteration 31, Batch 64/100: Batch Loss = 1.2662\n",
            "Iteration 31, Batch 65/100: Batch Loss = 1.2884\n",
            "Iteration 31, Batch 66/100: Batch Loss = 1.2551\n",
            "Iteration 31, Batch 67/100: Batch Loss = 1.2884\n",
            "Iteration 31, Batch 68/100: Batch Loss = 1.2816\n",
            "Iteration 31, Batch 69/100: Batch Loss = 1.3591\n",
            "Iteration 31, Batch 70/100: Batch Loss = 1.5211\n",
            "Iteration 31, Batch 71/100: Batch Loss = 1.2688\n",
            "Iteration 31, Batch 72/100: Batch Loss = 1.3414\n",
            "Iteration 31, Batch 73/100: Batch Loss = 1.3944\n",
            "Iteration 31, Batch 74/100: Batch Loss = 1.2442\n",
            "Iteration 31, Batch 75/100: Batch Loss = 1.2706\n",
            "Iteration 31, Batch 76/100: Batch Loss = 1.2994\n",
            "Iteration 31, Batch 77/100: Batch Loss = 1.2443\n",
            "Iteration 31, Batch 78/100: Batch Loss = 1.2354\n",
            "Iteration 31, Batch 79/100: Batch Loss = 1.2705\n",
            "Iteration 31, Batch 80/100: Batch Loss = 1.3545\n",
            "Iteration 31, Batch 81/100: Batch Loss = 1.2175\n",
            "Iteration 31, Batch 82/100: Batch Loss = 1.2532\n",
            "Iteration 31, Batch 83/100: Batch Loss = 1.2505\n",
            "Iteration 31, Batch 84/100: Batch Loss = 1.2985\n",
            "Iteration 31, Batch 85/100: Batch Loss = 1.2191\n",
            "Iteration 31, Batch 86/100: Batch Loss = 1.2555\n",
            "Iteration 31, Batch 87/100: Batch Loss = 1.2590\n",
            "Iteration 31, Batch 88/100: Batch Loss = 1.2388\n",
            "Iteration 31, Batch 89/100: Batch Loss = 1.4480\n",
            "Iteration 31, Batch 90/100: Batch Loss = 1.2313\n",
            "Iteration 31, Batch 91/100: Batch Loss = 1.2204\n",
            "Iteration 31, Batch 92/100: Batch Loss = 1.2162\n",
            "Iteration 31, Batch 93/100: Batch Loss = 1.2442\n",
            "Iteration 31, Batch 94/100: Batch Loss = 1.3135\n",
            "Iteration 31, Batch 95/100: Batch Loss = 1.3380\n",
            "Iteration 31, Batch 96/100: Batch Loss = 1.2019\n",
            "Iteration 31, Batch 97/100: Batch Loss = 1.2508\n",
            "Iteration 31, Batch 98/100: Batch Loss = 1.2293\n",
            "Iteration 31, Batch 99/100: Batch Loss = 1.2651\n",
            "Iteration 31, Batch 100/100: Batch Loss = 1.2570\n",
            "Iteration 31: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 32, Batch 1/100: Batch Loss = 1.2779\n",
            "Iteration 32, Batch 2/100: Batch Loss = 1.2291\n",
            "Iteration 32, Batch 3/100: Batch Loss = 1.3954\n",
            "Iteration 32, Batch 4/100: Batch Loss = 1.2700\n",
            "Iteration 32, Batch 5/100: Batch Loss = 1.3266\n",
            "Iteration 32, Batch 6/100: Batch Loss = 1.2886\n",
            "Iteration 32, Batch 7/100: Batch Loss = 1.2714\n",
            "Iteration 32, Batch 8/100: Batch Loss = 1.2570\n",
            "Iteration 32, Batch 9/100: Batch Loss = 1.3109\n",
            "Iteration 32, Batch 10/100: Batch Loss = 1.2090\n",
            "Iteration 32, Batch 11/100: Batch Loss = 1.2664\n",
            "Iteration 32, Batch 12/100: Batch Loss = 1.2205\n",
            "Iteration 32, Batch 13/100: Batch Loss = 1.2258\n",
            "Iteration 32, Batch 14/100: Batch Loss = 1.3281\n",
            "Iteration 32, Batch 15/100: Batch Loss = 1.2376\n",
            "Iteration 32, Batch 16/100: Batch Loss = 1.3647\n",
            "Iteration 32, Batch 17/100: Batch Loss = 1.2446\n",
            "Iteration 32, Batch 18/100: Batch Loss = 1.2567\n",
            "Iteration 32, Batch 19/100: Batch Loss = 1.1692\n",
            "Iteration 32, Batch 20/100: Batch Loss = 1.2593\n",
            "Iteration 32, Batch 21/100: Batch Loss = 1.2239\n",
            "Iteration 32, Batch 22/100: Batch Loss = 1.4497\n",
            "Iteration 32, Batch 23/100: Batch Loss = 1.3142\n",
            "Iteration 32, Batch 24/100: Batch Loss = 1.3937\n",
            "Iteration 32, Batch 25/100: Batch Loss = 1.2598\n",
            "Iteration 32, Batch 26/100: Batch Loss = 1.4314\n",
            "Iteration 32, Batch 27/100: Batch Loss = 1.2343\n",
            "Iteration 32, Batch 28/100: Batch Loss = 1.1665\n",
            "Iteration 32, Batch 29/100: Batch Loss = 1.4285\n",
            "Iteration 32, Batch 30/100: Batch Loss = 1.2017\n",
            "Iteration 32, Batch 31/100: Batch Loss = 1.1980\n",
            "Iteration 32, Batch 32/100: Batch Loss = 1.2632\n",
            "Iteration 32, Batch 33/100: Batch Loss = 1.1887\n",
            "Iteration 32, Batch 34/100: Batch Loss = 1.3113\n",
            "Iteration 32, Batch 35/100: Batch Loss = 1.3024\n",
            "Iteration 32, Batch 36/100: Batch Loss = 1.1823\n",
            "Iteration 32, Batch 37/100: Batch Loss = 1.3059\n",
            "Iteration 32, Batch 38/100: Batch Loss = 1.2104\n",
            "Iteration 32, Batch 39/100: Batch Loss = 1.3017\n",
            "Iteration 32, Batch 40/100: Batch Loss = 1.3762\n",
            "Iteration 32, Batch 41/100: Batch Loss = 1.2464\n",
            "Iteration 32, Batch 42/100: Batch Loss = 1.3978\n",
            "Iteration 32, Batch 43/100: Batch Loss = 1.3194\n",
            "Iteration 32, Batch 44/100: Batch Loss = 1.2810\n",
            "Iteration 32, Batch 45/100: Batch Loss = 1.1924\n",
            "Iteration 32, Batch 46/100: Batch Loss = 1.3024\n",
            "Iteration 32, Batch 47/100: Batch Loss = 1.2792\n",
            "Iteration 32, Batch 48/100: Batch Loss = 1.3144\n",
            "Iteration 32, Batch 49/100: Batch Loss = 1.2864\n",
            "Iteration 32, Batch 50/100: Batch Loss = 1.0529\n",
            "Iteration 32, Batch 51/100: Batch Loss = 1.2148\n",
            "Iteration 32, Batch 52/100: Batch Loss = 1.2634\n",
            "Iteration 32, Batch 53/100: Batch Loss = 1.2371\n",
            "Iteration 32, Batch 54/100: Batch Loss = 1.2434\n",
            "Iteration 32, Batch 55/100: Batch Loss = 1.4034\n",
            "Iteration 32, Batch 56/100: Batch Loss = 1.3970\n",
            "Iteration 32, Batch 57/100: Batch Loss = 1.2071\n",
            "Iteration 32, Batch 58/100: Batch Loss = 1.3247\n",
            "Iteration 32, Batch 59/100: Batch Loss = 1.2336\n",
            "Iteration 32, Batch 60/100: Batch Loss = 1.3047\n",
            "Iteration 32, Batch 61/100: Batch Loss = 1.3016\n",
            "Iteration 32, Batch 62/100: Batch Loss = 1.2313\n",
            "Iteration 32, Batch 63/100: Batch Loss = 1.3484\n",
            "Iteration 32, Batch 64/100: Batch Loss = 1.2022\n",
            "Iteration 32, Batch 65/100: Batch Loss = 1.3867\n",
            "Iteration 32, Batch 66/100: Batch Loss = 1.2889\n",
            "Iteration 32, Batch 67/100: Batch Loss = 1.3053\n",
            "Iteration 32, Batch 68/100: Batch Loss = 1.2229\n",
            "Iteration 32, Batch 69/100: Batch Loss = 1.5042\n",
            "Iteration 32, Batch 70/100: Batch Loss = 1.2507\n",
            "Iteration 32, Batch 71/100: Batch Loss = 1.2969\n",
            "Iteration 32, Batch 72/100: Batch Loss = 1.2185\n",
            "Iteration 32, Batch 73/100: Batch Loss = 1.1727\n",
            "Iteration 32, Batch 74/100: Batch Loss = 1.2729\n",
            "Iteration 32, Batch 75/100: Batch Loss = 1.3396\n",
            "Iteration 32, Batch 76/100: Batch Loss = 1.3047\n",
            "Iteration 32, Batch 77/100: Batch Loss = 1.2344\n",
            "Iteration 32, Batch 78/100: Batch Loss = 1.1874\n",
            "Iteration 32, Batch 79/100: Batch Loss = 1.3408\n",
            "Iteration 32, Batch 80/100: Batch Loss = 1.3065\n",
            "Iteration 32, Batch 81/100: Batch Loss = 1.1983\n",
            "Iteration 32, Batch 82/100: Batch Loss = 1.2849\n",
            "Iteration 32, Batch 83/100: Batch Loss = 1.4224\n",
            "Iteration 32, Batch 84/100: Batch Loss = 1.3806\n",
            "Iteration 32, Batch 85/100: Batch Loss = 1.1758\n",
            "Iteration 32, Batch 86/100: Batch Loss = 1.2195\n",
            "Iteration 32, Batch 87/100: Batch Loss = 1.3851\n",
            "Iteration 32, Batch 88/100: Batch Loss = 1.2642\n",
            "Iteration 32, Batch 89/100: Batch Loss = 1.3519\n",
            "Iteration 32, Batch 90/100: Batch Loss = 1.3896\n",
            "Iteration 32, Batch 91/100: Batch Loss = 1.2516\n",
            "Iteration 32, Batch 92/100: Batch Loss = 1.1596\n",
            "Iteration 32, Batch 93/100: Batch Loss = 1.3108\n",
            "Iteration 32, Batch 94/100: Batch Loss = 1.3038\n",
            "Iteration 32, Batch 95/100: Batch Loss = 1.2370\n",
            "Iteration 32, Batch 96/100: Batch Loss = 1.3324\n",
            "Iteration 32, Batch 97/100: Batch Loss = 1.2534\n",
            "Iteration 32, Batch 98/100: Batch Loss = 1.2808\n",
            "Iteration 32, Batch 99/100: Batch Loss = 1.3186\n",
            "Iteration 32, Batch 100/100: Batch Loss = 1.2370\n",
            "Iteration 32: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 33, Batch 1/100: Batch Loss = 1.1818\n",
            "Iteration 33, Batch 2/100: Batch Loss = 1.3222\n",
            "Iteration 33, Batch 3/100: Batch Loss = 1.1244\n",
            "Iteration 33, Batch 4/100: Batch Loss = 1.1244\n",
            "Iteration 33, Batch 5/100: Batch Loss = 1.4081\n",
            "Iteration 33, Batch 6/100: Batch Loss = 1.3241\n",
            "Iteration 33, Batch 7/100: Batch Loss = 1.2736\n",
            "Iteration 33, Batch 8/100: Batch Loss = 1.3842\n",
            "Iteration 33, Batch 9/100: Batch Loss = 1.2627\n",
            "Iteration 33, Batch 10/100: Batch Loss = 1.2094\n",
            "Iteration 33, Batch 11/100: Batch Loss = 1.2738\n",
            "Iteration 33, Batch 12/100: Batch Loss = 1.2905\n",
            "Iteration 33, Batch 13/100: Batch Loss = 1.3050\n",
            "Iteration 33, Batch 14/100: Batch Loss = 1.3861\n",
            "Iteration 33, Batch 15/100: Batch Loss = 1.2883\n",
            "Iteration 33, Batch 16/100: Batch Loss = 1.2185\n",
            "Iteration 33, Batch 17/100: Batch Loss = 1.1929\n",
            "Iteration 33, Batch 18/100: Batch Loss = 1.2551\n",
            "Iteration 33, Batch 19/100: Batch Loss = 1.3309\n",
            "Iteration 33, Batch 20/100: Batch Loss = 1.1537\n",
            "Iteration 33, Batch 21/100: Batch Loss = 1.2901\n",
            "Iteration 33, Batch 22/100: Batch Loss = 1.1979\n",
            "Iteration 33, Batch 23/100: Batch Loss = 1.1290\n",
            "Iteration 33, Batch 24/100: Batch Loss = 1.3065\n",
            "Iteration 33, Batch 25/100: Batch Loss = 1.2287\n",
            "Iteration 33, Batch 26/100: Batch Loss = 1.2520\n",
            "Iteration 33, Batch 27/100: Batch Loss = 1.2558\n",
            "Iteration 33, Batch 28/100: Batch Loss = 1.2011\n",
            "Iteration 33, Batch 29/100: Batch Loss = 1.2175\n",
            "Iteration 33, Batch 30/100: Batch Loss = 1.2710\n",
            "Iteration 33, Batch 31/100: Batch Loss = 1.2880\n",
            "Iteration 33, Batch 32/100: Batch Loss = 1.2877\n",
            "Iteration 33, Batch 33/100: Batch Loss = 1.2448\n",
            "Iteration 33, Batch 34/100: Batch Loss = 1.3646\n",
            "Iteration 33, Batch 35/100: Batch Loss = 1.2341\n",
            "Iteration 33, Batch 36/100: Batch Loss = 1.2855\n",
            "Iteration 33, Batch 37/100: Batch Loss = 1.3086\n",
            "Iteration 33, Batch 38/100: Batch Loss = 1.3678\n",
            "Iteration 33, Batch 39/100: Batch Loss = 1.4974\n",
            "Iteration 33, Batch 40/100: Batch Loss = 1.1949\n",
            "Iteration 33, Batch 41/100: Batch Loss = 1.2728\n",
            "Iteration 33, Batch 42/100: Batch Loss = 1.3718\n",
            "Iteration 33, Batch 43/100: Batch Loss = 1.2165\n",
            "Iteration 33, Batch 44/100: Batch Loss = 1.2545\n",
            "Iteration 33, Batch 45/100: Batch Loss = 1.3112\n",
            "Iteration 33, Batch 46/100: Batch Loss = 1.2897\n",
            "Iteration 33, Batch 47/100: Batch Loss = 1.3451\n",
            "Iteration 33, Batch 48/100: Batch Loss = 1.2516\n",
            "Iteration 33, Batch 49/100: Batch Loss = 1.2664\n",
            "Iteration 33, Batch 50/100: Batch Loss = 1.1398\n",
            "Iteration 33, Batch 51/100: Batch Loss = 1.3965\n",
            "Iteration 33, Batch 52/100: Batch Loss = 1.2675\n",
            "Iteration 33, Batch 53/100: Batch Loss = 1.1960\n",
            "Iteration 33, Batch 54/100: Batch Loss = 1.3005\n",
            "Iteration 33, Batch 55/100: Batch Loss = 1.3515\n",
            "Iteration 33, Batch 56/100: Batch Loss = 1.3756\n",
            "Iteration 33, Batch 57/100: Batch Loss = 1.2280\n",
            "Iteration 33, Batch 58/100: Batch Loss = 1.2218\n",
            "Iteration 33, Batch 59/100: Batch Loss = 1.3208\n",
            "Iteration 33, Batch 60/100: Batch Loss = 1.1811\n",
            "Iteration 33, Batch 61/100: Batch Loss = 1.2950\n",
            "Iteration 33, Batch 62/100: Batch Loss = 1.2461\n",
            "Iteration 33, Batch 63/100: Batch Loss = 1.3380\n",
            "Iteration 33, Batch 64/100: Batch Loss = 1.3078\n",
            "Iteration 33, Batch 65/100: Batch Loss = 1.4196\n",
            "Iteration 33, Batch 66/100: Batch Loss = 1.4367\n",
            "Iteration 33, Batch 67/100: Batch Loss = 1.2608\n",
            "Iteration 33, Batch 68/100: Batch Loss = 1.4150\n",
            "Iteration 33, Batch 69/100: Batch Loss = 1.2064\n",
            "Iteration 33, Batch 70/100: Batch Loss = 1.3437\n",
            "Iteration 33, Batch 71/100: Batch Loss = 1.2442\n",
            "Iteration 33, Batch 72/100: Batch Loss = 1.2221\n",
            "Iteration 33, Batch 73/100: Batch Loss = 1.2049\n",
            "Iteration 33, Batch 74/100: Batch Loss = 1.2600\n",
            "Iteration 33, Batch 75/100: Batch Loss = 1.2824\n",
            "Iteration 33, Batch 76/100: Batch Loss = 1.2441\n",
            "Iteration 33, Batch 77/100: Batch Loss = 1.2218\n",
            "Iteration 33, Batch 78/100: Batch Loss = 1.3651\n",
            "Iteration 33, Batch 79/100: Batch Loss = 1.3311\n",
            "Iteration 33, Batch 80/100: Batch Loss = 1.2994\n",
            "Iteration 33, Batch 81/100: Batch Loss = 1.3513\n",
            "Iteration 33, Batch 82/100: Batch Loss = 1.2111\n",
            "Iteration 33, Batch 83/100: Batch Loss = 1.3735\n",
            "Iteration 33, Batch 84/100: Batch Loss = 1.3478\n",
            "Iteration 33, Batch 85/100: Batch Loss = 1.3437\n",
            "Iteration 33, Batch 86/100: Batch Loss = 1.3801\n",
            "Iteration 33, Batch 87/100: Batch Loss = 1.2776\n",
            "Iteration 33, Batch 88/100: Batch Loss = 1.3553\n",
            "Iteration 33, Batch 89/100: Batch Loss = 1.2369\n",
            "Iteration 33, Batch 90/100: Batch Loss = 1.2497\n",
            "Iteration 33, Batch 91/100: Batch Loss = 1.3714\n",
            "Iteration 33, Batch 92/100: Batch Loss = 1.1819\n",
            "Iteration 33, Batch 93/100: Batch Loss = 1.2791\n",
            "Iteration 33, Batch 94/100: Batch Loss = 1.2317\n",
            "Iteration 33, Batch 95/100: Batch Loss = 1.1451\n",
            "Iteration 33, Batch 96/100: Batch Loss = 1.3142\n",
            "Iteration 33, Batch 97/100: Batch Loss = 1.1965\n",
            "Iteration 33, Batch 98/100: Batch Loss = 1.3180\n",
            "Iteration 33, Batch 99/100: Batch Loss = 1.2719\n",
            "Iteration 33, Batch 100/100: Batch Loss = 1.4559\n",
            "Iteration 33: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 34, Batch 1/100: Batch Loss = 1.2350\n",
            "Iteration 34, Batch 2/100: Batch Loss = 1.1539\n",
            "Iteration 34, Batch 3/100: Batch Loss = 1.3770\n",
            "Iteration 34, Batch 4/100: Batch Loss = 1.3034\n",
            "Iteration 34, Batch 5/100: Batch Loss = 1.2606\n",
            "Iteration 34, Batch 6/100: Batch Loss = 1.2038\n",
            "Iteration 34, Batch 7/100: Batch Loss = 1.3027\n",
            "Iteration 34, Batch 8/100: Batch Loss = 1.2358\n",
            "Iteration 34, Batch 9/100: Batch Loss = 1.2683\n",
            "Iteration 34, Batch 10/100: Batch Loss = 1.2219\n",
            "Iteration 34, Batch 11/100: Batch Loss = 1.2075\n",
            "Iteration 34, Batch 12/100: Batch Loss = 1.2382\n",
            "Iteration 34, Batch 13/100: Batch Loss = 1.2973\n",
            "Iteration 34, Batch 14/100: Batch Loss = 1.3348\n",
            "Iteration 34, Batch 15/100: Batch Loss = 1.2124\n",
            "Iteration 34, Batch 16/100: Batch Loss = 1.3903\n",
            "Iteration 34, Batch 17/100: Batch Loss = 1.2849\n",
            "Iteration 34, Batch 18/100: Batch Loss = 1.2678\n",
            "Iteration 34, Batch 19/100: Batch Loss = 1.4746\n",
            "Iteration 34, Batch 20/100: Batch Loss = 1.3086\n",
            "Iteration 34, Batch 21/100: Batch Loss = 1.3000\n",
            "Iteration 34, Batch 22/100: Batch Loss = 1.2955\n",
            "Iteration 34, Batch 23/100: Batch Loss = 1.4410\n",
            "Iteration 34, Batch 24/100: Batch Loss = 1.2424\n",
            "Iteration 34, Batch 25/100: Batch Loss = 1.2537\n",
            "Iteration 34, Batch 26/100: Batch Loss = 1.3154\n",
            "Iteration 34, Batch 27/100: Batch Loss = 1.2241\n",
            "Iteration 34, Batch 28/100: Batch Loss = 1.3397\n",
            "Iteration 34, Batch 29/100: Batch Loss = 1.2433\n",
            "Iteration 34, Batch 30/100: Batch Loss = 1.1283\n",
            "Iteration 34, Batch 31/100: Batch Loss = 1.1210\n",
            "Iteration 34, Batch 32/100: Batch Loss = 1.2793\n",
            "Iteration 34, Batch 33/100: Batch Loss = 1.2331\n",
            "Iteration 34, Batch 34/100: Batch Loss = 1.2569\n",
            "Iteration 34, Batch 35/100: Batch Loss = 1.2490\n",
            "Iteration 34, Batch 36/100: Batch Loss = 1.3301\n",
            "Iteration 34, Batch 37/100: Batch Loss = 1.2863\n",
            "Iteration 34, Batch 38/100: Batch Loss = 1.2267\n",
            "Iteration 34, Batch 39/100: Batch Loss = 1.2945\n",
            "Iteration 34, Batch 40/100: Batch Loss = 1.2611\n",
            "Iteration 34, Batch 41/100: Batch Loss = 1.3239\n",
            "Iteration 34, Batch 42/100: Batch Loss = 1.2769\n",
            "Iteration 34, Batch 43/100: Batch Loss = 1.2821\n",
            "Iteration 34, Batch 44/100: Batch Loss = 1.3321\n",
            "Iteration 34, Batch 45/100: Batch Loss = 1.2994\n",
            "Iteration 34, Batch 46/100: Batch Loss = 1.1650\n",
            "Iteration 34, Batch 47/100: Batch Loss = 1.2436\n",
            "Iteration 34, Batch 48/100: Batch Loss = 1.3127\n",
            "Iteration 34, Batch 49/100: Batch Loss = 1.3696\n",
            "Iteration 34, Batch 50/100: Batch Loss = 1.2897\n",
            "Iteration 34, Batch 51/100: Batch Loss = 1.2522\n",
            "Iteration 34, Batch 52/100: Batch Loss = 1.2447\n",
            "Iteration 34, Batch 53/100: Batch Loss = 1.3750\n",
            "Iteration 34, Batch 54/100: Batch Loss = 1.3252\n",
            "Iteration 34, Batch 55/100: Batch Loss = 1.1948\n",
            "Iteration 34, Batch 56/100: Batch Loss = 1.3732\n",
            "Iteration 34, Batch 57/100: Batch Loss = 1.2195\n",
            "Iteration 34, Batch 58/100: Batch Loss = 1.3149\n",
            "Iteration 34, Batch 59/100: Batch Loss = 1.1835\n",
            "Iteration 34, Batch 60/100: Batch Loss = 1.3598\n",
            "Iteration 34, Batch 61/100: Batch Loss = 1.2706\n",
            "Iteration 34, Batch 62/100: Batch Loss = 1.2770\n",
            "Iteration 34, Batch 63/100: Batch Loss = 1.3091\n",
            "Iteration 34, Batch 64/100: Batch Loss = 1.1751\n",
            "Iteration 34, Batch 65/100: Batch Loss = 1.2371\n",
            "Iteration 34, Batch 66/100: Batch Loss = 1.3369\n",
            "Iteration 34, Batch 67/100: Batch Loss = 1.3484\n",
            "Iteration 34, Batch 68/100: Batch Loss = 1.3709\n",
            "Iteration 34, Batch 69/100: Batch Loss = 1.3192\n",
            "Iteration 34, Batch 70/100: Batch Loss = 1.3373\n",
            "Iteration 34, Batch 71/100: Batch Loss = 1.2177\n",
            "Iteration 34, Batch 72/100: Batch Loss = 1.2131\n",
            "Iteration 34, Batch 73/100: Batch Loss = 1.3153\n",
            "Iteration 34, Batch 74/100: Batch Loss = 1.2770\n",
            "Iteration 34, Batch 75/100: Batch Loss = 1.3905\n",
            "Iteration 34, Batch 76/100: Batch Loss = 1.3457\n",
            "Iteration 34, Batch 77/100: Batch Loss = 1.3125\n",
            "Iteration 34, Batch 78/100: Batch Loss = 1.3348\n",
            "Iteration 34, Batch 79/100: Batch Loss = 1.1986\n",
            "Iteration 34, Batch 80/100: Batch Loss = 1.2303\n",
            "Iteration 34, Batch 81/100: Batch Loss = 1.2623\n",
            "Iteration 34, Batch 82/100: Batch Loss = 1.2863\n",
            "Iteration 34, Batch 83/100: Batch Loss = 1.1366\n",
            "Iteration 34, Batch 84/100: Batch Loss = 1.3506\n",
            "Iteration 34, Batch 85/100: Batch Loss = 1.2432\n",
            "Iteration 34, Batch 86/100: Batch Loss = 1.3179\n",
            "Iteration 34, Batch 87/100: Batch Loss = 1.2631\n",
            "Iteration 34, Batch 88/100: Batch Loss = 1.2777\n",
            "Iteration 34, Batch 89/100: Batch Loss = 1.3086\n",
            "Iteration 34, Batch 90/100: Batch Loss = 1.1995\n",
            "Iteration 34, Batch 91/100: Batch Loss = 1.2461\n",
            "Iteration 34, Batch 92/100: Batch Loss = 1.2008\n",
            "Iteration 34, Batch 93/100: Batch Loss = 1.1793\n",
            "Iteration 34, Batch 94/100: Batch Loss = 1.3734\n",
            "Iteration 34, Batch 95/100: Batch Loss = 1.4458\n",
            "Iteration 34, Batch 96/100: Batch Loss = 1.3237\n",
            "Iteration 34, Batch 97/100: Batch Loss = 1.2759\n",
            "Iteration 34, Batch 98/100: Batch Loss = 1.2847\n",
            "Iteration 34, Batch 99/100: Batch Loss = 1.4044\n",
            "Iteration 34, Batch 100/100: Batch Loss = 1.2922\n",
            "Iteration 34: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 35, Batch 1/100: Batch Loss = 1.3013\n",
            "Iteration 35, Batch 2/100: Batch Loss = 1.1099\n",
            "Iteration 35, Batch 3/100: Batch Loss = 1.2888\n",
            "Iteration 35, Batch 4/100: Batch Loss = 1.2257\n",
            "Iteration 35, Batch 5/100: Batch Loss = 1.2663\n",
            "Iteration 35, Batch 6/100: Batch Loss = 1.4322\n",
            "Iteration 35, Batch 7/100: Batch Loss = 1.2535\n",
            "Iteration 35, Batch 8/100: Batch Loss = 1.2297\n",
            "Iteration 35, Batch 9/100: Batch Loss = 1.4357\n",
            "Iteration 35, Batch 10/100: Batch Loss = 1.2422\n",
            "Iteration 35, Batch 11/100: Batch Loss = 1.4191\n",
            "Iteration 35, Batch 12/100: Batch Loss = 1.2391\n",
            "Iteration 35, Batch 13/100: Batch Loss = 1.2826\n",
            "Iteration 35, Batch 14/100: Batch Loss = 1.3268\n",
            "Iteration 35, Batch 15/100: Batch Loss = 1.4403\n",
            "Iteration 35, Batch 16/100: Batch Loss = 1.2777\n",
            "Iteration 35, Batch 17/100: Batch Loss = 1.2662\n",
            "Iteration 35, Batch 18/100: Batch Loss = 1.2631\n",
            "Iteration 35, Batch 19/100: Batch Loss = 1.1785\n",
            "Iteration 35, Batch 20/100: Batch Loss = 1.3545\n",
            "Iteration 35, Batch 21/100: Batch Loss = 1.1239\n",
            "Iteration 35, Batch 22/100: Batch Loss = 1.3231\n",
            "Iteration 35, Batch 23/100: Batch Loss = 1.3526\n",
            "Iteration 35, Batch 24/100: Batch Loss = 1.2518\n",
            "Iteration 35, Batch 25/100: Batch Loss = 1.2980\n",
            "Iteration 35, Batch 26/100: Batch Loss = 1.4130\n",
            "Iteration 35, Batch 27/100: Batch Loss = 1.2853\n",
            "Iteration 35, Batch 28/100: Batch Loss = 1.2072\n",
            "Iteration 35, Batch 29/100: Batch Loss = 1.3257\n",
            "Iteration 35, Batch 30/100: Batch Loss = 1.2825\n",
            "Iteration 35, Batch 31/100: Batch Loss = 1.1947\n",
            "Iteration 35, Batch 32/100: Batch Loss = 1.2070\n",
            "Iteration 35, Batch 33/100: Batch Loss = 1.4204\n",
            "Iteration 35, Batch 34/100: Batch Loss = 1.2773\n",
            "Iteration 35, Batch 35/100: Batch Loss = 1.1674\n",
            "Iteration 35, Batch 36/100: Batch Loss = 1.3430\n",
            "Iteration 35, Batch 37/100: Batch Loss = 1.2462\n",
            "Iteration 35, Batch 38/100: Batch Loss = 1.2664\n",
            "Iteration 35, Batch 39/100: Batch Loss = 1.2900\n",
            "Iteration 35, Batch 40/100: Batch Loss = 1.2645\n",
            "Iteration 35, Batch 41/100: Batch Loss = 1.2536\n",
            "Iteration 35, Batch 42/100: Batch Loss = 1.3195\n",
            "Iteration 35, Batch 43/100: Batch Loss = 1.2220\n",
            "Iteration 35, Batch 44/100: Batch Loss = 1.2441\n",
            "Iteration 35, Batch 45/100: Batch Loss = 1.4046\n",
            "Iteration 35, Batch 46/100: Batch Loss = 1.1911\n",
            "Iteration 35, Batch 47/100: Batch Loss = 1.3195\n",
            "Iteration 35, Batch 48/100: Batch Loss = 1.2791\n",
            "Iteration 35, Batch 49/100: Batch Loss = 1.4392\n",
            "Iteration 35, Batch 50/100: Batch Loss = 1.3309\n",
            "Iteration 35, Batch 51/100: Batch Loss = 1.2188\n",
            "Iteration 35, Batch 52/100: Batch Loss = 1.2980\n",
            "Iteration 35, Batch 53/100: Batch Loss = 1.3597\n",
            "Iteration 35, Batch 54/100: Batch Loss = 1.3121\n",
            "Iteration 35, Batch 55/100: Batch Loss = 1.3816\n",
            "Iteration 35, Batch 56/100: Batch Loss = 1.2451\n",
            "Iteration 35, Batch 57/100: Batch Loss = 1.3014\n",
            "Iteration 35, Batch 58/100: Batch Loss = 1.4660\n",
            "Iteration 35, Batch 59/100: Batch Loss = 1.3307\n",
            "Iteration 35, Batch 60/100: Batch Loss = 1.3536\n",
            "Iteration 35, Batch 61/100: Batch Loss = 1.2501\n",
            "Iteration 35, Batch 62/100: Batch Loss = 1.2919\n",
            "Iteration 35, Batch 63/100: Batch Loss = 1.3545\n",
            "Iteration 35, Batch 64/100: Batch Loss = 1.4124\n",
            "Iteration 35, Batch 65/100: Batch Loss = 1.1368\n",
            "Iteration 35, Batch 66/100: Batch Loss = 1.1824\n",
            "Iteration 35, Batch 67/100: Batch Loss = 1.3305\n",
            "Iteration 35, Batch 68/100: Batch Loss = 1.2993\n",
            "Iteration 35, Batch 69/100: Batch Loss = 1.2261\n",
            "Iteration 35, Batch 70/100: Batch Loss = 1.2411\n",
            "Iteration 35, Batch 71/100: Batch Loss = 1.3414\n",
            "Iteration 35, Batch 72/100: Batch Loss = 1.3251\n",
            "Iteration 35, Batch 73/100: Batch Loss = 1.2146\n",
            "Iteration 35, Batch 74/100: Batch Loss = 1.2643\n",
            "Iteration 35, Batch 75/100: Batch Loss = 1.2839\n",
            "Iteration 35, Batch 76/100: Batch Loss = 1.1786\n",
            "Iteration 35, Batch 77/100: Batch Loss = 1.2593\n",
            "Iteration 35, Batch 78/100: Batch Loss = 1.2706\n",
            "Iteration 35, Batch 79/100: Batch Loss = 1.3322\n",
            "Iteration 35, Batch 80/100: Batch Loss = 1.3106\n",
            "Iteration 35, Batch 81/100: Batch Loss = 1.2006\n",
            "Iteration 35, Batch 82/100: Batch Loss = 1.4554\n",
            "Iteration 35, Batch 83/100: Batch Loss = 1.2025\n",
            "Iteration 35, Batch 84/100: Batch Loss = 1.2738\n",
            "Iteration 35, Batch 85/100: Batch Loss = 1.2681\n",
            "Iteration 35, Batch 86/100: Batch Loss = 1.1089\n",
            "Iteration 35, Batch 87/100: Batch Loss = 1.2705\n",
            "Iteration 35, Batch 88/100: Batch Loss = 1.2130\n",
            "Iteration 35, Batch 89/100: Batch Loss = 1.1670\n",
            "Iteration 35, Batch 90/100: Batch Loss = 1.3308\n",
            "Iteration 35, Batch 91/100: Batch Loss = 1.1689\n",
            "Iteration 35, Batch 92/100: Batch Loss = 1.2869\n",
            "Iteration 35, Batch 93/100: Batch Loss = 1.1374\n",
            "Iteration 35, Batch 94/100: Batch Loss = 1.2425\n",
            "Iteration 35, Batch 95/100: Batch Loss = 1.2561\n",
            "Iteration 35, Batch 96/100: Batch Loss = 1.2648\n",
            "Iteration 35, Batch 97/100: Batch Loss = 1.2922\n",
            "Iteration 35, Batch 98/100: Batch Loss = 1.3160\n",
            "Iteration 35, Batch 99/100: Batch Loss = 1.2430\n",
            "Iteration 35, Batch 100/100: Batch Loss = 1.2811\n",
            "Iteration 35: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 36, Batch 1/100: Batch Loss = 1.4727\n",
            "Iteration 36, Batch 2/100: Batch Loss = 1.2717\n",
            "Iteration 36, Batch 3/100: Batch Loss = 1.2298\n",
            "Iteration 36, Batch 4/100: Batch Loss = 1.4594\n",
            "Iteration 36, Batch 5/100: Batch Loss = 1.2464\n",
            "Iteration 36, Batch 6/100: Batch Loss = 1.2552\n",
            "Iteration 36, Batch 7/100: Batch Loss = 1.2681\n",
            "Iteration 36, Batch 8/100: Batch Loss = 1.4424\n",
            "Iteration 36, Batch 9/100: Batch Loss = 1.1690\n",
            "Iteration 36, Batch 10/100: Batch Loss = 1.2815\n",
            "Iteration 36, Batch 11/100: Batch Loss = 1.2922\n",
            "Iteration 36, Batch 12/100: Batch Loss = 1.3198\n",
            "Iteration 36, Batch 13/100: Batch Loss = 1.2607\n",
            "Iteration 36, Batch 14/100: Batch Loss = 1.2208\n",
            "Iteration 36, Batch 15/100: Batch Loss = 1.2645\n",
            "Iteration 36, Batch 16/100: Batch Loss = 1.2996\n",
            "Iteration 36, Batch 17/100: Batch Loss = 1.2537\n",
            "Iteration 36, Batch 18/100: Batch Loss = 1.2093\n",
            "Iteration 36, Batch 19/100: Batch Loss = 1.2663\n",
            "Iteration 36, Batch 20/100: Batch Loss = 1.2755\n",
            "Iteration 36, Batch 21/100: Batch Loss = 1.1611\n",
            "Iteration 36, Batch 22/100: Batch Loss = 1.2694\n",
            "Iteration 36, Batch 23/100: Batch Loss = 1.2351\n",
            "Iteration 36, Batch 24/100: Batch Loss = 1.2219\n",
            "Iteration 36, Batch 25/100: Batch Loss = 1.2430\n",
            "Iteration 36, Batch 26/100: Batch Loss = 1.3692\n",
            "Iteration 36, Batch 27/100: Batch Loss = 1.2370\n",
            "Iteration 36, Batch 28/100: Batch Loss = 1.2238\n",
            "Iteration 36, Batch 29/100: Batch Loss = 1.3057\n",
            "Iteration 36, Batch 30/100: Batch Loss = 1.2480\n",
            "Iteration 36, Batch 31/100: Batch Loss = 1.1977\n",
            "Iteration 36, Batch 32/100: Batch Loss = 1.3067\n",
            "Iteration 36, Batch 33/100: Batch Loss = 1.4459\n",
            "Iteration 36, Batch 34/100: Batch Loss = 1.1907\n",
            "Iteration 36, Batch 35/100: Batch Loss = 1.2662\n",
            "Iteration 36, Batch 36/100: Batch Loss = 1.2772\n",
            "Iteration 36, Batch 37/100: Batch Loss = 1.3108\n",
            "Iteration 36, Batch 38/100: Batch Loss = 1.1241\n",
            "Iteration 36, Batch 39/100: Batch Loss = 1.3747\n",
            "Iteration 36, Batch 40/100: Batch Loss = 1.4176\n",
            "Iteration 36, Batch 41/100: Batch Loss = 1.1870\n",
            "Iteration 36, Batch 42/100: Batch Loss = 1.2015\n",
            "Iteration 36, Batch 43/100: Batch Loss = 1.2772\n",
            "Iteration 36, Batch 44/100: Batch Loss = 1.3830\n",
            "Iteration 36, Batch 45/100: Batch Loss = 1.3088\n",
            "Iteration 36, Batch 46/100: Batch Loss = 1.4046\n",
            "Iteration 36, Batch 47/100: Batch Loss = 1.2290\n",
            "Iteration 36, Batch 48/100: Batch Loss = 1.3585\n",
            "Iteration 36, Batch 49/100: Batch Loss = 1.1713\n",
            "Iteration 36, Batch 50/100: Batch Loss = 1.3276\n",
            "Iteration 36, Batch 51/100: Batch Loss = 1.1590\n",
            "Iteration 36, Batch 52/100: Batch Loss = 1.1942\n",
            "Iteration 36, Batch 53/100: Batch Loss = 1.1514\n",
            "Iteration 36, Batch 54/100: Batch Loss = 1.3274\n",
            "Iteration 36, Batch 55/100: Batch Loss = 1.1973\n",
            "Iteration 36, Batch 56/100: Batch Loss = 1.4006\n",
            "Iteration 36, Batch 57/100: Batch Loss = 1.3841\n",
            "Iteration 36, Batch 58/100: Batch Loss = 1.2623\n",
            "Iteration 36, Batch 59/100: Batch Loss = 1.3310\n",
            "Iteration 36, Batch 60/100: Batch Loss = 1.1450\n",
            "Iteration 36, Batch 61/100: Batch Loss = 1.3005\n",
            "Iteration 36, Batch 62/100: Batch Loss = 1.2886\n",
            "Iteration 36, Batch 63/100: Batch Loss = 1.2105\n",
            "Iteration 36, Batch 64/100: Batch Loss = 1.2943\n",
            "Iteration 36, Batch 65/100: Batch Loss = 1.1334\n",
            "Iteration 36, Batch 66/100: Batch Loss = 1.2925\n",
            "Iteration 36, Batch 67/100: Batch Loss = 1.2831\n",
            "Iteration 36, Batch 68/100: Batch Loss = 1.3031\n",
            "Iteration 36, Batch 69/100: Batch Loss = 1.1880\n",
            "Iteration 36, Batch 70/100: Batch Loss = 1.2367\n",
            "Iteration 36, Batch 71/100: Batch Loss = 1.3125\n",
            "Iteration 36, Batch 72/100: Batch Loss = 1.3231\n",
            "Iteration 36, Batch 73/100: Batch Loss = 1.2709\n",
            "Iteration 36, Batch 74/100: Batch Loss = 1.2723\n",
            "Iteration 36, Batch 75/100: Batch Loss = 1.3931\n",
            "Iteration 36, Batch 76/100: Batch Loss = 1.1905\n",
            "Iteration 36, Batch 77/100: Batch Loss = 1.2470\n",
            "Iteration 36, Batch 78/100: Batch Loss = 1.2756\n",
            "Iteration 36, Batch 79/100: Batch Loss = 1.3233\n",
            "Iteration 36, Batch 80/100: Batch Loss = 1.2951\n",
            "Iteration 36, Batch 81/100: Batch Loss = 1.3256\n",
            "Iteration 36, Batch 82/100: Batch Loss = 1.1646\n",
            "Iteration 36, Batch 83/100: Batch Loss = 1.4100\n",
            "Iteration 36, Batch 84/100: Batch Loss = 1.3160\n",
            "Iteration 36, Batch 85/100: Batch Loss = 1.2841\n",
            "Iteration 36, Batch 86/100: Batch Loss = 1.2757\n",
            "Iteration 36, Batch 87/100: Batch Loss = 1.2604\n",
            "Iteration 36, Batch 88/100: Batch Loss = 1.4233\n",
            "Iteration 36, Batch 89/100: Batch Loss = 1.2683\n",
            "Iteration 36, Batch 90/100: Batch Loss = 1.2776\n",
            "Iteration 36, Batch 91/100: Batch Loss = 1.3307\n",
            "Iteration 36, Batch 92/100: Batch Loss = 1.3554\n",
            "Iteration 36, Batch 93/100: Batch Loss = 1.3718\n",
            "Iteration 36, Batch 94/100: Batch Loss = 1.2866\n",
            "Iteration 36, Batch 95/100: Batch Loss = 1.2459\n",
            "Iteration 36, Batch 96/100: Batch Loss = 1.3199\n",
            "Iteration 36, Batch 97/100: Batch Loss = 1.2958\n",
            "Iteration 36, Batch 98/100: Batch Loss = 1.3936\n",
            "Iteration 36, Batch 99/100: Batch Loss = 1.2349\n",
            "Iteration 36, Batch 100/100: Batch Loss = 1.2685\n",
            "Iteration 36: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 37, Batch 1/100: Batch Loss = 1.3160\n",
            "Iteration 37, Batch 2/100: Batch Loss = 1.4154\n",
            "Iteration 37, Batch 3/100: Batch Loss = 1.3729\n",
            "Iteration 37, Batch 4/100: Batch Loss = 1.2331\n",
            "Iteration 37, Batch 5/100: Batch Loss = 1.1808\n",
            "Iteration 37, Batch 6/100: Batch Loss = 1.2467\n",
            "Iteration 37, Batch 7/100: Batch Loss = 1.3393\n",
            "Iteration 37, Batch 8/100: Batch Loss = 1.1975\n",
            "Iteration 37, Batch 9/100: Batch Loss = 1.2836\n",
            "Iteration 37, Batch 10/100: Batch Loss = 1.3598\n",
            "Iteration 37, Batch 11/100: Batch Loss = 1.1484\n",
            "Iteration 37, Batch 12/100: Batch Loss = 1.2802\n",
            "Iteration 37, Batch 13/100: Batch Loss = 1.3414\n",
            "Iteration 37, Batch 14/100: Batch Loss = 1.3144\n",
            "Iteration 37, Batch 15/100: Batch Loss = 1.2704\n",
            "Iteration 37, Batch 16/100: Batch Loss = 1.2256\n",
            "Iteration 37, Batch 17/100: Batch Loss = 1.3232\n",
            "Iteration 37, Batch 18/100: Batch Loss = 1.2768\n",
            "Iteration 37, Batch 19/100: Batch Loss = 1.2747\n",
            "Iteration 37, Batch 20/100: Batch Loss = 1.2452\n",
            "Iteration 37, Batch 21/100: Batch Loss = 1.2977\n",
            "Iteration 37, Batch 22/100: Batch Loss = 1.2201\n",
            "Iteration 37, Batch 23/100: Batch Loss = 1.3255\n",
            "Iteration 37, Batch 24/100: Batch Loss = 1.3625\n",
            "Iteration 37, Batch 25/100: Batch Loss = 1.4599\n",
            "Iteration 37, Batch 26/100: Batch Loss = 1.3674\n",
            "Iteration 37, Batch 27/100: Batch Loss = 1.2733\n",
            "Iteration 37, Batch 28/100: Batch Loss = 1.2301\n",
            "Iteration 37, Batch 29/100: Batch Loss = 1.3926\n",
            "Iteration 37, Batch 30/100: Batch Loss = 1.3063\n",
            "Iteration 37, Batch 31/100: Batch Loss = 1.3317\n",
            "Iteration 37, Batch 32/100: Batch Loss = 1.2586\n",
            "Iteration 37, Batch 33/100: Batch Loss = 1.1767\n",
            "Iteration 37, Batch 34/100: Batch Loss = 1.2537\n",
            "Iteration 37, Batch 35/100: Batch Loss = 1.3304\n",
            "Iteration 37, Batch 36/100: Batch Loss = 1.3607\n",
            "Iteration 37, Batch 37/100: Batch Loss = 1.3375\n",
            "Iteration 37, Batch 38/100: Batch Loss = 1.2047\n",
            "Iteration 37, Batch 39/100: Batch Loss = 1.1862\n",
            "Iteration 37, Batch 40/100: Batch Loss = 1.2996\n",
            "Iteration 37, Batch 41/100: Batch Loss = 1.2959\n",
            "Iteration 37, Batch 42/100: Batch Loss = 1.1946\n",
            "Iteration 37, Batch 43/100: Batch Loss = 1.3008\n",
            "Iteration 37, Batch 44/100: Batch Loss = 1.2522\n",
            "Iteration 37, Batch 45/100: Batch Loss = 1.1770\n",
            "Iteration 37, Batch 46/100: Batch Loss = 1.3404\n",
            "Iteration 37, Batch 47/100: Batch Loss = 1.2102\n",
            "Iteration 37, Batch 48/100: Batch Loss = 1.3172\n",
            "Iteration 37, Batch 49/100: Batch Loss = 1.1564\n",
            "Iteration 37, Batch 50/100: Batch Loss = 1.2663\n",
            "Iteration 37, Batch 51/100: Batch Loss = 1.3267\n",
            "Iteration 37, Batch 52/100: Batch Loss = 1.3500\n",
            "Iteration 37, Batch 53/100: Batch Loss = 1.1755\n",
            "Iteration 37, Batch 54/100: Batch Loss = 1.4013\n",
            "Iteration 37, Batch 55/100: Batch Loss = 1.2484\n",
            "Iteration 37, Batch 56/100: Batch Loss = 1.2957\n",
            "Iteration 37, Batch 57/100: Batch Loss = 1.2372\n",
            "Iteration 37, Batch 58/100: Batch Loss = 1.3624\n",
            "Iteration 37, Batch 59/100: Batch Loss = 1.3238\n",
            "Iteration 37, Batch 60/100: Batch Loss = 1.2806\n",
            "Iteration 37, Batch 61/100: Batch Loss = 1.1803\n",
            "Iteration 37, Batch 62/100: Batch Loss = 1.3013\n",
            "Iteration 37, Batch 63/100: Batch Loss = 1.2988\n",
            "Iteration 37, Batch 64/100: Batch Loss = 1.3600\n",
            "Iteration 37, Batch 65/100: Batch Loss = 1.2459\n",
            "Iteration 37, Batch 66/100: Batch Loss = 1.3881\n",
            "Iteration 37, Batch 67/100: Batch Loss = 1.3536\n",
            "Iteration 37, Batch 68/100: Batch Loss = 1.4378\n",
            "Iteration 37, Batch 69/100: Batch Loss = 1.3303\n",
            "Iteration 37, Batch 70/100: Batch Loss = 1.1195\n",
            "Iteration 37, Batch 71/100: Batch Loss = 1.2905\n",
            "Iteration 37, Batch 72/100: Batch Loss = 1.1729\n",
            "Iteration 37, Batch 73/100: Batch Loss = 1.2935\n",
            "Iteration 37, Batch 74/100: Batch Loss = 1.2882\n",
            "Iteration 37, Batch 75/100: Batch Loss = 1.2122\n",
            "Iteration 37, Batch 76/100: Batch Loss = 1.2188\n",
            "Iteration 37, Batch 77/100: Batch Loss = 1.2170\n",
            "Iteration 37, Batch 78/100: Batch Loss = 1.2333\n",
            "Iteration 37, Batch 79/100: Batch Loss = 1.2187\n",
            "Iteration 37, Batch 80/100: Batch Loss = 1.1781\n",
            "Iteration 37, Batch 81/100: Batch Loss = 1.4454\n",
            "Iteration 37, Batch 82/100: Batch Loss = 1.2368\n",
            "Iteration 37, Batch 83/100: Batch Loss = 1.2092\n",
            "Iteration 37, Batch 84/100: Batch Loss = 1.3382\n",
            "Iteration 37, Batch 85/100: Batch Loss = 1.2663\n",
            "Iteration 37, Batch 86/100: Batch Loss = 1.2017\n",
            "Iteration 37, Batch 87/100: Batch Loss = 1.1722\n",
            "Iteration 37, Batch 88/100: Batch Loss = 1.1571\n",
            "Iteration 37, Batch 89/100: Batch Loss = 1.2164\n",
            "Iteration 37, Batch 90/100: Batch Loss = 1.2867\n",
            "Iteration 37, Batch 91/100: Batch Loss = 1.4310\n",
            "Iteration 37, Batch 92/100: Batch Loss = 1.1885\n",
            "Iteration 37, Batch 93/100: Batch Loss = 1.2812\n",
            "Iteration 37, Batch 94/100: Batch Loss = 1.2848\n",
            "Iteration 37, Batch 95/100: Batch Loss = 1.3681\n",
            "Iteration 37, Batch 96/100: Batch Loss = 1.3513\n",
            "Iteration 37, Batch 97/100: Batch Loss = 1.4861\n",
            "Iteration 37, Batch 98/100: Batch Loss = 1.2500\n",
            "Iteration 37, Batch 99/100: Batch Loss = 1.2627\n",
            "Iteration 37, Batch 100/100: Batch Loss = 1.2221\n",
            "Iteration 37: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 38, Batch 1/100: Batch Loss = 1.2625\n",
            "Iteration 38, Batch 2/100: Batch Loss = 1.3549\n",
            "Iteration 38, Batch 3/100: Batch Loss = 1.2203\n",
            "Iteration 38, Batch 4/100: Batch Loss = 1.2553\n",
            "Iteration 38, Batch 5/100: Batch Loss = 1.3439\n",
            "Iteration 38, Batch 6/100: Batch Loss = 1.2809\n",
            "Iteration 38, Batch 7/100: Batch Loss = 1.1699\n",
            "Iteration 38, Batch 8/100: Batch Loss = 1.1697\n",
            "Iteration 38, Batch 9/100: Batch Loss = 1.2779\n",
            "Iteration 38, Batch 10/100: Batch Loss = 1.3582\n",
            "Iteration 38, Batch 11/100: Batch Loss = 1.2948\n",
            "Iteration 38, Batch 12/100: Batch Loss = 1.2649\n",
            "Iteration 38, Batch 13/100: Batch Loss = 1.1639\n",
            "Iteration 38, Batch 14/100: Batch Loss = 1.2311\n",
            "Iteration 38, Batch 15/100: Batch Loss = 1.2229\n",
            "Iteration 38, Batch 16/100: Batch Loss = 1.2314\n",
            "Iteration 38, Batch 17/100: Batch Loss = 1.2438\n",
            "Iteration 38, Batch 18/100: Batch Loss = 1.2640\n",
            "Iteration 38, Batch 19/100: Batch Loss = 1.0810\n",
            "Iteration 38, Batch 20/100: Batch Loss = 1.2234\n",
            "Iteration 38, Batch 21/100: Batch Loss = 1.1942\n",
            "Iteration 38, Batch 22/100: Batch Loss = 1.2074\n",
            "Iteration 38, Batch 23/100: Batch Loss = 1.2426\n",
            "Iteration 38, Batch 24/100: Batch Loss = 1.2331\n",
            "Iteration 38, Batch 25/100: Batch Loss = 1.3147\n",
            "Iteration 38, Batch 26/100: Batch Loss = 1.3122\n",
            "Iteration 38, Batch 27/100: Batch Loss = 1.2815\n",
            "Iteration 38, Batch 28/100: Batch Loss = 1.1711\n",
            "Iteration 38, Batch 29/100: Batch Loss = 1.2378\n",
            "Iteration 38, Batch 30/100: Batch Loss = 1.3266\n",
            "Iteration 38, Batch 31/100: Batch Loss = 1.3651\n",
            "Iteration 38, Batch 32/100: Batch Loss = 1.4239\n",
            "Iteration 38, Batch 33/100: Batch Loss = 1.2201\n",
            "Iteration 38, Batch 34/100: Batch Loss = 1.2167\n",
            "Iteration 38, Batch 35/100: Batch Loss = 1.2312\n",
            "Iteration 38, Batch 36/100: Batch Loss = 1.2200\n",
            "Iteration 38, Batch 37/100: Batch Loss = 1.2363\n",
            "Iteration 38, Batch 38/100: Batch Loss = 1.3424\n",
            "Iteration 38, Batch 39/100: Batch Loss = 1.2354\n",
            "Iteration 38, Batch 40/100: Batch Loss = 1.3730\n",
            "Iteration 38, Batch 41/100: Batch Loss = 1.2435\n",
            "Iteration 38, Batch 42/100: Batch Loss = 1.3328\n",
            "Iteration 38, Batch 43/100: Batch Loss = 1.1996\n",
            "Iteration 38, Batch 44/100: Batch Loss = 1.3353\n",
            "Iteration 38, Batch 45/100: Batch Loss = 1.3423\n",
            "Iteration 38, Batch 46/100: Batch Loss = 1.2158\n",
            "Iteration 38, Batch 47/100: Batch Loss = 1.3020\n",
            "Iteration 38, Batch 48/100: Batch Loss = 1.3474\n",
            "Iteration 38, Batch 49/100: Batch Loss = 1.2258\n",
            "Iteration 38, Batch 50/100: Batch Loss = 1.3014\n",
            "Iteration 38, Batch 51/100: Batch Loss = 1.3851\n",
            "Iteration 38, Batch 52/100: Batch Loss = 1.1709\n",
            "Iteration 38, Batch 53/100: Batch Loss = 1.2714\n",
            "Iteration 38, Batch 54/100: Batch Loss = 1.2722\n",
            "Iteration 38, Batch 55/100: Batch Loss = 1.2174\n",
            "Iteration 38, Batch 56/100: Batch Loss = 1.3113\n",
            "Iteration 38, Batch 57/100: Batch Loss = 1.1899\n",
            "Iteration 38, Batch 58/100: Batch Loss = 1.2844\n",
            "Iteration 38, Batch 59/100: Batch Loss = 1.3458\n",
            "Iteration 38, Batch 60/100: Batch Loss = 1.4359\n",
            "Iteration 38, Batch 61/100: Batch Loss = 1.2234\n",
            "Iteration 38, Batch 62/100: Batch Loss = 1.4284\n",
            "Iteration 38, Batch 63/100: Batch Loss = 1.2490\n",
            "Iteration 38, Batch 64/100: Batch Loss = 1.2496\n",
            "Iteration 38, Batch 65/100: Batch Loss = 1.4092\n",
            "Iteration 38, Batch 66/100: Batch Loss = 1.2591\n",
            "Iteration 38, Batch 67/100: Batch Loss = 1.3181\n",
            "Iteration 38, Batch 68/100: Batch Loss = 1.2900\n",
            "Iteration 38, Batch 69/100: Batch Loss = 1.2133\n",
            "Iteration 38, Batch 70/100: Batch Loss = 1.2530\n",
            "Iteration 38, Batch 71/100: Batch Loss = 1.3498\n",
            "Iteration 38, Batch 72/100: Batch Loss = 1.3364\n",
            "Iteration 38, Batch 73/100: Batch Loss = 1.1809\n",
            "Iteration 38, Batch 74/100: Batch Loss = 1.4379\n",
            "Iteration 38, Batch 75/100: Batch Loss = 1.4314\n",
            "Iteration 38, Batch 76/100: Batch Loss = 1.2537\n",
            "Iteration 38, Batch 77/100: Batch Loss = 1.3387\n",
            "Iteration 38, Batch 78/100: Batch Loss = 1.2629\n",
            "Iteration 38, Batch 79/100: Batch Loss = 1.4476\n",
            "Iteration 38, Batch 80/100: Batch Loss = 1.1981\n",
            "Iteration 38, Batch 81/100: Batch Loss = 1.3293\n",
            "Iteration 38, Batch 82/100: Batch Loss = 1.2869\n",
            "Iteration 38, Batch 83/100: Batch Loss = 1.2662\n",
            "Iteration 38, Batch 84/100: Batch Loss = 1.2807\n",
            "Iteration 38, Batch 85/100: Batch Loss = 1.2491\n",
            "Iteration 38, Batch 86/100: Batch Loss = 1.2752\n",
            "Iteration 38, Batch 87/100: Batch Loss = 1.3856\n",
            "Iteration 38, Batch 88/100: Batch Loss = 1.3217\n",
            "Iteration 38, Batch 89/100: Batch Loss = 1.1816\n",
            "Iteration 38, Batch 90/100: Batch Loss = 1.1963\n",
            "Iteration 38, Batch 91/100: Batch Loss = 1.2977\n",
            "Iteration 38, Batch 92/100: Batch Loss = 1.4325\n",
            "Iteration 38, Batch 93/100: Batch Loss = 1.2016\n",
            "Iteration 38, Batch 94/100: Batch Loss = 1.3108\n",
            "Iteration 38, Batch 95/100: Batch Loss = 1.3734\n",
            "Iteration 38, Batch 96/100: Batch Loss = 1.2591\n",
            "Iteration 38, Batch 97/100: Batch Loss = 1.3570\n",
            "Iteration 38, Batch 98/100: Batch Loss = 1.3035\n",
            "Iteration 38, Batch 99/100: Batch Loss = 1.3035\n",
            "Iteration 38, Batch 100/100: Batch Loss = 1.3568\n",
            "Iteration 38: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 39, Batch 1/100: Batch Loss = 1.2701\n",
            "Iteration 39, Batch 2/100: Batch Loss = 1.3104\n",
            "Iteration 39, Batch 3/100: Batch Loss = 1.2022\n",
            "Iteration 39, Batch 4/100: Batch Loss = 1.2773\n",
            "Iteration 39, Batch 5/100: Batch Loss = 1.2276\n",
            "Iteration 39, Batch 6/100: Batch Loss = 1.3087\n",
            "Iteration 39, Batch 7/100: Batch Loss = 1.2349\n",
            "Iteration 39, Batch 8/100: Batch Loss = 1.3235\n",
            "Iteration 39, Batch 9/100: Batch Loss = 1.1405\n",
            "Iteration 39, Batch 10/100: Batch Loss = 1.3544\n",
            "Iteration 39, Batch 11/100: Batch Loss = 1.3082\n",
            "Iteration 39, Batch 12/100: Batch Loss = 1.3107\n",
            "Iteration 39, Batch 13/100: Batch Loss = 1.3109\n",
            "Iteration 39, Batch 14/100: Batch Loss = 1.3383\n",
            "Iteration 39, Batch 15/100: Batch Loss = 1.4328\n",
            "Iteration 39, Batch 16/100: Batch Loss = 1.2827\n",
            "Iteration 39, Batch 17/100: Batch Loss = 1.2714\n",
            "Iteration 39, Batch 18/100: Batch Loss = 1.2603\n",
            "Iteration 39, Batch 19/100: Batch Loss = 1.1262\n",
            "Iteration 39, Batch 20/100: Batch Loss = 1.3269\n",
            "Iteration 39, Batch 21/100: Batch Loss = 1.2030\n",
            "Iteration 39, Batch 22/100: Batch Loss = 1.1856\n",
            "Iteration 39, Batch 23/100: Batch Loss = 1.3581\n",
            "Iteration 39, Batch 24/100: Batch Loss = 1.2249\n",
            "Iteration 39, Batch 25/100: Batch Loss = 1.1800\n",
            "Iteration 39, Batch 26/100: Batch Loss = 1.3931\n",
            "Iteration 39, Batch 27/100: Batch Loss = 1.2280\n",
            "Iteration 39, Batch 28/100: Batch Loss = 1.1913\n",
            "Iteration 39, Batch 29/100: Batch Loss = 1.2173\n",
            "Iteration 39, Batch 30/100: Batch Loss = 1.2419\n",
            "Iteration 39, Batch 31/100: Batch Loss = 1.1402\n",
            "Iteration 39, Batch 32/100: Batch Loss = 1.1807\n",
            "Iteration 39, Batch 33/100: Batch Loss = 1.2017\n",
            "Iteration 39, Batch 34/100: Batch Loss = 1.3760\n",
            "Iteration 39, Batch 35/100: Batch Loss = 1.3746\n",
            "Iteration 39, Batch 36/100: Batch Loss = 1.3070\n",
            "Iteration 39, Batch 37/100: Batch Loss = 1.2032\n",
            "Iteration 39, Batch 38/100: Batch Loss = 1.4056\n",
            "Iteration 39, Batch 39/100: Batch Loss = 1.4057\n",
            "Iteration 39, Batch 40/100: Batch Loss = 1.3024\n",
            "Iteration 39, Batch 41/100: Batch Loss = 1.2014\n",
            "Iteration 39, Batch 42/100: Batch Loss = 1.3519\n",
            "Iteration 39, Batch 43/100: Batch Loss = 1.3177\n",
            "Iteration 39, Batch 44/100: Batch Loss = 1.4545\n",
            "Iteration 39, Batch 45/100: Batch Loss = 1.4004\n",
            "Iteration 39, Batch 46/100: Batch Loss = 1.2898\n",
            "Iteration 39, Batch 47/100: Batch Loss = 1.2886\n",
            "Iteration 39, Batch 48/100: Batch Loss = 1.2694\n",
            "Iteration 39, Batch 49/100: Batch Loss = 1.4756\n",
            "Iteration 39, Batch 50/100: Batch Loss = 1.1971\n",
            "Iteration 39, Batch 51/100: Batch Loss = 1.1010\n",
            "Iteration 39, Batch 52/100: Batch Loss = 1.2897\n",
            "Iteration 39, Batch 53/100: Batch Loss = 1.3333\n",
            "Iteration 39, Batch 54/100: Batch Loss = 1.1697\n",
            "Iteration 39, Batch 55/100: Batch Loss = 1.3896\n",
            "Iteration 39, Batch 56/100: Batch Loss = 1.1957\n",
            "Iteration 39, Batch 57/100: Batch Loss = 1.2653\n",
            "Iteration 39, Batch 58/100: Batch Loss = 1.2314\n",
            "Iteration 39, Batch 59/100: Batch Loss = 1.1206\n",
            "Iteration 39, Batch 60/100: Batch Loss = 1.3369\n",
            "Iteration 39, Batch 61/100: Batch Loss = 1.3827\n",
            "Iteration 39, Batch 62/100: Batch Loss = 1.2918\n",
            "Iteration 39, Batch 63/100: Batch Loss = 1.4197\n",
            "Iteration 39, Batch 64/100: Batch Loss = 1.3567\n",
            "Iteration 39, Batch 65/100: Batch Loss = 1.3513\n",
            "Iteration 39, Batch 66/100: Batch Loss = 1.2622\n",
            "Iteration 39, Batch 67/100: Batch Loss = 1.1611\n",
            "Iteration 39, Batch 68/100: Batch Loss = 1.2569\n",
            "Iteration 39, Batch 69/100: Batch Loss = 1.3151\n",
            "Iteration 39, Batch 70/100: Batch Loss = 1.2818\n",
            "Iteration 39, Batch 71/100: Batch Loss = 1.2166\n",
            "Iteration 39, Batch 72/100: Batch Loss = 1.3138\n",
            "Iteration 39, Batch 73/100: Batch Loss = 1.4414\n",
            "Iteration 39, Batch 74/100: Batch Loss = 1.2947\n",
            "Iteration 39, Batch 75/100: Batch Loss = 1.2885\n",
            "Iteration 39, Batch 76/100: Batch Loss = 1.2836\n",
            "Iteration 39, Batch 77/100: Batch Loss = 1.2884\n",
            "Iteration 39, Batch 78/100: Batch Loss = 1.3366\n",
            "Iteration 39, Batch 79/100: Batch Loss = 1.2810\n",
            "Iteration 39, Batch 80/100: Batch Loss = 1.2699\n",
            "Iteration 39, Batch 81/100: Batch Loss = 1.3215\n",
            "Iteration 39, Batch 82/100: Batch Loss = 1.2884\n",
            "Iteration 39, Batch 83/100: Batch Loss = 1.1909\n",
            "Iteration 39, Batch 84/100: Batch Loss = 1.3069\n",
            "Iteration 39, Batch 85/100: Batch Loss = 1.2331\n",
            "Iteration 39, Batch 86/100: Batch Loss = 1.2629\n",
            "Iteration 39, Batch 87/100: Batch Loss = 1.3052\n",
            "Iteration 39, Batch 88/100: Batch Loss = 1.3531\n",
            "Iteration 39, Batch 89/100: Batch Loss = 1.2663\n",
            "Iteration 39, Batch 90/100: Batch Loss = 1.3066\n",
            "Iteration 39, Batch 91/100: Batch Loss = 1.3212\n",
            "Iteration 39, Batch 92/100: Batch Loss = 1.3373\n",
            "Iteration 39, Batch 93/100: Batch Loss = 1.2866\n",
            "Iteration 39, Batch 94/100: Batch Loss = 1.3076\n",
            "Iteration 39, Batch 95/100: Batch Loss = 1.1937\n",
            "Iteration 39, Batch 96/100: Batch Loss = 1.3498\n",
            "Iteration 39, Batch 97/100: Batch Loss = 1.1258\n",
            "Iteration 39, Batch 98/100: Batch Loss = 1.2149\n",
            "Iteration 39, Batch 99/100: Batch Loss = 1.2262\n",
            "Iteration 39, Batch 100/100: Batch Loss = 1.2200\n",
            "Iteration 39: Train Loss = 1.2813, Val Loss = 1.2788\n",
            "Iteration 40, Batch 1/100: Batch Loss = 1.3403\n",
            "Iteration 40, Batch 2/100: Batch Loss = 1.2386\n",
            "Iteration 40, Batch 3/100: Batch Loss = 1.2472\n",
            "Iteration 40, Batch 4/100: Batch Loss = 1.3086\n",
            "Iteration 40, Batch 5/100: Batch Loss = 1.2239\n",
            "Iteration 40, Batch 6/100: Batch Loss = 1.3811\n",
            "Iteration 40, Batch 7/100: Batch Loss = 1.2250\n",
            "Iteration 40, Batch 8/100: Batch Loss = 1.1285\n",
            "Iteration 40, Batch 9/100: Batch Loss = 1.2130\n",
            "Iteration 40, Batch 10/100: Batch Loss = 1.2106\n",
            "Iteration 40, Batch 11/100: Batch Loss = 1.2680\n",
            "Iteration 40, Batch 12/100: Batch Loss = 1.3909\n",
            "Iteration 40, Batch 13/100: Batch Loss = 1.3463\n",
            "Iteration 40, Batch 14/100: Batch Loss = 1.4528\n",
            "Iteration 40, Batch 15/100: Batch Loss = 1.1667\n",
            "Iteration 40, Batch 16/100: Batch Loss = 1.2485\n",
            "Iteration 40, Batch 17/100: Batch Loss = 1.1751\n",
            "Iteration 40, Batch 18/100: Batch Loss = 1.1634\n",
            "Iteration 40, Batch 19/100: Batch Loss = 1.2954\n",
            "Iteration 40, Batch 20/100: Batch Loss = 1.3304\n",
            "Iteration 40, Batch 21/100: Batch Loss = 1.1751\n",
            "Iteration 40, Batch 22/100: Batch Loss = 1.1996\n",
            "Iteration 40, Batch 23/100: Batch Loss = 1.2054\n",
            "Iteration 40, Batch 24/100: Batch Loss = 1.3016\n",
            "Iteration 40, Batch 25/100: Batch Loss = 1.3308\n",
            "Iteration 40, Batch 26/100: Batch Loss = 1.2954\n",
            "Iteration 40, Batch 27/100: Batch Loss = 1.3477\n",
            "Iteration 40, Batch 28/100: Batch Loss = 1.3995\n",
            "Iteration 40, Batch 29/100: Batch Loss = 1.2761\n",
            "Iteration 40, Batch 30/100: Batch Loss = 1.2148\n",
            "Iteration 40, Batch 31/100: Batch Loss = 1.3021\n",
            "Iteration 40, Batch 32/100: Batch Loss = 1.2187\n",
            "Iteration 40, Batch 33/100: Batch Loss = 1.1793\n",
            "Iteration 40, Batch 34/100: Batch Loss = 1.2439\n",
            "Iteration 40, Batch 35/100: Batch Loss = 1.2964\n",
            "Iteration 40, Batch 36/100: Batch Loss = 1.4110\n",
            "Iteration 40, Batch 37/100: Batch Loss = 1.2756\n",
            "Iteration 40, Batch 38/100: Batch Loss = 1.1863\n",
            "Iteration 40, Batch 39/100: Batch Loss = 1.3258\n",
            "Iteration 40, Batch 40/100: Batch Loss = 1.3145\n",
            "Iteration 40, Batch 41/100: Batch Loss = 1.2772\n",
            "Iteration 40, Batch 42/100: Batch Loss = 1.2922\n",
            "Iteration 40, Batch 43/100: Batch Loss = 1.2236\n",
            "Iteration 40, Batch 44/100: Batch Loss = 1.2717\n",
            "Iteration 40, Batch 45/100: Batch Loss = 1.2106\n",
            "Iteration 40, Batch 46/100: Batch Loss = 1.3293\n",
            "Iteration 40, Batch 47/100: Batch Loss = 1.3199\n",
            "Iteration 40, Batch 48/100: Batch Loss = 1.5084\n",
            "Iteration 40, Batch 49/100: Batch Loss = 1.3088\n",
            "Iteration 40, Batch 50/100: Batch Loss = 1.1061\n",
            "Iteration 40, Batch 51/100: Batch Loss = 1.3607\n",
            "Iteration 40, Batch 52/100: Batch Loss = 1.1165\n",
            "Iteration 40, Batch 53/100: Batch Loss = 1.2899\n",
            "Iteration 40, Batch 54/100: Batch Loss = 1.3110\n",
            "Iteration 40, Batch 55/100: Batch Loss = 1.3591\n",
            "Iteration 40, Batch 56/100: Batch Loss = 1.3400\n",
            "Iteration 40, Batch 57/100: Batch Loss = 1.2534\n",
            "Iteration 40, Batch 58/100: Batch Loss = 1.2237\n",
            "Iteration 40, Batch 59/100: Batch Loss = 1.2384\n",
            "Iteration 40, Batch 60/100: Batch Loss = 1.3378\n",
            "Iteration 40, Batch 61/100: Batch Loss = 1.2274\n",
            "Iteration 40, Batch 62/100: Batch Loss = 1.1124\n",
            "Iteration 40, Batch 63/100: Batch Loss = 1.2539\n",
            "Iteration 40, Batch 64/100: Batch Loss = 1.3126\n",
            "Iteration 40, Batch 65/100: Batch Loss = 1.3387\n",
            "Iteration 40, Batch 66/100: Batch Loss = 1.2791\n",
            "Iteration 40, Batch 67/100: Batch Loss = 1.1733\n",
            "Iteration 40, Batch 68/100: Batch Loss = 1.2367\n",
            "Iteration 40, Batch 69/100: Batch Loss = 1.2644\n",
            "Iteration 40, Batch 70/100: Batch Loss = 1.2812\n",
            "Iteration 40, Batch 71/100: Batch Loss = 1.2610\n",
            "Iteration 40, Batch 72/100: Batch Loss = 1.2382\n",
            "Iteration 40, Batch 73/100: Batch Loss = 1.4357\n",
            "Iteration 40, Batch 74/100: Batch Loss = 1.2624\n",
            "Iteration 40, Batch 75/100: Batch Loss = 1.3369\n",
            "Iteration 40, Batch 76/100: Batch Loss = 1.4072\n",
            "Iteration 40, Batch 77/100: Batch Loss = 1.2887\n",
            "Iteration 40, Batch 78/100: Batch Loss = 1.2493\n",
            "Iteration 40, Batch 79/100: Batch Loss = 1.3516\n",
            "Iteration 40, Batch 80/100: Batch Loss = 1.2861\n",
            "Iteration 40, Batch 81/100: Batch Loss = 1.3628\n",
            "Iteration 40, Batch 82/100: Batch Loss = 1.2501\n",
            "Iteration 40, Batch 83/100: Batch Loss = 1.2236\n",
            "Iteration 40, Batch 84/100: Batch Loss = 1.3012\n",
            "Iteration 40, Batch 85/100: Batch Loss = 1.2372\n",
            "Iteration 40, Batch 86/100: Batch Loss = 1.3089\n",
            "Iteration 40, Batch 87/100: Batch Loss = 1.4179\n",
            "Iteration 40, Batch 88/100: Batch Loss = 1.3034\n",
            "Iteration 40, Batch 89/100: Batch Loss = 1.3556\n",
            "Iteration 40, Batch 90/100: Batch Loss = 1.2905\n",
            "Iteration 40, Batch 91/100: Batch Loss = 1.3253\n",
            "Iteration 40, Batch 92/100: Batch Loss = 1.3712\n",
            "Iteration 40, Batch 93/100: Batch Loss = 1.2678\n",
            "Iteration 40, Batch 94/100: Batch Loss = 1.2611\n",
            "Iteration 40, Batch 95/100: Batch Loss = 1.4922\n",
            "Iteration 40, Batch 96/100: Batch Loss = 1.1005\n",
            "Iteration 40, Batch 97/100: Batch Loss = 1.2351\n",
            "Iteration 40, Batch 98/100: Batch Loss = 1.2070\n",
            "Iteration 40, Batch 99/100: Batch Loss = 1.3290\n",
            "Iteration 40, Batch 100/100: Batch Loss = 1.3608\n",
            "Iteration 40: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 41, Batch 1/100: Batch Loss = 1.3290\n",
            "Iteration 41, Batch 2/100: Batch Loss = 1.1892\n",
            "Iteration 41, Batch 3/100: Batch Loss = 1.2680\n",
            "Iteration 41, Batch 4/100: Batch Loss = 1.2255\n",
            "Iteration 41, Batch 5/100: Batch Loss = 1.1979\n",
            "Iteration 41, Batch 6/100: Batch Loss = 1.3901\n",
            "Iteration 41, Batch 7/100: Batch Loss = 1.2351\n",
            "Iteration 41, Batch 8/100: Batch Loss = 1.2863\n",
            "Iteration 41, Batch 9/100: Batch Loss = 1.2496\n",
            "Iteration 41, Batch 10/100: Batch Loss = 1.2351\n",
            "Iteration 41, Batch 11/100: Batch Loss = 1.2053\n",
            "Iteration 41, Batch 12/100: Batch Loss = 1.2405\n",
            "Iteration 41, Batch 13/100: Batch Loss = 1.2775\n",
            "Iteration 41, Batch 14/100: Batch Loss = 1.2516\n",
            "Iteration 41, Batch 15/100: Batch Loss = 1.1830\n",
            "Iteration 41, Batch 16/100: Batch Loss = 1.2254\n",
            "Iteration 41, Batch 17/100: Batch Loss = 1.2105\n",
            "Iteration 41, Batch 18/100: Batch Loss = 1.1825\n",
            "Iteration 41, Batch 19/100: Batch Loss = 1.4616\n",
            "Iteration 41, Batch 20/100: Batch Loss = 1.2402\n",
            "Iteration 41, Batch 21/100: Batch Loss = 1.2943\n",
            "Iteration 41, Batch 22/100: Batch Loss = 1.2125\n",
            "Iteration 41, Batch 23/100: Batch Loss = 1.4036\n",
            "Iteration 41, Batch 24/100: Batch Loss = 1.2753\n",
            "Iteration 41, Batch 25/100: Batch Loss = 1.3034\n",
            "Iteration 41, Batch 26/100: Batch Loss = 1.1679\n",
            "Iteration 41, Batch 27/100: Batch Loss = 1.2086\n",
            "Iteration 41, Batch 28/100: Batch Loss = 1.4314\n",
            "Iteration 41, Batch 29/100: Batch Loss = 1.2923\n",
            "Iteration 41, Batch 30/100: Batch Loss = 1.2923\n",
            "Iteration 41, Batch 31/100: Batch Loss = 1.1870\n",
            "Iteration 41, Batch 32/100: Batch Loss = 1.2587\n",
            "Iteration 41, Batch 33/100: Batch Loss = 1.3218\n",
            "Iteration 41, Batch 34/100: Batch Loss = 1.3215\n",
            "Iteration 41, Batch 35/100: Batch Loss = 1.3479\n",
            "Iteration 41, Batch 36/100: Batch Loss = 1.3035\n",
            "Iteration 41, Batch 37/100: Batch Loss = 1.2791\n",
            "Iteration 41, Batch 38/100: Batch Loss = 1.3363\n",
            "Iteration 41, Batch 39/100: Batch Loss = 1.3551\n",
            "Iteration 41, Batch 40/100: Batch Loss = 1.2438\n",
            "Iteration 41, Batch 41/100: Batch Loss = 1.3452\n",
            "Iteration 41, Batch 42/100: Batch Loss = 1.2649\n",
            "Iteration 41, Batch 43/100: Batch Loss = 1.3476\n",
            "Iteration 41, Batch 44/100: Batch Loss = 1.1889\n",
            "Iteration 41, Batch 45/100: Batch Loss = 1.3180\n",
            "Iteration 41, Batch 46/100: Batch Loss = 1.2068\n",
            "Iteration 41, Batch 47/100: Batch Loss = 1.2122\n",
            "Iteration 41, Batch 48/100: Batch Loss = 1.2767\n",
            "Iteration 41, Batch 49/100: Batch Loss = 1.2302\n",
            "Iteration 41, Batch 50/100: Batch Loss = 1.3398\n",
            "Iteration 41, Batch 51/100: Batch Loss = 1.3681\n",
            "Iteration 41, Batch 52/100: Batch Loss = 1.3201\n",
            "Iteration 41, Batch 53/100: Batch Loss = 1.2852\n",
            "Iteration 41, Batch 54/100: Batch Loss = 1.2445\n",
            "Iteration 41, Batch 55/100: Batch Loss = 1.4067\n",
            "Iteration 41, Batch 56/100: Batch Loss = 1.2356\n",
            "Iteration 41, Batch 57/100: Batch Loss = 1.3116\n",
            "Iteration 41, Batch 58/100: Batch Loss = 1.2058\n",
            "Iteration 41, Batch 59/100: Batch Loss = 1.3023\n",
            "Iteration 41, Batch 60/100: Batch Loss = 1.2216\n",
            "Iteration 41, Batch 61/100: Batch Loss = 1.3177\n",
            "Iteration 41, Batch 62/100: Batch Loss = 1.2237\n",
            "Iteration 41, Batch 63/100: Batch Loss = 1.3670\n",
            "Iteration 41, Batch 64/100: Batch Loss = 1.2548\n",
            "Iteration 41, Batch 65/100: Batch Loss = 1.2931\n",
            "Iteration 41, Batch 66/100: Batch Loss = 1.2817\n",
            "Iteration 41, Batch 67/100: Batch Loss = 1.3422\n",
            "Iteration 41, Batch 68/100: Batch Loss = 1.2249\n",
            "Iteration 41, Batch 69/100: Batch Loss = 1.2752\n",
            "Iteration 41, Batch 70/100: Batch Loss = 1.3811\n",
            "Iteration 41, Batch 71/100: Batch Loss = 1.3082\n",
            "Iteration 41, Batch 72/100: Batch Loss = 1.2607\n",
            "Iteration 41, Batch 73/100: Batch Loss = 1.3223\n",
            "Iteration 41, Batch 74/100: Batch Loss = 1.3860\n",
            "Iteration 41, Batch 75/100: Batch Loss = 1.2384\n",
            "Iteration 41, Batch 76/100: Batch Loss = 1.1668\n",
            "Iteration 41, Batch 77/100: Batch Loss = 1.5115\n",
            "Iteration 41, Batch 78/100: Batch Loss = 1.3044\n",
            "Iteration 41, Batch 79/100: Batch Loss = 1.3404\n",
            "Iteration 41, Batch 80/100: Batch Loss = 1.2252\n",
            "Iteration 41, Batch 81/100: Batch Loss = 1.3160\n",
            "Iteration 41, Batch 82/100: Batch Loss = 1.2722\n",
            "Iteration 41, Batch 83/100: Batch Loss = 1.3566\n",
            "Iteration 41, Batch 84/100: Batch Loss = 1.2499\n",
            "Iteration 41, Batch 85/100: Batch Loss = 1.2199\n",
            "Iteration 41, Batch 86/100: Batch Loss = 1.1806\n",
            "Iteration 41, Batch 87/100: Batch Loss = 1.3043\n",
            "Iteration 41, Batch 88/100: Batch Loss = 1.3240\n",
            "Iteration 41, Batch 89/100: Batch Loss = 1.3412\n",
            "Iteration 41, Batch 90/100: Batch Loss = 1.2828\n",
            "Iteration 41, Batch 91/100: Batch Loss = 1.2086\n",
            "Iteration 41, Batch 92/100: Batch Loss = 1.3430\n",
            "Iteration 41, Batch 93/100: Batch Loss = 1.3070\n",
            "Iteration 41, Batch 94/100: Batch Loss = 1.2866\n",
            "Iteration 41, Batch 95/100: Batch Loss = 1.3093\n",
            "Iteration 41, Batch 96/100: Batch Loss = 1.2996\n",
            "Iteration 41, Batch 97/100: Batch Loss = 1.2222\n",
            "Iteration 41, Batch 98/100: Batch Loss = 1.2191\n",
            "Iteration 41, Batch 99/100: Batch Loss = 1.2116\n",
            "Iteration 41, Batch 100/100: Batch Loss = 1.2967\n",
            "Iteration 41: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 42, Batch 1/100: Batch Loss = 1.2642\n",
            "Iteration 42, Batch 2/100: Batch Loss = 1.3199\n",
            "Iteration 42, Batch 3/100: Batch Loss = 1.2845\n",
            "Iteration 42, Batch 4/100: Batch Loss = 1.3273\n",
            "Iteration 42, Batch 5/100: Batch Loss = 1.3853\n",
            "Iteration 42, Batch 6/100: Batch Loss = 1.1889\n",
            "Iteration 42, Batch 7/100: Batch Loss = 1.1969\n",
            "Iteration 42, Batch 8/100: Batch Loss = 1.3771\n",
            "Iteration 42, Batch 9/100: Batch Loss = 1.3330\n",
            "Iteration 42, Batch 10/100: Batch Loss = 1.2494\n",
            "Iteration 42, Batch 11/100: Batch Loss = 1.1487\n",
            "Iteration 42, Batch 12/100: Batch Loss = 1.3902\n",
            "Iteration 42, Batch 13/100: Batch Loss = 1.4064\n",
            "Iteration 42, Batch 14/100: Batch Loss = 1.2584\n",
            "Iteration 42, Batch 15/100: Batch Loss = 1.2442\n",
            "Iteration 42, Batch 16/100: Batch Loss = 1.2442\n",
            "Iteration 42, Batch 17/100: Batch Loss = 1.3086\n",
            "Iteration 42, Batch 18/100: Batch Loss = 1.3014\n",
            "Iteration 42, Batch 19/100: Batch Loss = 1.0951\n",
            "Iteration 42, Batch 20/100: Batch Loss = 1.2391\n",
            "Iteration 42, Batch 21/100: Batch Loss = 1.2034\n",
            "Iteration 42, Batch 22/100: Batch Loss = 1.2218\n",
            "Iteration 42, Batch 23/100: Batch Loss = 1.2975\n",
            "Iteration 42, Batch 24/100: Batch Loss = 1.1480\n",
            "Iteration 42, Batch 25/100: Batch Loss = 1.3553\n",
            "Iteration 42, Batch 26/100: Batch Loss = 1.1477\n",
            "Iteration 42, Batch 27/100: Batch Loss = 1.3697\n",
            "Iteration 42, Batch 28/100: Batch Loss = 1.1963\n",
            "Iteration 42, Batch 29/100: Batch Loss = 1.1344\n",
            "Iteration 42, Batch 30/100: Batch Loss = 1.3291\n",
            "Iteration 42, Batch 31/100: Batch Loss = 1.2072\n",
            "Iteration 42, Batch 32/100: Batch Loss = 1.3444\n",
            "Iteration 42, Batch 33/100: Batch Loss = 1.3217\n",
            "Iteration 42, Batch 34/100: Batch Loss = 1.3301\n",
            "Iteration 42, Batch 35/100: Batch Loss = 1.2256\n",
            "Iteration 42, Batch 36/100: Batch Loss = 1.3146\n",
            "Iteration 42, Batch 37/100: Batch Loss = 1.2161\n",
            "Iteration 42, Batch 38/100: Batch Loss = 1.3515\n",
            "Iteration 42, Batch 39/100: Batch Loss = 1.3926\n",
            "Iteration 42, Batch 40/100: Batch Loss = 1.1496\n",
            "Iteration 42, Batch 41/100: Batch Loss = 1.2086\n",
            "Iteration 42, Batch 42/100: Batch Loss = 1.1194\n",
            "Iteration 42, Batch 43/100: Batch Loss = 1.4074\n",
            "Iteration 42, Batch 44/100: Batch Loss = 1.3200\n",
            "Iteration 42, Batch 45/100: Batch Loss = 1.3049\n",
            "Iteration 42, Batch 46/100: Batch Loss = 1.3018\n",
            "Iteration 42, Batch 47/100: Batch Loss = 1.3514\n",
            "Iteration 42, Batch 48/100: Batch Loss = 1.2195\n",
            "Iteration 42, Batch 49/100: Batch Loss = 1.4547\n",
            "Iteration 42, Batch 50/100: Batch Loss = 1.3647\n",
            "Iteration 42, Batch 51/100: Batch Loss = 1.2744\n",
            "Iteration 42, Batch 52/100: Batch Loss = 1.2591\n",
            "Iteration 42, Batch 53/100: Batch Loss = 1.1740\n",
            "Iteration 42, Batch 54/100: Batch Loss = 1.2885\n",
            "Iteration 42, Batch 55/100: Batch Loss = 1.2721\n",
            "Iteration 42, Batch 56/100: Batch Loss = 1.0716\n",
            "Iteration 42, Batch 57/100: Batch Loss = 1.3836\n",
            "Iteration 42, Batch 58/100: Batch Loss = 1.2236\n",
            "Iteration 42, Batch 59/100: Batch Loss = 1.2589\n",
            "Iteration 42, Batch 60/100: Batch Loss = 1.4184\n",
            "Iteration 42, Batch 61/100: Batch Loss = 1.2310\n",
            "Iteration 42, Batch 62/100: Batch Loss = 1.2921\n",
            "Iteration 42, Batch 63/100: Batch Loss = 1.2198\n",
            "Iteration 42, Batch 64/100: Batch Loss = 1.3474\n",
            "Iteration 42, Batch 65/100: Batch Loss = 1.3531\n",
            "Iteration 42, Batch 66/100: Batch Loss = 1.2236\n",
            "Iteration 42, Batch 67/100: Batch Loss = 1.3614\n",
            "Iteration 42, Batch 68/100: Batch Loss = 1.2946\n",
            "Iteration 42, Batch 69/100: Batch Loss = 1.2193\n",
            "Iteration 42, Batch 70/100: Batch Loss = 1.3109\n",
            "Iteration 42, Batch 71/100: Batch Loss = 1.3052\n",
            "Iteration 42, Batch 72/100: Batch Loss = 1.2791\n",
            "Iteration 42, Batch 73/100: Batch Loss = 1.3144\n",
            "Iteration 42, Batch 74/100: Batch Loss = 1.2520\n",
            "Iteration 42, Batch 75/100: Batch Loss = 1.3298\n",
            "Iteration 42, Batch 76/100: Batch Loss = 1.1738\n",
            "Iteration 42, Batch 77/100: Batch Loss = 1.2811\n",
            "Iteration 42, Batch 78/100: Batch Loss = 1.2919\n",
            "Iteration 42, Batch 79/100: Batch Loss = 1.3179\n",
            "Iteration 42, Batch 80/100: Batch Loss = 1.3644\n",
            "Iteration 42, Batch 81/100: Batch Loss = 1.2034\n",
            "Iteration 42, Batch 82/100: Batch Loss = 1.3350\n",
            "Iteration 42, Batch 83/100: Batch Loss = 1.2872\n",
            "Iteration 42, Batch 84/100: Batch Loss = 1.4514\n",
            "Iteration 42, Batch 85/100: Batch Loss = 1.2570\n",
            "Iteration 42, Batch 86/100: Batch Loss = 1.2792\n",
            "Iteration 42, Batch 87/100: Batch Loss = 1.2625\n",
            "Iteration 42, Batch 88/100: Batch Loss = 1.4068\n",
            "Iteration 42, Batch 89/100: Batch Loss = 1.2460\n",
            "Iteration 42, Batch 90/100: Batch Loss = 1.1904\n",
            "Iteration 42, Batch 91/100: Batch Loss = 1.2590\n",
            "Iteration 42, Batch 92/100: Batch Loss = 1.3198\n",
            "Iteration 42, Batch 93/100: Batch Loss = 1.2443\n",
            "Iteration 42, Batch 94/100: Batch Loss = 1.3793\n",
            "Iteration 42, Batch 95/100: Batch Loss = 1.2995\n",
            "Iteration 42, Batch 96/100: Batch Loss = 1.3955\n",
            "Iteration 42, Batch 97/100: Batch Loss = 1.1295\n",
            "Iteration 42, Batch 98/100: Batch Loss = 1.3014\n",
            "Iteration 42, Batch 99/100: Batch Loss = 1.4015\n",
            "Iteration 42, Batch 100/100: Batch Loss = 1.2203\n",
            "Iteration 42: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 43, Batch 1/100: Batch Loss = 1.2422\n",
            "Iteration 43, Batch 2/100: Batch Loss = 1.2538\n",
            "Iteration 43, Batch 3/100: Batch Loss = 1.2218\n",
            "Iteration 43, Batch 4/100: Batch Loss = 1.3197\n",
            "Iteration 43, Batch 5/100: Batch Loss = 1.3271\n",
            "Iteration 43, Batch 6/100: Batch Loss = 1.2511\n",
            "Iteration 43, Batch 7/100: Batch Loss = 1.3625\n",
            "Iteration 43, Batch 8/100: Batch Loss = 1.1684\n",
            "Iteration 43, Batch 9/100: Batch Loss = 1.2849\n",
            "Iteration 43, Batch 10/100: Batch Loss = 1.2237\n",
            "Iteration 43, Batch 11/100: Batch Loss = 1.3377\n",
            "Iteration 43, Batch 12/100: Batch Loss = 1.2183\n",
            "Iteration 43, Batch 13/100: Batch Loss = 1.2302\n",
            "Iteration 43, Batch 14/100: Batch Loss = 1.3178\n",
            "Iteration 43, Batch 15/100: Batch Loss = 1.3041\n",
            "Iteration 43, Batch 16/100: Batch Loss = 1.4026\n",
            "Iteration 43, Batch 17/100: Batch Loss = 1.3280\n",
            "Iteration 43, Batch 18/100: Batch Loss = 1.2312\n",
            "Iteration 43, Batch 19/100: Batch Loss = 1.2476\n",
            "Iteration 43, Batch 20/100: Batch Loss = 1.3802\n",
            "Iteration 43, Batch 21/100: Batch Loss = 1.2808\n",
            "Iteration 43, Batch 22/100: Batch Loss = 1.2153\n",
            "Iteration 43, Batch 23/100: Batch Loss = 1.3187\n",
            "Iteration 43, Batch 24/100: Batch Loss = 1.2998\n",
            "Iteration 43, Batch 25/100: Batch Loss = 1.1511\n",
            "Iteration 43, Batch 26/100: Batch Loss = 1.2881\n",
            "Iteration 43, Batch 27/100: Batch Loss = 1.1463\n",
            "Iteration 43, Batch 28/100: Batch Loss = 1.3321\n",
            "Iteration 43, Batch 29/100: Batch Loss = 1.2047\n",
            "Iteration 43, Batch 30/100: Batch Loss = 1.3014\n",
            "Iteration 43, Batch 31/100: Batch Loss = 1.3437\n",
            "Iteration 43, Batch 32/100: Batch Loss = 1.4081\n",
            "Iteration 43, Batch 33/100: Batch Loss = 1.3554\n",
            "Iteration 43, Batch 34/100: Batch Loss = 1.3040\n",
            "Iteration 43, Batch 35/100: Batch Loss = 1.3744\n",
            "Iteration 43, Batch 36/100: Batch Loss = 1.4312\n",
            "Iteration 43, Batch 37/100: Batch Loss = 1.2372\n",
            "Iteration 43, Batch 38/100: Batch Loss = 1.3258\n",
            "Iteration 43, Batch 39/100: Batch Loss = 1.2021\n",
            "Iteration 43, Batch 40/100: Batch Loss = 1.1236\n",
            "Iteration 43, Batch 41/100: Batch Loss = 1.3410\n",
            "Iteration 43, Batch 42/100: Batch Loss = 1.2268\n",
            "Iteration 43, Batch 43/100: Batch Loss = 1.2210\n",
            "Iteration 43, Batch 44/100: Batch Loss = 1.2885\n",
            "Iteration 43, Batch 45/100: Batch Loss = 1.3585\n",
            "Iteration 43, Batch 46/100: Batch Loss = 1.1714\n",
            "Iteration 43, Batch 47/100: Batch Loss = 1.2529\n",
            "Iteration 43, Batch 48/100: Batch Loss = 1.3435\n",
            "Iteration 43, Batch 49/100: Batch Loss = 1.3482\n",
            "Iteration 43, Batch 50/100: Batch Loss = 1.2557\n",
            "Iteration 43, Batch 51/100: Batch Loss = 1.3403\n",
            "Iteration 43, Batch 52/100: Batch Loss = 1.2486\n",
            "Iteration 43, Batch 53/100: Batch Loss = 1.1708\n",
            "Iteration 43, Batch 54/100: Batch Loss = 1.2573\n",
            "Iteration 43, Batch 55/100: Batch Loss = 1.3263\n",
            "Iteration 43, Batch 56/100: Batch Loss = 1.3128\n",
            "Iteration 43, Batch 57/100: Batch Loss = 1.1929\n",
            "Iteration 43, Batch 58/100: Batch Loss = 1.4017\n",
            "Iteration 43, Batch 59/100: Batch Loss = 1.2928\n",
            "Iteration 43, Batch 60/100: Batch Loss = 1.2706\n",
            "Iteration 43, Batch 61/100: Batch Loss = 1.2441\n",
            "Iteration 43, Batch 62/100: Batch Loss = 1.2480\n",
            "Iteration 43, Batch 63/100: Batch Loss = 1.1903\n",
            "Iteration 43, Batch 64/100: Batch Loss = 1.2947\n",
            "Iteration 43, Batch 65/100: Batch Loss = 1.3180\n",
            "Iteration 43, Batch 66/100: Batch Loss = 1.1964\n",
            "Iteration 43, Batch 67/100: Batch Loss = 1.1437\n",
            "Iteration 43, Batch 68/100: Batch Loss = 1.1960\n",
            "Iteration 43, Batch 69/100: Batch Loss = 1.2235\n",
            "Iteration 43, Batch 70/100: Batch Loss = 1.3017\n",
            "Iteration 43, Batch 71/100: Batch Loss = 1.3717\n",
            "Iteration 43, Batch 72/100: Batch Loss = 1.2068\n",
            "Iteration 43, Batch 73/100: Batch Loss = 1.4351\n",
            "Iteration 43, Batch 74/100: Batch Loss = 1.3906\n",
            "Iteration 43, Batch 75/100: Batch Loss = 1.2287\n",
            "Iteration 43, Batch 76/100: Batch Loss = 1.2634\n",
            "Iteration 43, Batch 77/100: Batch Loss = 1.2291\n",
            "Iteration 43, Batch 78/100: Batch Loss = 1.2832\n",
            "Iteration 43, Batch 79/100: Batch Loss = 1.2836\n",
            "Iteration 43, Batch 80/100: Batch Loss = 1.2552\n",
            "Iteration 43, Batch 81/100: Batch Loss = 1.2736\n",
            "Iteration 43, Batch 82/100: Batch Loss = 1.1902\n",
            "Iteration 43, Batch 83/100: Batch Loss = 1.2294\n",
            "Iteration 43, Batch 84/100: Batch Loss = 1.2698\n",
            "Iteration 43, Batch 85/100: Batch Loss = 1.3271\n",
            "Iteration 43, Batch 86/100: Batch Loss = 1.3634\n",
            "Iteration 43, Batch 87/100: Batch Loss = 1.2143\n",
            "Iteration 43, Batch 88/100: Batch Loss = 1.3500\n",
            "Iteration 43, Batch 89/100: Batch Loss = 1.2106\n",
            "Iteration 43, Batch 90/100: Batch Loss = 1.2644\n",
            "Iteration 43, Batch 91/100: Batch Loss = 1.3088\n",
            "Iteration 43, Batch 92/100: Batch Loss = 1.4293\n",
            "Iteration 43, Batch 93/100: Batch Loss = 1.2666\n",
            "Iteration 43, Batch 94/100: Batch Loss = 1.3348\n",
            "Iteration 43, Batch 95/100: Batch Loss = 1.3755\n",
            "Iteration 43, Batch 96/100: Batch Loss = 1.2257\n",
            "Iteration 43, Batch 97/100: Batch Loss = 1.3052\n",
            "Iteration 43, Batch 98/100: Batch Loss = 1.3494\n",
            "Iteration 43, Batch 99/100: Batch Loss = 1.2627\n",
            "Iteration 43, Batch 100/100: Batch Loss = 1.3550\n",
            "Iteration 43: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 44, Batch 1/100: Batch Loss = 1.3144\n",
            "Iteration 44, Batch 2/100: Batch Loss = 1.2938\n",
            "Iteration 44, Batch 3/100: Batch Loss = 1.2072\n",
            "Iteration 44, Batch 4/100: Batch Loss = 1.3788\n",
            "Iteration 44, Batch 5/100: Batch Loss = 1.3512\n",
            "Iteration 44, Batch 6/100: Batch Loss = 1.3103\n",
            "Iteration 44, Batch 7/100: Batch Loss = 1.1761\n",
            "Iteration 44, Batch 8/100: Batch Loss = 1.3861\n",
            "Iteration 44, Batch 9/100: Batch Loss = 1.2421\n",
            "Iteration 44, Batch 10/100: Batch Loss = 1.3050\n",
            "Iteration 44, Batch 11/100: Batch Loss = 1.2800\n",
            "Iteration 44, Batch 12/100: Batch Loss = 1.1964\n",
            "Iteration 44, Batch 13/100: Batch Loss = 1.3157\n",
            "Iteration 44, Batch 14/100: Batch Loss = 1.1373\n",
            "Iteration 44, Batch 15/100: Batch Loss = 1.3939\n",
            "Iteration 44, Batch 16/100: Batch Loss = 1.3103\n",
            "Iteration 44, Batch 17/100: Batch Loss = 1.2238\n",
            "Iteration 44, Batch 18/100: Batch Loss = 1.2904\n",
            "Iteration 44, Batch 19/100: Batch Loss = 1.3047\n",
            "Iteration 44, Batch 20/100: Batch Loss = 1.2826\n",
            "Iteration 44, Batch 21/100: Batch Loss = 1.1983\n",
            "Iteration 44, Batch 22/100: Batch Loss = 1.3143\n",
            "Iteration 44, Batch 23/100: Batch Loss = 1.2399\n",
            "Iteration 44, Batch 24/100: Batch Loss = 1.2476\n",
            "Iteration 44, Batch 25/100: Batch Loss = 1.1529\n",
            "Iteration 44, Batch 26/100: Batch Loss = 1.1984\n",
            "Iteration 44, Batch 27/100: Batch Loss = 1.3739\n",
            "Iteration 44, Batch 28/100: Batch Loss = 1.2742\n",
            "Iteration 44, Batch 29/100: Batch Loss = 1.1607\n",
            "Iteration 44, Batch 30/100: Batch Loss = 1.2457\n",
            "Iteration 44, Batch 31/100: Batch Loss = 1.3012\n",
            "Iteration 44, Batch 32/100: Batch Loss = 1.2632\n",
            "Iteration 44, Batch 33/100: Batch Loss = 1.2141\n",
            "Iteration 44, Batch 34/100: Batch Loss = 1.1871\n",
            "Iteration 44, Batch 35/100: Batch Loss = 1.3329\n",
            "Iteration 44, Batch 36/100: Batch Loss = 1.2409\n",
            "Iteration 44, Batch 37/100: Batch Loss = 1.2852\n",
            "Iteration 44, Batch 38/100: Batch Loss = 1.3407\n",
            "Iteration 44, Batch 39/100: Batch Loss = 1.2408\n",
            "Iteration 44, Batch 40/100: Batch Loss = 1.2200\n",
            "Iteration 44, Batch 41/100: Batch Loss = 1.2550\n",
            "Iteration 44, Batch 42/100: Batch Loss = 1.2234\n",
            "Iteration 44, Batch 43/100: Batch Loss = 1.3032\n",
            "Iteration 44, Batch 44/100: Batch Loss = 1.1891\n",
            "Iteration 44, Batch 45/100: Batch Loss = 1.2588\n",
            "Iteration 44, Batch 46/100: Batch Loss = 1.1470\n",
            "Iteration 44, Batch 47/100: Batch Loss = 1.2699\n",
            "Iteration 44, Batch 48/100: Batch Loss = 1.2811\n",
            "Iteration 44, Batch 49/100: Batch Loss = 1.3072\n",
            "Iteration 44, Batch 50/100: Batch Loss = 1.2624\n",
            "Iteration 44, Batch 51/100: Batch Loss = 1.2363\n",
            "Iteration 44, Batch 52/100: Batch Loss = 1.5084\n",
            "Iteration 44, Batch 53/100: Batch Loss = 1.3832\n",
            "Iteration 44, Batch 54/100: Batch Loss = 1.2884\n",
            "Iteration 44, Batch 55/100: Batch Loss = 1.2721\n",
            "Iteration 44, Batch 56/100: Batch Loss = 1.3071\n",
            "Iteration 44, Batch 57/100: Batch Loss = 1.3534\n",
            "Iteration 44, Batch 58/100: Batch Loss = 1.1938\n",
            "Iteration 44, Batch 59/100: Batch Loss = 1.3124\n",
            "Iteration 44, Batch 60/100: Batch Loss = 1.2629\n",
            "Iteration 44, Batch 61/100: Batch Loss = 1.3516\n",
            "Iteration 44, Batch 62/100: Batch Loss = 1.2736\n",
            "Iteration 44, Batch 63/100: Batch Loss = 1.3423\n",
            "Iteration 44, Batch 64/100: Batch Loss = 1.1272\n",
            "Iteration 44, Batch 65/100: Batch Loss = 1.1938\n",
            "Iteration 44, Batch 66/100: Batch Loss = 1.3443\n",
            "Iteration 44, Batch 67/100: Batch Loss = 1.4613\n",
            "Iteration 44, Batch 68/100: Batch Loss = 1.3311\n",
            "Iteration 44, Batch 69/100: Batch Loss = 1.3716\n",
            "Iteration 44, Batch 70/100: Batch Loss = 1.3088\n",
            "Iteration 44, Batch 71/100: Batch Loss = 1.2457\n",
            "Iteration 44, Batch 72/100: Batch Loss = 1.2480\n",
            "Iteration 44, Batch 73/100: Batch Loss = 1.2419\n",
            "Iteration 44, Batch 74/100: Batch Loss = 1.2310\n",
            "Iteration 44, Batch 75/100: Batch Loss = 1.3315\n",
            "Iteration 44, Batch 76/100: Batch Loss = 1.3458\n",
            "Iteration 44, Batch 77/100: Batch Loss = 1.2431\n",
            "Iteration 44, Batch 78/100: Batch Loss = 1.3627\n",
            "Iteration 44, Batch 79/100: Batch Loss = 1.2098\n",
            "Iteration 44, Batch 80/100: Batch Loss = 1.2065\n",
            "Iteration 44, Batch 81/100: Batch Loss = 1.3817\n",
            "Iteration 44, Batch 82/100: Batch Loss = 1.3025\n",
            "Iteration 44, Batch 83/100: Batch Loss = 1.2703\n",
            "Iteration 44, Batch 84/100: Batch Loss = 1.2662\n",
            "Iteration 44, Batch 85/100: Batch Loss = 1.2636\n",
            "Iteration 44, Batch 86/100: Batch Loss = 1.2440\n",
            "Iteration 44, Batch 87/100: Batch Loss = 1.3069\n",
            "Iteration 44, Batch 88/100: Batch Loss = 1.3160\n",
            "Iteration 44, Batch 89/100: Batch Loss = 1.2633\n",
            "Iteration 44, Batch 90/100: Batch Loss = 1.2498\n",
            "Iteration 44, Batch 91/100: Batch Loss = 1.3480\n",
            "Iteration 44, Batch 92/100: Batch Loss = 1.2068\n",
            "Iteration 44, Batch 93/100: Batch Loss = 1.4148\n",
            "Iteration 44, Batch 94/100: Batch Loss = 1.3699\n",
            "Iteration 44, Batch 95/100: Batch Loss = 1.2554\n",
            "Iteration 44, Batch 96/100: Batch Loss = 1.3700\n",
            "Iteration 44, Batch 97/100: Batch Loss = 1.2349\n",
            "Iteration 44, Batch 98/100: Batch Loss = 1.3421\n",
            "Iteration 44, Batch 99/100: Batch Loss = 1.2513\n",
            "Iteration 44, Batch 100/100: Batch Loss = 1.3497\n",
            "Iteration 44: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 45, Batch 1/100: Batch Loss = 1.2645\n",
            "Iteration 45, Batch 2/100: Batch Loss = 1.3031\n",
            "Iteration 45, Batch 3/100: Batch Loss = 1.3787\n",
            "Iteration 45, Batch 4/100: Batch Loss = 1.2535\n",
            "Iteration 45, Batch 5/100: Batch Loss = 1.2755\n",
            "Iteration 45, Batch 6/100: Batch Loss = 1.3489\n",
            "Iteration 45, Batch 7/100: Batch Loss = 1.2939\n",
            "Iteration 45, Batch 8/100: Batch Loss = 1.3238\n",
            "Iteration 45, Batch 9/100: Batch Loss = 1.3237\n",
            "Iteration 45, Batch 10/100: Batch Loss = 1.1262\n",
            "Iteration 45, Batch 11/100: Batch Loss = 1.3733\n",
            "Iteration 45, Batch 12/100: Batch Loss = 1.4525\n",
            "Iteration 45, Batch 13/100: Batch Loss = 1.2112\n",
            "Iteration 45, Batch 14/100: Batch Loss = 1.2567\n",
            "Iteration 45, Batch 15/100: Batch Loss = 1.3249\n",
            "Iteration 45, Batch 16/100: Batch Loss = 1.2835\n",
            "Iteration 45, Batch 17/100: Batch Loss = 1.4650\n",
            "Iteration 45, Batch 18/100: Batch Loss = 1.2931\n",
            "Iteration 45, Batch 19/100: Batch Loss = 1.2578\n",
            "Iteration 45, Batch 20/100: Batch Loss = 1.2502\n",
            "Iteration 45, Batch 21/100: Batch Loss = 1.3151\n",
            "Iteration 45, Batch 22/100: Batch Loss = 1.3179\n",
            "Iteration 45, Batch 23/100: Batch Loss = 1.3026\n",
            "Iteration 45, Batch 24/100: Batch Loss = 1.2868\n",
            "Iteration 45, Batch 25/100: Batch Loss = 1.3893\n",
            "Iteration 45, Batch 26/100: Batch Loss = 1.2496\n",
            "Iteration 45, Batch 27/100: Batch Loss = 1.3772\n",
            "Iteration 45, Batch 28/100: Batch Loss = 1.3122\n",
            "Iteration 45, Batch 29/100: Batch Loss = 1.3235\n",
            "Iteration 45, Batch 30/100: Batch Loss = 1.2341\n",
            "Iteration 45, Batch 31/100: Batch Loss = 1.3198\n",
            "Iteration 45, Batch 32/100: Batch Loss = 1.2643\n",
            "Iteration 45, Batch 33/100: Batch Loss = 1.2497\n",
            "Iteration 45, Batch 34/100: Batch Loss = 1.2660\n",
            "Iteration 45, Batch 35/100: Batch Loss = 1.2474\n",
            "Iteration 45, Batch 36/100: Batch Loss = 1.1887\n",
            "Iteration 45, Batch 37/100: Batch Loss = 1.2479\n",
            "Iteration 45, Batch 38/100: Batch Loss = 1.2260\n",
            "Iteration 45, Batch 39/100: Batch Loss = 1.2118\n",
            "Iteration 45, Batch 40/100: Batch Loss = 1.2211\n",
            "Iteration 45, Batch 41/100: Batch Loss = 1.2001\n",
            "Iteration 45, Batch 42/100: Batch Loss = 1.1864\n",
            "Iteration 45, Batch 43/100: Batch Loss = 1.3765\n",
            "Iteration 45, Batch 44/100: Batch Loss = 1.3106\n",
            "Iteration 45, Batch 45/100: Batch Loss = 1.2523\n",
            "Iteration 45, Batch 46/100: Batch Loss = 1.1796\n",
            "Iteration 45, Batch 47/100: Batch Loss = 1.3433\n",
            "Iteration 45, Batch 48/100: Batch Loss = 1.2130\n",
            "Iteration 45, Batch 49/100: Batch Loss = 1.2316\n",
            "Iteration 45, Batch 50/100: Batch Loss = 1.2273\n",
            "Iteration 45, Batch 51/100: Batch Loss = 1.1526\n",
            "Iteration 45, Batch 52/100: Batch Loss = 1.3140\n",
            "Iteration 45, Batch 53/100: Batch Loss = 1.3710\n",
            "Iteration 45, Batch 54/100: Batch Loss = 1.2885\n",
            "Iteration 45, Batch 55/100: Batch Loss = 1.1543\n",
            "Iteration 45, Batch 56/100: Batch Loss = 1.3121\n",
            "Iteration 45, Batch 57/100: Batch Loss = 1.2568\n",
            "Iteration 45, Batch 58/100: Batch Loss = 1.4401\n",
            "Iteration 45, Batch 59/100: Batch Loss = 1.2728\n",
            "Iteration 45, Batch 60/100: Batch Loss = 1.2885\n",
            "Iteration 45, Batch 61/100: Batch Loss = 1.1478\n",
            "Iteration 45, Batch 62/100: Batch Loss = 1.1646\n",
            "Iteration 45, Batch 63/100: Batch Loss = 1.3617\n",
            "Iteration 45, Batch 64/100: Batch Loss = 1.1809\n",
            "Iteration 45, Batch 65/100: Batch Loss = 1.2595\n",
            "Iteration 45, Batch 66/100: Batch Loss = 1.3753\n",
            "Iteration 45, Batch 67/100: Batch Loss = 1.3492\n",
            "Iteration 45, Batch 68/100: Batch Loss = 1.1986\n",
            "Iteration 45, Batch 69/100: Batch Loss = 1.2761\n",
            "Iteration 45, Batch 70/100: Batch Loss = 1.4292\n",
            "Iteration 45, Batch 71/100: Batch Loss = 1.2858\n",
            "Iteration 45, Batch 72/100: Batch Loss = 1.3784\n",
            "Iteration 45, Batch 73/100: Batch Loss = 1.2321\n",
            "Iteration 45, Batch 74/100: Batch Loss = 1.3651\n",
            "Iteration 45, Batch 75/100: Batch Loss = 1.2175\n",
            "Iteration 45, Batch 76/100: Batch Loss = 1.1225\n",
            "Iteration 45, Batch 77/100: Batch Loss = 1.2391\n",
            "Iteration 45, Batch 78/100: Batch Loss = 1.3312\n",
            "Iteration 45, Batch 79/100: Batch Loss = 1.2789\n",
            "Iteration 45, Batch 80/100: Batch Loss = 1.1726\n",
            "Iteration 45, Batch 81/100: Batch Loss = 1.3865\n",
            "Iteration 45, Batch 82/100: Batch Loss = 1.2648\n",
            "Iteration 45, Batch 83/100: Batch Loss = 1.2098\n",
            "Iteration 45, Batch 84/100: Batch Loss = 1.3377\n",
            "Iteration 45, Batch 85/100: Batch Loss = 1.3311\n",
            "Iteration 45, Batch 86/100: Batch Loss = 1.3216\n",
            "Iteration 45, Batch 87/100: Batch Loss = 1.4525\n",
            "Iteration 45, Batch 88/100: Batch Loss = 1.2444\n",
            "Iteration 45, Batch 89/100: Batch Loss = 1.2758\n",
            "Iteration 45, Batch 90/100: Batch Loss = 1.2675\n",
            "Iteration 45, Batch 91/100: Batch Loss = 1.1048\n",
            "Iteration 45, Batch 92/100: Batch Loss = 1.3575\n",
            "Iteration 45, Batch 93/100: Batch Loss = 1.1854\n",
            "Iteration 45, Batch 94/100: Batch Loss = 1.2406\n",
            "Iteration 45, Batch 95/100: Batch Loss = 1.3201\n",
            "Iteration 45, Batch 96/100: Batch Loss = 1.2349\n",
            "Iteration 45, Batch 97/100: Batch Loss = 1.2791\n",
            "Iteration 45, Batch 98/100: Batch Loss = 1.2367\n",
            "Iteration 45, Batch 99/100: Batch Loss = 1.2778\n",
            "Iteration 45, Batch 100/100: Batch Loss = 1.4514\n",
            "Iteration 45: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 46, Batch 1/100: Batch Loss = 1.3383\n",
            "Iteration 46, Batch 2/100: Batch Loss = 1.2662\n",
            "Iteration 46, Batch 3/100: Batch Loss = 1.2715\n",
            "Iteration 46, Batch 4/100: Batch Loss = 1.1973\n",
            "Iteration 46, Batch 5/100: Batch Loss = 1.2237\n",
            "Iteration 46, Batch 6/100: Batch Loss = 1.2441\n",
            "Iteration 46, Batch 7/100: Batch Loss = 1.1788\n",
            "Iteration 46, Batch 8/100: Batch Loss = 1.2571\n",
            "Iteration 46, Batch 9/100: Batch Loss = 1.1860\n",
            "Iteration 46, Batch 10/100: Batch Loss = 1.4556\n",
            "Iteration 46, Batch 11/100: Batch Loss = 1.2522\n",
            "Iteration 46, Batch 12/100: Batch Loss = 1.1845\n",
            "Iteration 46, Batch 13/100: Batch Loss = 1.2725\n",
            "Iteration 46, Batch 14/100: Batch Loss = 1.2645\n",
            "Iteration 46, Batch 15/100: Batch Loss = 1.3753\n",
            "Iteration 46, Batch 16/100: Batch Loss = 1.3543\n",
            "Iteration 46, Batch 17/100: Batch Loss = 1.3062\n",
            "Iteration 46, Batch 18/100: Batch Loss = 1.2203\n",
            "Iteration 46, Batch 19/100: Batch Loss = 1.2052\n",
            "Iteration 46, Batch 20/100: Batch Loss = 1.1836\n",
            "Iteration 46, Batch 21/100: Batch Loss = 1.2276\n",
            "Iteration 46, Batch 22/100: Batch Loss = 1.4253\n",
            "Iteration 46, Batch 23/100: Batch Loss = 1.2712\n",
            "Iteration 46, Batch 24/100: Batch Loss = 1.2275\n",
            "Iteration 46, Batch 25/100: Batch Loss = 1.2457\n",
            "Iteration 46, Batch 26/100: Batch Loss = 1.3065\n",
            "Iteration 46, Batch 27/100: Batch Loss = 1.2677\n",
            "Iteration 46, Batch 28/100: Batch Loss = 1.2827\n",
            "Iteration 46, Batch 29/100: Batch Loss = 1.2291\n",
            "Iteration 46, Batch 30/100: Batch Loss = 1.2918\n",
            "Iteration 46, Batch 31/100: Batch Loss = 1.1467\n",
            "Iteration 46, Batch 32/100: Batch Loss = 1.3045\n",
            "Iteration 46, Batch 33/100: Batch Loss = 1.3534\n",
            "Iteration 46, Batch 34/100: Batch Loss = 1.2075\n",
            "Iteration 46, Batch 35/100: Batch Loss = 1.4092\n",
            "Iteration 46, Batch 36/100: Batch Loss = 1.2880\n",
            "Iteration 46, Batch 37/100: Batch Loss = 1.2701\n",
            "Iteration 46, Batch 38/100: Batch Loss = 1.2388\n",
            "Iteration 46, Batch 39/100: Batch Loss = 1.2851\n",
            "Iteration 46, Batch 40/100: Batch Loss = 1.4405\n",
            "Iteration 46, Batch 41/100: Batch Loss = 1.3865\n",
            "Iteration 46, Batch 42/100: Batch Loss = 1.2533\n",
            "Iteration 46, Batch 43/100: Batch Loss = 1.3141\n",
            "Iteration 46, Batch 44/100: Batch Loss = 1.2534\n",
            "Iteration 46, Batch 45/100: Batch Loss = 1.2937\n",
            "Iteration 46, Batch 46/100: Batch Loss = 1.3887\n",
            "Iteration 46, Batch 47/100: Batch Loss = 1.2997\n",
            "Iteration 46, Batch 48/100: Batch Loss = 1.2161\n",
            "Iteration 46, Batch 49/100: Batch Loss = 1.3142\n",
            "Iteration 46, Batch 50/100: Batch Loss = 1.1629\n",
            "Iteration 46, Batch 51/100: Batch Loss = 1.2753\n",
            "Iteration 46, Batch 52/100: Batch Loss = 1.2922\n",
            "Iteration 46, Batch 53/100: Batch Loss = 1.1448\n",
            "Iteration 46, Batch 54/100: Batch Loss = 1.2257\n",
            "Iteration 46, Batch 55/100: Batch Loss = 1.2662\n",
            "Iteration 46, Batch 56/100: Batch Loss = 1.3223\n",
            "Iteration 46, Batch 57/100: Batch Loss = 1.4519\n",
            "Iteration 46, Batch 58/100: Batch Loss = 1.3441\n",
            "Iteration 46, Batch 59/100: Batch Loss = 1.2290\n",
            "Iteration 46, Batch 60/100: Batch Loss = 1.2327\n",
            "Iteration 46, Batch 61/100: Batch Loss = 1.1597\n",
            "Iteration 46, Batch 62/100: Batch Loss = 1.2416\n",
            "Iteration 46, Batch 63/100: Batch Loss = 1.4053\n",
            "Iteration 46, Batch 64/100: Batch Loss = 1.2769\n",
            "Iteration 46, Batch 65/100: Batch Loss = 1.3326\n",
            "Iteration 46, Batch 66/100: Batch Loss = 1.3291\n",
            "Iteration 46, Batch 67/100: Batch Loss = 1.3479\n",
            "Iteration 46, Batch 68/100: Batch Loss = 1.3028\n",
            "Iteration 46, Batch 69/100: Batch Loss = 1.3066\n",
            "Iteration 46, Batch 70/100: Batch Loss = 1.1905\n",
            "Iteration 46, Batch 71/100: Batch Loss = 1.1843\n",
            "Iteration 46, Batch 72/100: Batch Loss = 1.3421\n",
            "Iteration 46, Batch 73/100: Batch Loss = 1.2590\n",
            "Iteration 46, Batch 74/100: Batch Loss = 1.4160\n",
            "Iteration 46, Batch 75/100: Batch Loss = 1.2248\n",
            "Iteration 46, Batch 76/100: Batch Loss = 1.3128\n",
            "Iteration 46, Batch 77/100: Batch Loss = 1.1988\n",
            "Iteration 46, Batch 78/100: Batch Loss = 1.3039\n",
            "Iteration 46, Batch 79/100: Batch Loss = 1.3101\n",
            "Iteration 46, Batch 80/100: Batch Loss = 1.2887\n",
            "Iteration 46, Batch 81/100: Batch Loss = 1.3262\n",
            "Iteration 46, Batch 82/100: Batch Loss = 1.4220\n",
            "Iteration 46, Batch 83/100: Batch Loss = 1.2650\n",
            "Iteration 46, Batch 84/100: Batch Loss = 1.3259\n",
            "Iteration 46, Batch 85/100: Batch Loss = 1.3380\n",
            "Iteration 46, Batch 86/100: Batch Loss = 1.1665\n",
            "Iteration 46, Batch 87/100: Batch Loss = 1.2751\n",
            "Iteration 46, Batch 88/100: Batch Loss = 1.3833\n",
            "Iteration 46, Batch 89/100: Batch Loss = 1.2980\n",
            "Iteration 46, Batch 90/100: Batch Loss = 1.2786\n",
            "Iteration 46, Batch 91/100: Batch Loss = 1.2550\n",
            "Iteration 46, Batch 92/100: Batch Loss = 1.3328\n",
            "Iteration 46, Batch 93/100: Batch Loss = 1.2366\n",
            "Iteration 46, Batch 94/100: Batch Loss = 1.3260\n",
            "Iteration 46, Batch 95/100: Batch Loss = 1.2275\n",
            "Iteration 46, Batch 96/100: Batch Loss = 1.2109\n",
            "Iteration 46, Batch 97/100: Batch Loss = 1.1868\n",
            "Iteration 46, Batch 98/100: Batch Loss = 1.3347\n",
            "Iteration 46, Batch 99/100: Batch Loss = 1.2164\n",
            "Iteration 46, Batch 100/100: Batch Loss = 1.3661\n",
            "Iteration 46: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 47, Batch 1/100: Batch Loss = 1.2900\n",
            "Iteration 47, Batch 2/100: Batch Loss = 1.3659\n",
            "Iteration 47, Batch 3/100: Batch Loss = 1.2458\n",
            "Iteration 47, Batch 4/100: Batch Loss = 1.2760\n",
            "Iteration 47, Batch 5/100: Batch Loss = 1.3880\n",
            "Iteration 47, Batch 6/100: Batch Loss = 1.3475\n",
            "Iteration 47, Batch 7/100: Batch Loss = 1.3473\n",
            "Iteration 47, Batch 8/100: Batch Loss = 1.3730\n",
            "Iteration 47, Batch 9/100: Batch Loss = 1.3444\n",
            "Iteration 47, Batch 10/100: Batch Loss = 1.3070\n",
            "Iteration 47, Batch 11/100: Batch Loss = 1.2691\n",
            "Iteration 47, Batch 12/100: Batch Loss = 1.1890\n",
            "Iteration 47, Batch 13/100: Batch Loss = 1.1514\n",
            "Iteration 47, Batch 14/100: Batch Loss = 1.4606\n",
            "Iteration 47, Batch 15/100: Batch Loss = 1.3759\n",
            "Iteration 47, Batch 16/100: Batch Loss = 1.4252\n",
            "Iteration 47, Batch 17/100: Batch Loss = 1.3938\n",
            "Iteration 47, Batch 18/100: Batch Loss = 1.3957\n",
            "Iteration 47, Batch 19/100: Batch Loss = 1.2371\n",
            "Iteration 47, Batch 20/100: Batch Loss = 1.1754\n",
            "Iteration 47, Batch 21/100: Batch Loss = 1.3595\n",
            "Iteration 47, Batch 22/100: Batch Loss = 1.2634\n",
            "Iteration 47, Batch 23/100: Batch Loss = 1.2866\n",
            "Iteration 47, Batch 24/100: Batch Loss = 1.1918\n",
            "Iteration 47, Batch 25/100: Batch Loss = 1.2835\n",
            "Iteration 47, Batch 26/100: Batch Loss = 1.2128\n",
            "Iteration 47, Batch 27/100: Batch Loss = 1.3767\n",
            "Iteration 47, Batch 28/100: Batch Loss = 1.2355\n",
            "Iteration 47, Batch 29/100: Batch Loss = 1.2866\n",
            "Iteration 47, Batch 30/100: Batch Loss = 1.1932\n",
            "Iteration 47, Batch 31/100: Batch Loss = 1.2978\n",
            "Iteration 47, Batch 32/100: Batch Loss = 1.1247\n",
            "Iteration 47, Batch 33/100: Batch Loss = 1.2520\n",
            "Iteration 47, Batch 34/100: Batch Loss = 1.2109\n",
            "Iteration 47, Batch 35/100: Batch Loss = 1.1725\n",
            "Iteration 47, Batch 36/100: Batch Loss = 1.4195\n",
            "Iteration 47, Batch 37/100: Batch Loss = 1.2369\n",
            "Iteration 47, Batch 38/100: Batch Loss = 1.3973\n",
            "Iteration 47, Batch 39/100: Batch Loss = 1.1764\n",
            "Iteration 47, Batch 40/100: Batch Loss = 1.1576\n",
            "Iteration 47, Batch 41/100: Batch Loss = 1.3473\n",
            "Iteration 47, Batch 42/100: Batch Loss = 1.1663\n",
            "Iteration 47, Batch 43/100: Batch Loss = 1.2090\n",
            "Iteration 47, Batch 44/100: Batch Loss = 1.2946\n",
            "Iteration 47, Batch 45/100: Batch Loss = 1.3033\n",
            "Iteration 47, Batch 46/100: Batch Loss = 1.3157\n",
            "Iteration 47, Batch 47/100: Batch Loss = 1.2504\n",
            "Iteration 47, Batch 48/100: Batch Loss = 1.2480\n",
            "Iteration 47, Batch 49/100: Batch Loss = 1.2367\n",
            "Iteration 47, Batch 50/100: Batch Loss = 1.3271\n",
            "Iteration 47, Batch 51/100: Batch Loss = 1.2680\n",
            "Iteration 47, Batch 52/100: Batch Loss = 1.3312\n",
            "Iteration 47, Batch 53/100: Batch Loss = 1.2290\n",
            "Iteration 47, Batch 54/100: Batch Loss = 1.2996\n",
            "Iteration 47, Batch 55/100: Batch Loss = 1.2492\n",
            "Iteration 47, Batch 56/100: Batch Loss = 1.2774\n",
            "Iteration 47, Batch 57/100: Batch Loss = 1.1541\n",
            "Iteration 47, Batch 58/100: Batch Loss = 1.3787\n",
            "Iteration 47, Batch 59/100: Batch Loss = 1.2368\n",
            "Iteration 47, Batch 60/100: Batch Loss = 1.4513\n",
            "Iteration 47, Batch 61/100: Batch Loss = 1.2218\n",
            "Iteration 47, Batch 62/100: Batch Loss = 1.2824\n",
            "Iteration 47, Batch 63/100: Batch Loss = 1.2237\n",
            "Iteration 47, Batch 64/100: Batch Loss = 1.2410\n",
            "Iteration 47, Batch 65/100: Batch Loss = 1.2117\n",
            "Iteration 47, Batch 66/100: Batch Loss = 1.4291\n",
            "Iteration 47, Batch 67/100: Batch Loss = 1.2629\n",
            "Iteration 47, Batch 68/100: Batch Loss = 1.4288\n",
            "Iteration 47, Batch 69/100: Batch Loss = 1.2998\n",
            "Iteration 47, Batch 70/100: Batch Loss = 1.2755\n",
            "Iteration 47, Batch 71/100: Batch Loss = 1.2754\n",
            "Iteration 47, Batch 72/100: Batch Loss = 1.3309\n",
            "Iteration 47, Batch 73/100: Batch Loss = 1.3067\n",
            "Iteration 47, Batch 74/100: Batch Loss = 1.2344\n",
            "Iteration 47, Batch 75/100: Batch Loss = 1.3550\n",
            "Iteration 47, Batch 76/100: Batch Loss = 1.2289\n",
            "Iteration 47, Batch 77/100: Batch Loss = 1.2008\n",
            "Iteration 47, Batch 78/100: Batch Loss = 1.3570\n",
            "Iteration 47, Batch 79/100: Batch Loss = 1.2187\n",
            "Iteration 47, Batch 80/100: Batch Loss = 1.2623\n",
            "Iteration 47, Batch 81/100: Batch Loss = 1.1522\n",
            "Iteration 47, Batch 82/100: Batch Loss = 1.2884\n",
            "Iteration 47, Batch 83/100: Batch Loss = 1.3309\n",
            "Iteration 47, Batch 84/100: Batch Loss = 1.3087\n",
            "Iteration 47, Batch 85/100: Batch Loss = 1.3068\n",
            "Iteration 47, Batch 86/100: Batch Loss = 1.2662\n",
            "Iteration 47, Batch 87/100: Batch Loss = 1.3719\n",
            "Iteration 47, Batch 88/100: Batch Loss = 1.2329\n",
            "Iteration 47, Batch 89/100: Batch Loss = 1.2611\n",
            "Iteration 47, Batch 90/100: Batch Loss = 1.2115\n",
            "Iteration 47, Batch 91/100: Batch Loss = 1.1757\n",
            "Iteration 47, Batch 92/100: Batch Loss = 1.2440\n",
            "Iteration 47, Batch 93/100: Batch Loss = 1.2683\n",
            "Iteration 47, Batch 94/100: Batch Loss = 1.3624\n",
            "Iteration 47, Batch 95/100: Batch Loss = 1.2998\n",
            "Iteration 47, Batch 96/100: Batch Loss = 1.2717\n",
            "Iteration 47, Batch 97/100: Batch Loss = 1.3517\n",
            "Iteration 47, Batch 98/100: Batch Loss = 1.2792\n",
            "Iteration 47, Batch 99/100: Batch Loss = 1.2662\n",
            "Iteration 47, Batch 100/100: Batch Loss = 1.1646\n",
            "Iteration 47: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 48, Batch 1/100: Batch Loss = 1.2533\n",
            "Iteration 48, Batch 2/100: Batch Loss = 1.3292\n",
            "Iteration 48, Batch 3/100: Batch Loss = 1.3217\n",
            "Iteration 48, Batch 4/100: Batch Loss = 1.1572\n",
            "Iteration 48, Batch 5/100: Batch Loss = 1.2977\n",
            "Iteration 48, Batch 6/100: Batch Loss = 1.2994\n",
            "Iteration 48, Batch 7/100: Batch Loss = 1.4053\n",
            "Iteration 48, Batch 8/100: Batch Loss = 1.4198\n",
            "Iteration 48, Batch 9/100: Batch Loss = 1.3496\n",
            "Iteration 48, Batch 10/100: Batch Loss = 1.3216\n",
            "Iteration 48, Batch 11/100: Batch Loss = 1.2055\n",
            "Iteration 48, Batch 12/100: Batch Loss = 1.2442\n",
            "Iteration 48, Batch 13/100: Batch Loss = 1.1832\n",
            "Iteration 48, Batch 14/100: Batch Loss = 1.2992\n",
            "Iteration 48, Batch 15/100: Batch Loss = 1.3217\n",
            "Iteration 48, Batch 16/100: Batch Loss = 1.2483\n",
            "Iteration 48, Batch 17/100: Batch Loss = 1.2849\n",
            "Iteration 48, Batch 18/100: Batch Loss = 1.3585\n",
            "Iteration 48, Batch 19/100: Batch Loss = 1.1777\n",
            "Iteration 48, Batch 20/100: Batch Loss = 1.1922\n",
            "Iteration 48, Batch 21/100: Batch Loss = 1.2552\n",
            "Iteration 48, Batch 22/100: Batch Loss = 1.2588\n",
            "Iteration 48, Batch 23/100: Batch Loss = 1.4122\n",
            "Iteration 48, Batch 24/100: Batch Loss = 1.1953\n",
            "Iteration 48, Batch 25/100: Batch Loss = 1.3388\n",
            "Iteration 48, Batch 26/100: Batch Loss = 1.2872\n",
            "Iteration 48, Batch 27/100: Batch Loss = 1.2613\n",
            "Iteration 48, Batch 28/100: Batch Loss = 1.2125\n",
            "Iteration 48, Batch 29/100: Batch Loss = 1.2866\n",
            "Iteration 48, Batch 30/100: Batch Loss = 1.3033\n",
            "Iteration 48, Batch 31/100: Batch Loss = 1.3161\n",
            "Iteration 48, Batch 32/100: Batch Loss = 1.2274\n",
            "Iteration 48, Batch 33/100: Batch Loss = 1.2330\n",
            "Iteration 48, Batch 34/100: Batch Loss = 1.2921\n",
            "Iteration 48, Batch 35/100: Batch Loss = 1.3442\n",
            "Iteration 48, Batch 36/100: Batch Loss = 1.4515\n",
            "Iteration 48, Batch 37/100: Batch Loss = 1.1997\n",
            "Iteration 48, Batch 38/100: Batch Loss = 1.1387\n",
            "Iteration 48, Batch 39/100: Batch Loss = 1.2905\n",
            "Iteration 48, Batch 40/100: Batch Loss = 1.1830\n",
            "Iteration 48, Batch 41/100: Batch Loss = 1.2419\n",
            "Iteration 48, Batch 42/100: Batch Loss = 1.4020\n",
            "Iteration 48, Batch 43/100: Batch Loss = 1.3310\n",
            "Iteration 48, Batch 44/100: Batch Loss = 1.2441\n",
            "Iteration 48, Batch 45/100: Batch Loss = 1.2865\n",
            "Iteration 48, Batch 46/100: Batch Loss = 1.4272\n",
            "Iteration 48, Batch 47/100: Batch Loss = 1.2571\n",
            "Iteration 48, Batch 48/100: Batch Loss = 1.3329\n",
            "Iteration 48, Batch 49/100: Batch Loss = 1.3290\n",
            "Iteration 48, Batch 50/100: Batch Loss = 1.2291\n",
            "Iteration 48, Batch 51/100: Batch Loss = 1.2830\n",
            "Iteration 48, Batch 52/100: Batch Loss = 1.2056\n",
            "Iteration 48, Batch 53/100: Batch Loss = 1.3852\n",
            "Iteration 48, Batch 54/100: Batch Loss = 1.3789\n",
            "Iteration 48, Batch 55/100: Batch Loss = 1.2664\n",
            "Iteration 48, Batch 56/100: Batch Loss = 1.2480\n",
            "Iteration 48, Batch 57/100: Batch Loss = 1.2700\n",
            "Iteration 48, Batch 58/100: Batch Loss = 1.2387\n",
            "Iteration 48, Batch 59/100: Batch Loss = 1.3089\n",
            "Iteration 48, Batch 60/100: Batch Loss = 1.2514\n",
            "Iteration 48, Batch 61/100: Batch Loss = 1.2329\n",
            "Iteration 48, Batch 62/100: Batch Loss = 1.1589\n",
            "Iteration 48, Batch 63/100: Batch Loss = 1.2236\n",
            "Iteration 48, Batch 64/100: Batch Loss = 1.2553\n",
            "Iteration 48, Batch 65/100: Batch Loss = 1.1508\n",
            "Iteration 48, Batch 66/100: Batch Loss = 1.2716\n",
            "Iteration 48, Batch 67/100: Batch Loss = 1.1738\n",
            "Iteration 48, Batch 68/100: Batch Loss = 1.3155\n",
            "Iteration 48, Batch 69/100: Batch Loss = 1.3405\n",
            "Iteration 48, Batch 70/100: Batch Loss = 1.3126\n",
            "Iteration 48, Batch 71/100: Batch Loss = 1.2290\n",
            "Iteration 48, Batch 72/100: Batch Loss = 1.1919\n",
            "Iteration 48, Batch 73/100: Batch Loss = 1.3351\n",
            "Iteration 48, Batch 74/100: Batch Loss = 1.3924\n",
            "Iteration 48, Batch 75/100: Batch Loss = 1.4091\n",
            "Iteration 48, Batch 76/100: Batch Loss = 1.3349\n",
            "Iteration 48, Batch 77/100: Batch Loss = 1.3440\n",
            "Iteration 48, Batch 78/100: Batch Loss = 1.3642\n",
            "Iteration 48, Batch 79/100: Batch Loss = 1.2662\n",
            "Iteration 48, Batch 80/100: Batch Loss = 1.2367\n",
            "Iteration 48, Batch 81/100: Batch Loss = 1.2515\n",
            "Iteration 48, Batch 82/100: Batch Loss = 1.2683\n",
            "Iteration 48, Batch 83/100: Batch Loss = 1.3773\n",
            "Iteration 48, Batch 84/100: Batch Loss = 1.5101\n",
            "Iteration 48, Batch 85/100: Batch Loss = 1.2350\n",
            "Iteration 48, Batch 86/100: Batch Loss = 1.3087\n",
            "Iteration 48, Batch 87/100: Batch Loss = 1.3072\n",
            "Iteration 48, Batch 88/100: Batch Loss = 1.2701\n",
            "Iteration 48, Batch 89/100: Batch Loss = 1.2830\n",
            "Iteration 48, Batch 90/100: Batch Loss = 1.2700\n",
            "Iteration 48, Batch 91/100: Batch Loss = 1.1872\n",
            "Iteration 48, Batch 92/100: Batch Loss = 1.2312\n",
            "Iteration 48, Batch 93/100: Batch Loss = 1.3512\n",
            "Iteration 48, Batch 94/100: Batch Loss = 1.3013\n",
            "Iteration 48, Batch 95/100: Batch Loss = 1.2476\n",
            "Iteration 48, Batch 96/100: Batch Loss = 1.2233\n",
            "Iteration 48, Batch 97/100: Batch Loss = 1.2041\n",
            "Iteration 48, Batch 98/100: Batch Loss = 1.2587\n",
            "Iteration 48, Batch 99/100: Batch Loss = 1.2001\n",
            "Iteration 48, Batch 100/100: Batch Loss = 1.2274\n",
            "Iteration 48: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Iteration 49, Batch 1/100: Batch Loss = 1.3702\n",
            "Iteration 49, Batch 2/100: Batch Loss = 1.3199\n",
            "Iteration 49, Batch 3/100: Batch Loss = 1.3624\n",
            "Iteration 49, Batch 4/100: Batch Loss = 1.2719\n",
            "Iteration 49, Batch 5/100: Batch Loss = 1.2681\n",
            "Iteration 49, Batch 6/100: Batch Loss = 1.3677\n",
            "Iteration 49, Batch 7/100: Batch Loss = 1.1549\n",
            "Iteration 49, Batch 8/100: Batch Loss = 1.3676\n",
            "Iteration 49, Batch 9/100: Batch Loss = 1.4570\n",
            "Iteration 49, Batch 10/100: Batch Loss = 1.2268\n",
            "Iteration 49, Batch 11/100: Batch Loss = 1.2873\n",
            "Iteration 49, Batch 12/100: Batch Loss = 1.2550\n",
            "Iteration 49, Batch 13/100: Batch Loss = 1.3278\n",
            "Iteration 49, Batch 14/100: Batch Loss = 1.1861\n",
            "Iteration 49, Batch 15/100: Batch Loss = 1.2759\n",
            "Iteration 49, Batch 16/100: Batch Loss = 1.2185\n",
            "Iteration 49, Batch 17/100: Batch Loss = 1.2514\n",
            "Iteration 49, Batch 18/100: Batch Loss = 1.2791\n",
            "Iteration 49, Batch 19/100: Batch Loss = 1.2948\n",
            "Iteration 49, Batch 20/100: Batch Loss = 1.3378\n",
            "Iteration 49, Batch 21/100: Batch Loss = 1.2492\n",
            "Iteration 49, Batch 22/100: Batch Loss = 1.2473\n",
            "Iteration 49, Batch 23/100: Batch Loss = 1.3902\n",
            "Iteration 49, Batch 24/100: Batch Loss = 1.2854\n",
            "Iteration 49, Batch 25/100: Batch Loss = 1.4052\n",
            "Iteration 49, Batch 26/100: Batch Loss = 1.1973\n",
            "Iteration 49, Batch 27/100: Batch Loss = 1.2538\n",
            "Iteration 49, Batch 28/100: Batch Loss = 1.3089\n",
            "Iteration 49, Batch 29/100: Batch Loss = 1.2618\n",
            "Iteration 49, Batch 30/100: Batch Loss = 1.2144\n",
            "Iteration 49, Batch 31/100: Batch Loss = 1.2821\n",
            "Iteration 49, Batch 32/100: Batch Loss = 1.3421\n",
            "Iteration 49, Batch 33/100: Batch Loss = 1.3028\n",
            "Iteration 49, Batch 34/100: Batch Loss = 1.2332\n",
            "Iteration 49, Batch 35/100: Batch Loss = 1.2408\n",
            "Iteration 49, Batch 36/100: Batch Loss = 1.2224\n",
            "Iteration 49, Batch 37/100: Batch Loss = 1.2850\n",
            "Iteration 49, Batch 38/100: Batch Loss = 1.2242\n",
            "Iteration 49, Batch 39/100: Batch Loss = 1.2977\n",
            "Iteration 49, Batch 40/100: Batch Loss = 1.3232\n",
            "Iteration 49, Batch 41/100: Batch Loss = 1.1543\n",
            "Iteration 49, Batch 42/100: Batch Loss = 1.1609\n",
            "Iteration 49, Batch 43/100: Batch Loss = 1.0939\n",
            "Iteration 49, Batch 44/100: Batch Loss = 1.2662\n",
            "Iteration 49, Batch 45/100: Batch Loss = 1.3398\n",
            "Iteration 49, Batch 46/100: Batch Loss = 1.2202\n",
            "Iteration 49, Batch 47/100: Batch Loss = 1.4123\n",
            "Iteration 49, Batch 48/100: Batch Loss = 1.4926\n",
            "Iteration 49, Batch 49/100: Batch Loss = 1.2348\n",
            "Iteration 49, Batch 50/100: Batch Loss = 1.2868\n",
            "Iteration 49, Batch 51/100: Batch Loss = 1.1827\n",
            "Iteration 49, Batch 52/100: Batch Loss = 1.3728\n",
            "Iteration 49, Batch 53/100: Batch Loss = 1.2016\n",
            "Iteration 49, Batch 54/100: Batch Loss = 1.2790\n",
            "Iteration 49, Batch 55/100: Batch Loss = 1.3721\n",
            "Iteration 49, Batch 56/100: Batch Loss = 1.2475\n",
            "Iteration 49, Batch 57/100: Batch Loss = 1.2551\n",
            "Iteration 49, Batch 58/100: Batch Loss = 1.2269\n",
            "Iteration 49, Batch 59/100: Batch Loss = 1.3818\n",
            "Iteration 49, Batch 60/100: Batch Loss = 1.3470\n",
            "Iteration 49, Batch 61/100: Batch Loss = 1.3961\n",
            "Iteration 49, Batch 62/100: Batch Loss = 1.3579\n",
            "Iteration 49, Batch 63/100: Batch Loss = 1.3276\n",
            "Iteration 49, Batch 64/100: Batch Loss = 1.2000\n",
            "Iteration 49, Batch 65/100: Batch Loss = 1.3150\n",
            "Iteration 49, Batch 66/100: Batch Loss = 1.2444\n",
            "Iteration 49, Batch 67/100: Batch Loss = 1.2471\n",
            "Iteration 49, Batch 68/100: Batch Loss = 1.2373\n",
            "Iteration 49, Batch 69/100: Batch Loss = 1.3082\n",
            "Iteration 49, Batch 70/100: Batch Loss = 1.2129\n",
            "Iteration 49, Batch 71/100: Batch Loss = 1.1985\n",
            "Iteration 49, Batch 72/100: Batch Loss = 1.2994\n",
            "Iteration 49, Batch 73/100: Batch Loss = 1.1813\n",
            "Iteration 49, Batch 74/100: Batch Loss = 1.1964\n",
            "Iteration 49, Batch 75/100: Batch Loss = 1.2013\n",
            "Iteration 49, Batch 76/100: Batch Loss = 1.2442\n",
            "Iteration 49, Batch 77/100: Batch Loss = 1.1756\n",
            "Iteration 49, Batch 78/100: Batch Loss = 1.2480\n",
            "Iteration 49, Batch 79/100: Batch Loss = 1.2384\n",
            "Iteration 49, Batch 80/100: Batch Loss = 1.4614\n",
            "Iteration 49, Batch 81/100: Batch Loss = 1.3441\n",
            "Iteration 49, Batch 82/100: Batch Loss = 1.3198\n",
            "Iteration 49, Batch 83/100: Batch Loss = 1.3736\n",
            "Iteration 49, Batch 84/100: Batch Loss = 1.4586\n",
            "Iteration 49, Batch 85/100: Batch Loss = 1.3535\n",
            "Iteration 49, Batch 86/100: Batch Loss = 1.2775\n",
            "Iteration 49, Batch 87/100: Batch Loss = 1.3199\n",
            "Iteration 49, Batch 88/100: Batch Loss = 1.3568\n",
            "Iteration 49, Batch 89/100: Batch Loss = 1.2862\n",
            "Iteration 49, Batch 90/100: Batch Loss = 1.1966\n",
            "Iteration 49, Batch 91/100: Batch Loss = 1.1813\n",
            "Iteration 49, Batch 92/100: Batch Loss = 1.2216\n",
            "Iteration 49, Batch 93/100: Batch Loss = 1.1461\n",
            "Iteration 49, Batch 94/100: Batch Loss = 1.3935\n",
            "Iteration 49, Batch 95/100: Batch Loss = 1.2906\n",
            "Iteration 49, Batch 96/100: Batch Loss = 1.2935\n",
            "Iteration 49, Batch 97/100: Batch Loss = 1.3193\n",
            "Iteration 49, Batch 98/100: Batch Loss = 1.3012\n",
            "Iteration 49, Batch 99/100: Batch Loss = 1.2377\n",
            "Iteration 49, Batch 100/100: Batch Loss = 1.1898\n",
            "Iteration 49: Train Loss = 1.2813, Val Loss = 1.2787\n",
            "Test Accuracy: 39.00%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 39.00%\n",
            "Sample 0:\n",
            "  Features: [ 0.23460178  0.36875884 -0.66104846 -0.64409844 -0.79400121 -0.51824907\n",
            "  0.11646977 -0.72303939 -1.1813074 ]\n",
            "  True Label: 0\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 1:\n",
            "  Features: [-1.26743862  0.46932479 -0.50321423 -0.69150571 -0.13393883 -0.74244565\n",
            " -1.02049155  1.16992062  0.31553988]\n",
            "  True Label: 1\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 2:\n",
            "  Features: [ 1.23596204  1.10414735  0.73922446  1.219372    2.1259358   1.05112697\n",
            "  1.82191174 -1.30762998  0.90126273]\n",
            "  True Label: 3\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 3:\n",
            "  Features: [ 0.23460178  1.17957181 -0.53963751 -0.42164893 -0.38006378  0.42337655\n",
            " -0.1769396  -0.05493586  0.28299972]\n",
            "  True Label: 3\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 4:\n",
            "  Features: [ 0.48494184  1.79553826  0.03099394  0.23840615  0.97362349  0.90166258\n",
            "  0.42821722 -0.91790292  0.51728886]\n",
            "  True Label: 1\n",
            "  Predicted Label: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/updated_pollution_dataset.csv\")\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_test, val = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "train, test = train_test_split(train_test, test_size=0.2, random_state=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train.iloc[:, :-1])\n",
        "test_scaled = scaler.transform(test.iloc[:, :-1])\n",
        "val_scaled = scaler.transform(val.iloc[:, :-1])\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder()\n",
        "train_labels = encoder.fit_transform(train.iloc[:, -1].values.reshape(-1, 1))\n",
        "val_labels = encoder.transform(val.iloc[:, -1].values.reshape(-1, 1))\n",
        "test_labels = encoder.transform(test.iloc[:, -1].values.reshape(-1, 1))\n",
        "\n",
        "# Transpose data for neural network\n",
        "train_X = train_scaled.T\n",
        "train_Y = train_labels.toarray().T\n",
        "val_X = val_scaled.T\n",
        "val_Y = val_labels.toarray().T\n",
        "test_X = test_scaled.T\n",
        "test_Y = test_labels.toarray().T\n",
        "\n",
        "# Print shapes\n",
        "print(\"Train X shape:\", train_X.shape)\n",
        "print(\"Train Y shape:\", train_Y.shape)\n",
        "print(\"Validation X shape:\", val_X.shape)\n",
        "print(\"Validation Y shape:\", val_Y.shape)\n",
        "print(\"Test X shape:\", test_X.shape)\n",
        "print(\"Test Y shape:\", test_Y.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize parameters for multiple hidden layers\n",
        "def initialize_parameters(nx, nh, ny):\n",
        "    tf.random.set_seed(1)\n",
        "    W = []\n",
        "    b = []\n",
        "\n",
        "    for i in range(len(nh)):\n",
        "        if i == 0:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nx), minval=-0.01, maxval=0.01)))\n",
        "        else:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nh[i - 1]), minval=-0.01, maxval=0.01)))\n",
        "        b.append(tf.Variable(tf.zeros(shape=(nh[i], 1))))\n",
        "\n",
        "    W2 = tf.Variable(tf.random.uniform(shape=(ny, nh[-1]), minval=-0.01, maxval=0.01))\n",
        "    b2 = tf.Variable(tf.zeros(shape=(ny, 1)))\n",
        "\n",
        "    parameters = {\"W\": W,\n",
        "                  \"b\": b,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters\n",
        "\n",
        "# Log-softmax for numerical stability\n",
        "def log_softmax(x):\n",
        "    x_max = tf.reduce_max(x, axis=0, keepdims=True)\n",
        "    return x - x_max - tf.math.log(tf.reduce_sum(tf.exp(x - x_max), axis=0, keepdims=True))\n",
        "\n",
        "# Forward pass with multiple hidden layers\n",
        "def forward_pass(parameters, X):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "    Z = []\n",
        "    A = [X]\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        Z.append(tf.matmul(parameters[\"W\"][i], A[-1]) + parameters[\"b\"][i])\n",
        "        A.append(tf.nn.relu(Z[-1]))\n",
        "\n",
        "    Yhat = tf.matmul(parameters[\"W2\"], A[-1]) + parameters[\"b2\"]\n",
        "    return tf.nn.softmax(Yhat, axis=0)\n",
        "\n",
        "\n",
        "# Compute loss using log-softmax\n",
        "def compute_loss(Y, Yhat):\n",
        "    eps = 1e-10\n",
        "    Yhat_clipped = tf.clip_by_value(Yhat, eps, 1.0)#debugged using ai\n",
        "    individual_losses = -tf.reduce_sum(Y * tf.math.log(Yhat_clipped), axis=0)\n",
        "    total_loss = tf.reduce_mean(individual_losses)\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters)\n",
        "    return gradients\n",
        "\n",
        "# Update parameters\n",
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        parameters[\"W\"][i].assign_sub(learning_rate * gradients[\"W\"][i])\n",
        "        parameters[\"b\"][i].assign_sub(learning_rate * gradients[\"b\"][i])\n",
        "\n",
        "    parameters[\"W2\"].assign_sub(learning_rate * gradients[\"W2\"])\n",
        "    parameters[\"b2\"].assign_sub(learning_rate * gradients[\"b2\"])\n",
        "    return parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create and train the model\n",
        "def create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations, learning_rate,batch_size):\n",
        "    # Safety checks\n",
        "    assert train_X.shape[0] == val_X.shape[0], \"train_X and val_X must have the same number of features\"\n",
        "    assert train_X.shape[1] == train_Y.shape[1], \"train_X and train_Y must have the same number of examples\"\n",
        "    assert val_X.shape[1] == val_Y.shape[1], \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "    # Get the number of features and classes\n",
        "    nx = train_X.shape[0]\n",
        "    ny = train_Y.shape[0]  # Number of classes\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(nx, nh, ny)\n",
        "\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iterations):\n",
        "        permuted_indices = np.random.permutation(train_X.shape[1])\n",
        "        train_X_shuffled = train_X[:, permuted_indices]\n",
        "        train_Y_shuffled = train_Y[:, permuted_indices]\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = train_X_shuffled.shape[1] // batch_size\n",
        "\n",
        "        for j in range(num_batches):\n",
        "            start_idx = j * batch_size\n",
        "            end_idx = (j + 1) * batch_size\n",
        "            batch_X = train_X_shuffled[:, start_idx:end_idx]\n",
        "            batch_Y = train_Y_shuffled[:, start_idx:end_idx]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass on batch\n",
        "                batch_Yhat = forward_pass(parameters, batch_X)\n",
        "                batch_loss = compute_loss(batch_Y, batch_Yhat)\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            gradients = backward_pass(parameters, batch_loss, tape)\n",
        "            parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "            print(f\"Iteration {i}, Batch {j + 1}/{num_batches}: Batch Loss = {batch_loss:.4f}\")\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        train_loss = epoch_loss.numpy()\n",
        "\n",
        "\n",
        "        # Forward pass on validation data\n",
        "        val_Yhat = forward_pass(parameters, val_X)\n",
        "        val_loss = compute_loss(val_Y, val_Yhat)\n",
        "\n",
        "        # Print losses\n",
        "        print(f\"Iteration {i}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "        # Append losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    # Return parameters and history\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return parameters, history\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(parameters, X, Y):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predictions = tf.argmax(Yhat, axis=0)\n",
        "    actual = tf.argmax(Y, axis=0)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, actual), tf.float32))\n",
        "    return accuracy.numpy()\n",
        "\n",
        "# Train the model\n",
        "nx = train_X.shape[0]\n",
        "nh =[10, 8,5,4,2]\n",
        "ny = train_Y.shape[0]\n",
        "\n",
        "\n",
        "parameters, history = create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations=50, learning_rate=0.01,batch_size=32)\n",
        "\n",
        "\n",
        "test_accuracy = evaluate_model(parameters, test_X, test_Y)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "def predict(parameters, X):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predicted_labels = tf.argmax(Yhat, axis=0)\n",
        "    return predicted_labels\n",
        "\n",
        "predicted_labels = predict(parameters, test_X)\n",
        "true_labels = tf.argmax(test_Y, axis=0)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true_labels), tf.float32))\n",
        "print(f\"Test Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "num_examples = 5\n",
        "for i in range(num_examples):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"  Features: {val_X[:, i]}\")\n",
        "    print(f\"  True Label: {true_labels[i].numpy()}\")\n",
        "    print(f\"  Predicted Label: {predicted_labels[i].numpy()}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er-m2lLAgE-w"
      },
      "source": [
        "#q2\n",
        "2- Modify update_parameters method to update parameters using Adam optimization ( use the\n",
        "formula in slide 47 of module 2 (3pt) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ixCiw7EYCsjw",
        "outputId": "dd15f153-b753-46f5-9268-b93b26a7ded8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 0, Batch 52/100: Batch Loss = 0.7071\n",
            "Iteration 0, Batch 53/100: Batch Loss = 0.7340\n",
            "Iteration 0, Batch 54/100: Batch Loss = 0.7884\n",
            "Iteration 0, Batch 55/100: Batch Loss = 0.7266\n",
            "Iteration 0, Batch 56/100: Batch Loss = 0.4929\n",
            "Iteration 0, Batch 57/100: Batch Loss = 0.7219\n",
            "Iteration 0, Batch 58/100: Batch Loss = 0.5943\n",
            "Iteration 0, Batch 59/100: Batch Loss = 0.6661\n",
            "Iteration 0, Batch 60/100: Batch Loss = 0.6863\n",
            "Iteration 0, Batch 61/100: Batch Loss = 0.6930\n",
            "Iteration 0, Batch 62/100: Batch Loss = 0.8810\n",
            "Iteration 0, Batch 63/100: Batch Loss = 0.5478\n",
            "Iteration 0, Batch 64/100: Batch Loss = 0.5698\n",
            "Iteration 0, Batch 65/100: Batch Loss = 0.4316\n",
            "Iteration 0, Batch 66/100: Batch Loss = 0.6756\n",
            "Iteration 0, Batch 67/100: Batch Loss = 0.7969\n",
            "Iteration 0, Batch 68/100: Batch Loss = 0.6449\n",
            "Iteration 0, Batch 69/100: Batch Loss = 0.5875\n",
            "Iteration 0, Batch 70/100: Batch Loss = 0.7149\n",
            "Iteration 0, Batch 71/100: Batch Loss = 0.5645\n",
            "Iteration 0, Batch 72/100: Batch Loss = 0.7217\n",
            "Iteration 0, Batch 73/100: Batch Loss = 0.4760\n",
            "Iteration 0, Batch 74/100: Batch Loss = 0.6767\n",
            "Iteration 0, Batch 75/100: Batch Loss = 0.8576\n",
            "Iteration 0, Batch 76/100: Batch Loss = 0.4876\n",
            "Iteration 0, Batch 77/100: Batch Loss = 0.8164\n",
            "Iteration 0, Batch 78/100: Batch Loss = 0.6910\n",
            "Iteration 0, Batch 79/100: Batch Loss = 0.5811\n",
            "Iteration 0, Batch 80/100: Batch Loss = 0.6708\n",
            "Iteration 0, Batch 81/100: Batch Loss = 0.5686\n",
            "Iteration 0, Batch 82/100: Batch Loss = 0.7952\n",
            "Iteration 0, Batch 83/100: Batch Loss = 0.6154\n",
            "Iteration 0, Batch 84/100: Batch Loss = 0.6056\n",
            "Iteration 0, Batch 85/100: Batch Loss = 0.7051\n",
            "Iteration 0, Batch 86/100: Batch Loss = 0.6172\n",
            "Iteration 0, Batch 87/100: Batch Loss = 0.6246\n",
            "Iteration 0, Batch 88/100: Batch Loss = 0.7475\n",
            "Iteration 0, Batch 89/100: Batch Loss = 0.7271\n",
            "Iteration 0, Batch 90/100: Batch Loss = 0.7979\n",
            "Iteration 0, Batch 91/100: Batch Loss = 0.5951\n",
            "Iteration 0, Batch 92/100: Batch Loss = 0.4323\n",
            "Iteration 0, Batch 93/100: Batch Loss = 0.5842\n",
            "Iteration 0, Batch 94/100: Batch Loss = 0.6046\n",
            "Iteration 0, Batch 95/100: Batch Loss = 0.5848\n",
            "Iteration 0, Batch 96/100: Batch Loss = 0.5231\n",
            "Iteration 0, Batch 97/100: Batch Loss = 0.6408\n",
            "Iteration 0, Batch 98/100: Batch Loss = 0.8624\n",
            "Iteration 0, Batch 99/100: Batch Loss = 0.5327\n",
            "Iteration 0, Batch 100/100: Batch Loss = 0.5724\n",
            "Iteration 0: Train Loss = 0.7822, Val Loss = 0.6248\n",
            "Iteration 1, Batch 1/100: Batch Loss = 0.5959\n",
            "Iteration 1, Batch 2/100: Batch Loss = 0.6260\n",
            "Iteration 1, Batch 3/100: Batch Loss = 0.5113\n",
            "Iteration 1, Batch 4/100: Batch Loss = 0.6205\n",
            "Iteration 1, Batch 5/100: Batch Loss = 0.7187\n",
            "Iteration 1, Batch 6/100: Batch Loss = 0.5727\n",
            "Iteration 1, Batch 7/100: Batch Loss = 0.7492\n",
            "Iteration 1, Batch 8/100: Batch Loss = 0.5350\n",
            "Iteration 1, Batch 9/100: Batch Loss = 0.4483\n",
            "Iteration 1, Batch 10/100: Batch Loss = 0.5381\n",
            "Iteration 1, Batch 11/100: Batch Loss = 0.5324\n",
            "Iteration 1, Batch 12/100: Batch Loss = 0.5754\n",
            "Iteration 1, Batch 13/100: Batch Loss = 0.8074\n",
            "Iteration 1, Batch 14/100: Batch Loss = 0.6784\n",
            "Iteration 1, Batch 15/100: Batch Loss = 0.5939\n",
            "Iteration 1, Batch 16/100: Batch Loss = 0.4981\n",
            "Iteration 1, Batch 17/100: Batch Loss = 0.6202\n",
            "Iteration 1, Batch 18/100: Batch Loss = 0.4908\n",
            "Iteration 1, Batch 19/100: Batch Loss = 0.7215\n",
            "Iteration 1, Batch 20/100: Batch Loss = 0.6701\n",
            "Iteration 1, Batch 21/100: Batch Loss = 0.6921\n",
            "Iteration 1, Batch 22/100: Batch Loss = 0.4942\n",
            "Iteration 1, Batch 23/100: Batch Loss = 0.6633\n",
            "Iteration 1, Batch 24/100: Batch Loss = 0.5489\n",
            "Iteration 1, Batch 25/100: Batch Loss = 0.4269\n",
            "Iteration 1, Batch 26/100: Batch Loss = 0.8034\n",
            "Iteration 1, Batch 27/100: Batch Loss = 0.6211\n",
            "Iteration 1, Batch 28/100: Batch Loss = 0.4230\n",
            "Iteration 1, Batch 29/100: Batch Loss = 0.7095\n",
            "Iteration 1, Batch 30/100: Batch Loss = 0.8156\n",
            "Iteration 1, Batch 31/100: Batch Loss = 0.7570\n",
            "Iteration 1, Batch 32/100: Batch Loss = 0.6648\n",
            "Iteration 1, Batch 33/100: Batch Loss = 0.7050\n",
            "Iteration 1, Batch 34/100: Batch Loss = 0.6242\n",
            "Iteration 1, Batch 35/100: Batch Loss = 0.7902\n",
            "Iteration 1, Batch 36/100: Batch Loss = 0.8572\n",
            "Iteration 1, Batch 37/100: Batch Loss = 0.6766\n",
            "Iteration 1, Batch 38/100: Batch Loss = 0.7103\n",
            "Iteration 1, Batch 39/100: Batch Loss = 0.5821\n",
            "Iteration 1, Batch 40/100: Batch Loss = 0.7614\n",
            "Iteration 1, Batch 41/100: Batch Loss = 0.6598\n",
            "Iteration 1, Batch 42/100: Batch Loss = 0.6103\n",
            "Iteration 1, Batch 43/100: Batch Loss = 0.6817\n",
            "Iteration 1, Batch 44/100: Batch Loss = 0.5646\n",
            "Iteration 1, Batch 45/100: Batch Loss = 0.4963\n",
            "Iteration 1, Batch 46/100: Batch Loss = 0.7295\n",
            "Iteration 1, Batch 47/100: Batch Loss = 0.5676\n",
            "Iteration 1, Batch 48/100: Batch Loss = 0.6632\n",
            "Iteration 1, Batch 49/100: Batch Loss = 0.5521\n",
            "Iteration 1, Batch 50/100: Batch Loss = 0.5312\n",
            "Iteration 1, Batch 51/100: Batch Loss = 0.6571\n",
            "Iteration 1, Batch 52/100: Batch Loss = 0.5670\n",
            "Iteration 1, Batch 53/100: Batch Loss = 0.7065\n",
            "Iteration 1, Batch 54/100: Batch Loss = 0.5430\n",
            "Iteration 1, Batch 55/100: Batch Loss = 0.5444\n",
            "Iteration 1, Batch 56/100: Batch Loss = 0.5904\n",
            "Iteration 1, Batch 57/100: Batch Loss = 0.7033\n",
            "Iteration 1, Batch 58/100: Batch Loss = 0.7130\n",
            "Iteration 1, Batch 59/100: Batch Loss = 0.3415\n",
            "Iteration 1, Batch 60/100: Batch Loss = 0.6960\n",
            "Iteration 1, Batch 61/100: Batch Loss = 0.7308\n",
            "Iteration 1, Batch 62/100: Batch Loss = 0.6780\n",
            "Iteration 1, Batch 63/100: Batch Loss = 0.5560\n",
            "Iteration 1, Batch 64/100: Batch Loss = 0.7383\n",
            "Iteration 1, Batch 65/100: Batch Loss = 0.8234\n",
            "Iteration 1, Batch 66/100: Batch Loss = 0.5287\n",
            "Iteration 1, Batch 67/100: Batch Loss = 0.6423\n",
            "Iteration 1, Batch 68/100: Batch Loss = 0.5826\n",
            "Iteration 1, Batch 69/100: Batch Loss = 0.5962\n",
            "Iteration 1, Batch 70/100: Batch Loss = 0.5424\n",
            "Iteration 1, Batch 71/100: Batch Loss = 0.8050\n",
            "Iteration 1, Batch 72/100: Batch Loss = 0.6009\n",
            "Iteration 1, Batch 73/100: Batch Loss = 0.7573\n",
            "Iteration 1, Batch 74/100: Batch Loss = 0.7732\n",
            "Iteration 1, Batch 75/100: Batch Loss = 0.5217\n",
            "Iteration 1, Batch 76/100: Batch Loss = 0.6646\n",
            "Iteration 1, Batch 77/100: Batch Loss = 0.7358\n",
            "Iteration 1, Batch 78/100: Batch Loss = 0.6378\n",
            "Iteration 1, Batch 79/100: Batch Loss = 0.5789\n",
            "Iteration 1, Batch 80/100: Batch Loss = 0.6149\n",
            "Iteration 1, Batch 81/100: Batch Loss = 0.6256\n",
            "Iteration 1, Batch 82/100: Batch Loss = 0.5196\n",
            "Iteration 1, Batch 83/100: Batch Loss = 0.6411\n",
            "Iteration 1, Batch 84/100: Batch Loss = 0.7945\n",
            "Iteration 1, Batch 85/100: Batch Loss = 0.4526\n",
            "Iteration 1, Batch 86/100: Batch Loss = 0.6117\n",
            "Iteration 1, Batch 87/100: Batch Loss = 0.3452\n",
            "Iteration 1, Batch 88/100: Batch Loss = 0.5319\n",
            "Iteration 1, Batch 89/100: Batch Loss = 0.8696\n",
            "Iteration 1, Batch 90/100: Batch Loss = 0.4911\n",
            "Iteration 1, Batch 91/100: Batch Loss = 0.5582\n",
            "Iteration 1, Batch 92/100: Batch Loss = 0.6116\n",
            "Iteration 1, Batch 93/100: Batch Loss = 0.6850\n",
            "Iteration 1, Batch 94/100: Batch Loss = 0.6366\n",
            "Iteration 1, Batch 95/100: Batch Loss = 0.6021\n",
            "Iteration 1, Batch 96/100: Batch Loss = 0.5516\n",
            "Iteration 1, Batch 97/100: Batch Loss = 0.4121\n",
            "Iteration 1, Batch 98/100: Batch Loss = 0.5942\n",
            "Iteration 1, Batch 99/100: Batch Loss = 0.6560\n",
            "Iteration 1, Batch 100/100: Batch Loss = 0.6449\n",
            "Iteration 1: Train Loss = 0.6249, Val Loss = 0.6170\n",
            "Iteration 2, Batch 1/100: Batch Loss = 0.5796\n",
            "Iteration 2, Batch 2/100: Batch Loss = 0.6780\n",
            "Iteration 2, Batch 3/100: Batch Loss = 0.5475\n",
            "Iteration 2, Batch 4/100: Batch Loss = 0.5031\n",
            "Iteration 2, Batch 5/100: Batch Loss = 0.4466\n",
            "Iteration 2, Batch 6/100: Batch Loss = 0.6468\n",
            "Iteration 2, Batch 7/100: Batch Loss = 0.6233\n",
            "Iteration 2, Batch 8/100: Batch Loss = 0.7919\n",
            "Iteration 2, Batch 9/100: Batch Loss = 0.3434\n",
            "Iteration 2, Batch 10/100: Batch Loss = 0.4430\n",
            "Iteration 2, Batch 11/100: Batch Loss = 0.7597\n",
            "Iteration 2, Batch 12/100: Batch Loss = 0.6556\n",
            "Iteration 2, Batch 13/100: Batch Loss = 0.7580\n",
            "Iteration 2, Batch 14/100: Batch Loss = 0.5549\n",
            "Iteration 2, Batch 15/100: Batch Loss = 0.7027\n",
            "Iteration 2, Batch 16/100: Batch Loss = 0.6248\n",
            "Iteration 2, Batch 17/100: Batch Loss = 0.7109\n",
            "Iteration 2, Batch 18/100: Batch Loss = 0.8431\n",
            "Iteration 2, Batch 19/100: Batch Loss = 0.4265\n",
            "Iteration 2, Batch 20/100: Batch Loss = 0.5456\n",
            "Iteration 2, Batch 21/100: Batch Loss = 0.6559\n",
            "Iteration 2, Batch 22/100: Batch Loss = 0.5948\n",
            "Iteration 2, Batch 23/100: Batch Loss = 0.6994\n",
            "Iteration 2, Batch 24/100: Batch Loss = 0.6330\n",
            "Iteration 2, Batch 25/100: Batch Loss = 0.5369\n",
            "Iteration 2, Batch 26/100: Batch Loss = 0.6027\n",
            "Iteration 2, Batch 27/100: Batch Loss = 0.5934\n",
            "Iteration 2, Batch 28/100: Batch Loss = 0.6235\n",
            "Iteration 2, Batch 29/100: Batch Loss = 0.5860\n",
            "Iteration 2, Batch 30/100: Batch Loss = 0.8004\n",
            "Iteration 2, Batch 31/100: Batch Loss = 0.6791\n",
            "Iteration 2, Batch 32/100: Batch Loss = 0.5726\n",
            "Iteration 2, Batch 33/100: Batch Loss = 0.5544\n",
            "Iteration 2, Batch 34/100: Batch Loss = 0.6430\n",
            "Iteration 2, Batch 35/100: Batch Loss = 0.5809\n",
            "Iteration 2, Batch 36/100: Batch Loss = 0.6244\n",
            "Iteration 2, Batch 37/100: Batch Loss = 0.4406\n",
            "Iteration 2, Batch 38/100: Batch Loss = 0.4718\n",
            "Iteration 2, Batch 39/100: Batch Loss = 0.7144\n",
            "Iteration 2, Batch 40/100: Batch Loss = 0.5070\n",
            "Iteration 2, Batch 41/100: Batch Loss = 0.4551\n",
            "Iteration 2, Batch 42/100: Batch Loss = 0.6089\n",
            "Iteration 2, Batch 43/100: Batch Loss = 0.6551\n",
            "Iteration 2, Batch 44/100: Batch Loss = 0.7290\n",
            "Iteration 2, Batch 45/100: Batch Loss = 0.6115\n",
            "Iteration 2, Batch 46/100: Batch Loss = 0.6612\n",
            "Iteration 2, Batch 47/100: Batch Loss = 0.6077\n",
            "Iteration 2, Batch 48/100: Batch Loss = 0.4987\n",
            "Iteration 2, Batch 49/100: Batch Loss = 0.5735\n",
            "Iteration 2, Batch 50/100: Batch Loss = 0.7406\n",
            "Iteration 2, Batch 51/100: Batch Loss = 0.6071\n",
            "Iteration 2, Batch 52/100: Batch Loss = 0.7208\n",
            "Iteration 2, Batch 53/100: Batch Loss = 0.4560\n",
            "Iteration 2, Batch 54/100: Batch Loss = 0.5304\n",
            "Iteration 2, Batch 55/100: Batch Loss = 0.6815\n",
            "Iteration 2, Batch 56/100: Batch Loss = 0.5922\n",
            "Iteration 2, Batch 57/100: Batch Loss = 0.5331\n",
            "Iteration 2, Batch 58/100: Batch Loss = 0.6651\n",
            "Iteration 2, Batch 59/100: Batch Loss = 0.6456\n",
            "Iteration 2, Batch 60/100: Batch Loss = 0.4051\n",
            "Iteration 2, Batch 61/100: Batch Loss = 0.8121\n",
            "Iteration 2, Batch 62/100: Batch Loss = 0.4565\n",
            "Iteration 2, Batch 63/100: Batch Loss = 0.6431\n",
            "Iteration 2, Batch 64/100: Batch Loss = 0.6411\n",
            "Iteration 2, Batch 65/100: Batch Loss = 0.5437\n",
            "Iteration 2, Batch 66/100: Batch Loss = 0.5444\n",
            "Iteration 2, Batch 67/100: Batch Loss = 0.7151\n",
            "Iteration 2, Batch 68/100: Batch Loss = 0.6374\n",
            "Iteration 2, Batch 69/100: Batch Loss = 0.7604\n",
            "Iteration 2, Batch 70/100: Batch Loss = 0.4635\n",
            "Iteration 2, Batch 71/100: Batch Loss = 0.6748\n",
            "Iteration 2, Batch 72/100: Batch Loss = 0.6365\n",
            "Iteration 2, Batch 73/100: Batch Loss = 0.5610\n",
            "Iteration 2, Batch 74/100: Batch Loss = 0.7005\n",
            "Iteration 2, Batch 75/100: Batch Loss = 0.5023\n",
            "Iteration 2, Batch 76/100: Batch Loss = 0.5999\n",
            "Iteration 2, Batch 77/100: Batch Loss = 0.4607\n",
            "Iteration 2, Batch 78/100: Batch Loss = 0.6003\n",
            "Iteration 2, Batch 79/100: Batch Loss = 0.7734\n",
            "Iteration 2, Batch 80/100: Batch Loss = 0.5890\n",
            "Iteration 2, Batch 81/100: Batch Loss = 0.5154\n",
            "Iteration 2, Batch 82/100: Batch Loss = 0.6165\n",
            "Iteration 2, Batch 83/100: Batch Loss = 0.6710\n",
            "Iteration 2, Batch 84/100: Batch Loss = 0.5797\n",
            "Iteration 2, Batch 85/100: Batch Loss = 0.7012\n",
            "Iteration 2, Batch 86/100: Batch Loss = 0.6545\n",
            "Iteration 2, Batch 87/100: Batch Loss = 0.6893\n",
            "Iteration 2, Batch 88/100: Batch Loss = 0.7195\n",
            "Iteration 2, Batch 89/100: Batch Loss = 0.6043\n",
            "Iteration 2, Batch 90/100: Batch Loss = 0.5570\n",
            "Iteration 2, Batch 91/100: Batch Loss = 0.4193\n",
            "Iteration 2, Batch 92/100: Batch Loss = 0.4874\n",
            "Iteration 2, Batch 93/100: Batch Loss = 0.7222\n",
            "Iteration 2, Batch 94/100: Batch Loss = 0.7317\n",
            "Iteration 2, Batch 95/100: Batch Loss = 0.6453\n",
            "Iteration 2, Batch 96/100: Batch Loss = 0.4267\n",
            "Iteration 2, Batch 97/100: Batch Loss = 0.3965\n",
            "Iteration 2, Batch 98/100: Batch Loss = 0.6486\n",
            "Iteration 2, Batch 99/100: Batch Loss = 0.5736\n",
            "Iteration 2, Batch 100/100: Batch Loss = 0.5951\n",
            "Iteration 2: Train Loss = 0.6055, Val Loss = 0.5955\n",
            "Iteration 3, Batch 1/100: Batch Loss = 0.6782\n",
            "Iteration 3, Batch 2/100: Batch Loss = 0.5267\n",
            "Iteration 3, Batch 3/100: Batch Loss = 0.5904\n",
            "Iteration 3, Batch 4/100: Batch Loss = 0.4942\n",
            "Iteration 3, Batch 5/100: Batch Loss = 0.6431\n",
            "Iteration 3, Batch 6/100: Batch Loss = 0.4888\n",
            "Iteration 3, Batch 7/100: Batch Loss = 0.8421\n",
            "Iteration 3, Batch 8/100: Batch Loss = 0.6553\n",
            "Iteration 3, Batch 9/100: Batch Loss = 0.4805\n",
            "Iteration 3, Batch 10/100: Batch Loss = 0.5788\n",
            "Iteration 3, Batch 11/100: Batch Loss = 0.5325\n",
            "Iteration 3, Batch 12/100: Batch Loss = 0.7408\n",
            "Iteration 3, Batch 13/100: Batch Loss = 0.4803\n",
            "Iteration 3, Batch 14/100: Batch Loss = 0.5208\n",
            "Iteration 3, Batch 15/100: Batch Loss = 0.6146\n",
            "Iteration 3, Batch 16/100: Batch Loss = 0.3867\n",
            "Iteration 3, Batch 17/100: Batch Loss = 0.7293\n",
            "Iteration 3, Batch 18/100: Batch Loss = 0.5952\n",
            "Iteration 3, Batch 19/100: Batch Loss = 0.5255\n",
            "Iteration 3, Batch 20/100: Batch Loss = 0.6402\n",
            "Iteration 3, Batch 21/100: Batch Loss = 0.7601\n",
            "Iteration 3, Batch 22/100: Batch Loss = 0.5209\n",
            "Iteration 3, Batch 23/100: Batch Loss = 0.5607\n",
            "Iteration 3, Batch 24/100: Batch Loss = 0.3667\n",
            "Iteration 3, Batch 25/100: Batch Loss = 0.6770\n",
            "Iteration 3, Batch 26/100: Batch Loss = 0.6789\n",
            "Iteration 3, Batch 27/100: Batch Loss = 0.4492\n",
            "Iteration 3, Batch 28/100: Batch Loss = 0.6669\n",
            "Iteration 3, Batch 29/100: Batch Loss = 0.4132\n",
            "Iteration 3, Batch 30/100: Batch Loss = 0.3148\n",
            "Iteration 3, Batch 31/100: Batch Loss = 0.6244\n",
            "Iteration 3, Batch 32/100: Batch Loss = 0.5798\n",
            "Iteration 3, Batch 33/100: Batch Loss = 0.7061\n",
            "Iteration 3, Batch 34/100: Batch Loss = 0.5015\n",
            "Iteration 3, Batch 35/100: Batch Loss = 0.4991\n",
            "Iteration 3, Batch 36/100: Batch Loss = 0.5445\n",
            "Iteration 3, Batch 37/100: Batch Loss = 0.5068\n",
            "Iteration 3, Batch 38/100: Batch Loss = 0.3754\n",
            "Iteration 3, Batch 39/100: Batch Loss = 0.5151\n",
            "Iteration 3, Batch 40/100: Batch Loss = 0.6344\n",
            "Iteration 3, Batch 41/100: Batch Loss = 0.5862\n",
            "Iteration 3, Batch 42/100: Batch Loss = 0.6467\n",
            "Iteration 3, Batch 43/100: Batch Loss = 0.5650\n",
            "Iteration 3, Batch 44/100: Batch Loss = 0.5957\n",
            "Iteration 3, Batch 45/100: Batch Loss = 0.5368\n",
            "Iteration 3, Batch 46/100: Batch Loss = 0.6391\n",
            "Iteration 3, Batch 47/100: Batch Loss = 0.2405\n",
            "Iteration 3, Batch 48/100: Batch Loss = 0.4936\n",
            "Iteration 3, Batch 49/100: Batch Loss = 0.7876\n",
            "Iteration 3, Batch 50/100: Batch Loss = 0.5013\n",
            "Iteration 3, Batch 51/100: Batch Loss = 0.5944\n",
            "Iteration 3, Batch 52/100: Batch Loss = 0.5855\n",
            "Iteration 3, Batch 53/100: Batch Loss = 0.4558\n",
            "Iteration 3, Batch 54/100: Batch Loss = 0.4568\n",
            "Iteration 3, Batch 55/100: Batch Loss = 0.6653\n",
            "Iteration 3, Batch 56/100: Batch Loss = 0.5387\n",
            "Iteration 3, Batch 57/100: Batch Loss = 0.5417\n",
            "Iteration 3, Batch 58/100: Batch Loss = 0.6231\n",
            "Iteration 3, Batch 59/100: Batch Loss = 0.4690\n",
            "Iteration 3, Batch 60/100: Batch Loss = 0.5220\n",
            "Iteration 3, Batch 61/100: Batch Loss = 0.5704\n",
            "Iteration 3, Batch 62/100: Batch Loss = 0.3031\n",
            "Iteration 3, Batch 63/100: Batch Loss = 0.4253\n",
            "Iteration 3, Batch 64/100: Batch Loss = 0.5320\n",
            "Iteration 3, Batch 65/100: Batch Loss = 0.6368\n",
            "Iteration 3, Batch 66/100: Batch Loss = 0.4063\n",
            "Iteration 3, Batch 67/100: Batch Loss = 0.6054\n",
            "Iteration 3, Batch 68/100: Batch Loss = 0.7205\n",
            "Iteration 3, Batch 69/100: Batch Loss = 0.5607\n",
            "Iteration 3, Batch 70/100: Batch Loss = 0.4123\n",
            "Iteration 3, Batch 71/100: Batch Loss = 0.5823\n",
            "Iteration 3, Batch 72/100: Batch Loss = 0.6648\n",
            "Iteration 3, Batch 73/100: Batch Loss = 0.4014\n",
            "Iteration 3, Batch 74/100: Batch Loss = 0.5477\n",
            "Iteration 3, Batch 75/100: Batch Loss = 0.7229\n",
            "Iteration 3, Batch 76/100: Batch Loss = 0.4195\n",
            "Iteration 3, Batch 77/100: Batch Loss = 0.5270\n",
            "Iteration 3, Batch 78/100: Batch Loss = 0.4080\n",
            "Iteration 3, Batch 79/100: Batch Loss = 0.5053\n",
            "Iteration 3, Batch 80/100: Batch Loss = 0.5406\n",
            "Iteration 3, Batch 81/100: Batch Loss = 0.4430\n",
            "Iteration 3, Batch 82/100: Batch Loss = 0.4269\n",
            "Iteration 3, Batch 83/100: Batch Loss = 0.5220\n",
            "Iteration 3, Batch 84/100: Batch Loss = 0.4650\n",
            "Iteration 3, Batch 85/100: Batch Loss = 0.4357\n",
            "Iteration 3, Batch 86/100: Batch Loss = 0.4725\n",
            "Iteration 3, Batch 87/100: Batch Loss = 0.4631\n",
            "Iteration 3, Batch 88/100: Batch Loss = 0.4152\n",
            "Iteration 3, Batch 89/100: Batch Loss = 0.4008\n",
            "Iteration 3, Batch 90/100: Batch Loss = 0.3522\n",
            "Iteration 3, Batch 91/100: Batch Loss = 0.4411\n",
            "Iteration 3, Batch 92/100: Batch Loss = 0.3883\n",
            "Iteration 3, Batch 93/100: Batch Loss = 0.5225\n",
            "Iteration 3, Batch 94/100: Batch Loss = 0.5145\n",
            "Iteration 3, Batch 95/100: Batch Loss = 0.3500\n",
            "Iteration 3, Batch 96/100: Batch Loss = 0.4379\n",
            "Iteration 3, Batch 97/100: Batch Loss = 0.3243\n",
            "Iteration 3, Batch 98/100: Batch Loss = 0.3748\n",
            "Iteration 3, Batch 99/100: Batch Loss = 0.4903\n",
            "Iteration 3, Batch 100/100: Batch Loss = 0.2762\n",
            "Iteration 3: Train Loss = 0.5269, Val Loss = 0.3971\n",
            "Iteration 4, Batch 1/100: Batch Loss = 0.6045\n",
            "Iteration 4, Batch 2/100: Batch Loss = 0.4579\n",
            "Iteration 4, Batch 3/100: Batch Loss = 0.2684\n",
            "Iteration 4, Batch 4/100: Batch Loss = 0.4354\n",
            "Iteration 4, Batch 5/100: Batch Loss = 0.4801\n",
            "Iteration 4, Batch 6/100: Batch Loss = 0.2920\n",
            "Iteration 4, Batch 7/100: Batch Loss = 0.4326\n",
            "Iteration 4, Batch 8/100: Batch Loss = 0.4583\n",
            "Iteration 4, Batch 9/100: Batch Loss = 0.4156\n",
            "Iteration 4, Batch 10/100: Batch Loss = 0.4223\n",
            "Iteration 4, Batch 11/100: Batch Loss = 0.3116\n",
            "Iteration 4, Batch 12/100: Batch Loss = 0.4477\n",
            "Iteration 4, Batch 13/100: Batch Loss = 0.3666\n",
            "Iteration 4, Batch 14/100: Batch Loss = 0.3124\n",
            "Iteration 4, Batch 15/100: Batch Loss = 0.3275\n",
            "Iteration 4, Batch 16/100: Batch Loss = 0.2407\n",
            "Iteration 4, Batch 17/100: Batch Loss = 0.4499\n",
            "Iteration 4, Batch 18/100: Batch Loss = 0.3380\n",
            "Iteration 4, Batch 19/100: Batch Loss = 0.2582\n",
            "Iteration 4, Batch 20/100: Batch Loss = 0.3691\n",
            "Iteration 4, Batch 21/100: Batch Loss = 0.2856\n",
            "Iteration 4, Batch 22/100: Batch Loss = 0.4130\n",
            "Iteration 4, Batch 23/100: Batch Loss = 0.4594\n",
            "Iteration 4, Batch 24/100: Batch Loss = 0.4722\n",
            "Iteration 4, Batch 25/100: Batch Loss = 0.2884\n",
            "Iteration 4, Batch 26/100: Batch Loss = 0.4627\n",
            "Iteration 4, Batch 27/100: Batch Loss = 0.3895\n",
            "Iteration 4, Batch 28/100: Batch Loss = 0.2830\n",
            "Iteration 4, Batch 29/100: Batch Loss = 0.3731\n",
            "Iteration 4, Batch 30/100: Batch Loss = 0.4376\n",
            "Iteration 4, Batch 31/100: Batch Loss = 0.5184\n",
            "Iteration 4, Batch 32/100: Batch Loss = 0.4307\n",
            "Iteration 4, Batch 33/100: Batch Loss = 0.5237\n",
            "Iteration 4, Batch 34/100: Batch Loss = 0.3182\n",
            "Iteration 4, Batch 35/100: Batch Loss = 0.3861\n",
            "Iteration 4, Batch 36/100: Batch Loss = 0.4632\n",
            "Iteration 4, Batch 37/100: Batch Loss = 0.3565\n",
            "Iteration 4, Batch 38/100: Batch Loss = 0.2593\n",
            "Iteration 4, Batch 39/100: Batch Loss = 0.2816\n",
            "Iteration 4, Batch 40/100: Batch Loss = 0.3224\n",
            "Iteration 4, Batch 41/100: Batch Loss = 0.2731\n",
            "Iteration 4, Batch 42/100: Batch Loss = 0.2730\n",
            "Iteration 4, Batch 43/100: Batch Loss = 0.4015\n",
            "Iteration 4, Batch 44/100: Batch Loss = 0.4910\n",
            "Iteration 4, Batch 45/100: Batch Loss = 0.4292\n",
            "Iteration 4, Batch 46/100: Batch Loss = 0.2800\n",
            "Iteration 4, Batch 47/100: Batch Loss = 0.3642\n",
            "Iteration 4, Batch 48/100: Batch Loss = 0.3727\n",
            "Iteration 4, Batch 49/100: Batch Loss = 0.4437\n",
            "Iteration 4, Batch 50/100: Batch Loss = 0.3596\n",
            "Iteration 4, Batch 51/100: Batch Loss = 0.2669\n",
            "Iteration 4, Batch 52/100: Batch Loss = 0.3316\n",
            "Iteration 4, Batch 53/100: Batch Loss = 0.3468\n",
            "Iteration 4, Batch 54/100: Batch Loss = 0.2855\n",
            "Iteration 4, Batch 55/100: Batch Loss = 0.4009\n",
            "Iteration 4, Batch 56/100: Batch Loss = 0.4365\n",
            "Iteration 4, Batch 57/100: Batch Loss = 0.5923\n",
            "Iteration 4, Batch 58/100: Batch Loss = 0.1859\n",
            "Iteration 4, Batch 59/100: Batch Loss = 0.3407\n",
            "Iteration 4, Batch 60/100: Batch Loss = 0.2321\n",
            "Iteration 4, Batch 61/100: Batch Loss = 0.3039\n",
            "Iteration 4, Batch 62/100: Batch Loss = 0.2449\n",
            "Iteration 4, Batch 63/100: Batch Loss = 0.3051\n",
            "Iteration 4, Batch 64/100: Batch Loss = 0.3525\n",
            "Iteration 4, Batch 65/100: Batch Loss = 0.2382\n",
            "Iteration 4, Batch 66/100: Batch Loss = 0.2938\n",
            "Iteration 4, Batch 67/100: Batch Loss = 0.2968\n",
            "Iteration 4, Batch 68/100: Batch Loss = 0.2587\n",
            "Iteration 4, Batch 69/100: Batch Loss = 0.3777\n",
            "Iteration 4, Batch 70/100: Batch Loss = 0.2500\n",
            "Iteration 4, Batch 71/100: Batch Loss = 0.2310\n",
            "Iteration 4, Batch 72/100: Batch Loss = 0.3527\n",
            "Iteration 4, Batch 73/100: Batch Loss = 0.3378\n",
            "Iteration 4, Batch 74/100: Batch Loss = 0.3304\n",
            "Iteration 4, Batch 75/100: Batch Loss = 0.2707\n",
            "Iteration 4, Batch 76/100: Batch Loss = 0.2283\n",
            "Iteration 4, Batch 77/100: Batch Loss = 0.3945\n",
            "Iteration 4, Batch 78/100: Batch Loss = 0.4828\n",
            "Iteration 4, Batch 79/100: Batch Loss = 0.5364\n",
            "Iteration 4, Batch 80/100: Batch Loss = 0.2307\n",
            "Iteration 4, Batch 81/100: Batch Loss = 0.2081\n",
            "Iteration 4, Batch 82/100: Batch Loss = 0.3422\n",
            "Iteration 4, Batch 83/100: Batch Loss = 0.3816\n",
            "Iteration 4, Batch 84/100: Batch Loss = 0.4779\n",
            "Iteration 4, Batch 85/100: Batch Loss = 0.3252\n",
            "Iteration 4, Batch 86/100: Batch Loss = 0.2902\n",
            "Iteration 4, Batch 87/100: Batch Loss = 0.3514\n",
            "Iteration 4, Batch 88/100: Batch Loss = 0.3207\n",
            "Iteration 4, Batch 89/100: Batch Loss = 0.1629\n",
            "Iteration 4, Batch 90/100: Batch Loss = 0.4029\n",
            "Iteration 4, Batch 91/100: Batch Loss = 0.3661\n",
            "Iteration 4, Batch 92/100: Batch Loss = 0.3535\n",
            "Iteration 4, Batch 93/100: Batch Loss = 0.4205\n",
            "Iteration 4, Batch 94/100: Batch Loss = 0.2266\n",
            "Iteration 4, Batch 95/100: Batch Loss = 0.3335\n",
            "Iteration 4, Batch 96/100: Batch Loss = 0.2278\n",
            "Iteration 4, Batch 97/100: Batch Loss = 0.3339\n",
            "Iteration 4, Batch 98/100: Batch Loss = 0.2008\n",
            "Iteration 4, Batch 99/100: Batch Loss = 0.3693\n",
            "Iteration 4, Batch 100/100: Batch Loss = 0.1869\n",
            "Iteration 4: Train Loss = 0.3512, Val Loss = 0.3313\n",
            "Iteration 5, Batch 1/100: Batch Loss = 0.3167\n",
            "Iteration 5, Batch 2/100: Batch Loss = 0.3560\n",
            "Iteration 5, Batch 3/100: Batch Loss = 0.2566\n",
            "Iteration 5, Batch 4/100: Batch Loss = 0.5054\n",
            "Iteration 5, Batch 5/100: Batch Loss = 0.2096\n",
            "Iteration 5, Batch 6/100: Batch Loss = 0.2883\n",
            "Iteration 5, Batch 7/100: Batch Loss = 0.4762\n",
            "Iteration 5, Batch 8/100: Batch Loss = 0.4132\n",
            "Iteration 5, Batch 9/100: Batch Loss = 0.3614\n",
            "Iteration 5, Batch 10/100: Batch Loss = 0.2323\n",
            "Iteration 5, Batch 11/100: Batch Loss = 0.3443\n",
            "Iteration 5, Batch 12/100: Batch Loss = 0.4094\n",
            "Iteration 5, Batch 13/100: Batch Loss = 0.4151\n",
            "Iteration 5, Batch 14/100: Batch Loss = 0.3257\n",
            "Iteration 5, Batch 15/100: Batch Loss = 0.3431\n",
            "Iteration 5, Batch 16/100: Batch Loss = 0.2901\n",
            "Iteration 5, Batch 17/100: Batch Loss = 0.2445\n",
            "Iteration 5, Batch 18/100: Batch Loss = 0.3109\n",
            "Iteration 5, Batch 19/100: Batch Loss = 0.3383\n",
            "Iteration 5, Batch 20/100: Batch Loss = 0.4491\n",
            "Iteration 5, Batch 21/100: Batch Loss = 0.2611\n",
            "Iteration 5, Batch 22/100: Batch Loss = 0.2660\n",
            "Iteration 5, Batch 23/100: Batch Loss = 0.3306\n",
            "Iteration 5, Batch 24/100: Batch Loss = 0.4521\n",
            "Iteration 5, Batch 25/100: Batch Loss = 0.2701\n",
            "Iteration 5, Batch 26/100: Batch Loss = 0.3010\n",
            "Iteration 5, Batch 27/100: Batch Loss = 0.4351\n",
            "Iteration 5, Batch 28/100: Batch Loss = 0.3871\n",
            "Iteration 5, Batch 29/100: Batch Loss = 0.3191\n",
            "Iteration 5, Batch 30/100: Batch Loss = 0.3026\n",
            "Iteration 5, Batch 31/100: Batch Loss = 0.2774\n",
            "Iteration 5, Batch 32/100: Batch Loss = 0.2944\n",
            "Iteration 5, Batch 33/100: Batch Loss = 0.3299\n",
            "Iteration 5, Batch 34/100: Batch Loss = 0.3170\n",
            "Iteration 5, Batch 35/100: Batch Loss = 0.2132\n",
            "Iteration 5, Batch 36/100: Batch Loss = 0.2876\n",
            "Iteration 5, Batch 37/100: Batch Loss = 0.2785\n",
            "Iteration 5, Batch 38/100: Batch Loss = 0.2058\n",
            "Iteration 5, Batch 39/100: Batch Loss = 0.2275\n",
            "Iteration 5, Batch 40/100: Batch Loss = 0.1963\n",
            "Iteration 5, Batch 41/100: Batch Loss = 0.2489\n",
            "Iteration 5, Batch 42/100: Batch Loss = 0.2922\n",
            "Iteration 5, Batch 43/100: Batch Loss = 0.2884\n",
            "Iteration 5, Batch 44/100: Batch Loss = 0.3951\n",
            "Iteration 5, Batch 45/100: Batch Loss = 0.3222\n",
            "Iteration 5, Batch 46/100: Batch Loss = 0.4569\n",
            "Iteration 5, Batch 47/100: Batch Loss = 0.1859\n",
            "Iteration 5, Batch 48/100: Batch Loss = 0.2612\n",
            "Iteration 5, Batch 49/100: Batch Loss = 0.2937\n",
            "Iteration 5, Batch 50/100: Batch Loss = 0.1821\n",
            "Iteration 5, Batch 51/100: Batch Loss = 0.1566\n",
            "Iteration 5, Batch 52/100: Batch Loss = 0.3300\n",
            "Iteration 5, Batch 53/100: Batch Loss = 0.2188\n",
            "Iteration 5, Batch 54/100: Batch Loss = 0.2628\n",
            "Iteration 5, Batch 55/100: Batch Loss = 0.2456\n",
            "Iteration 5, Batch 56/100: Batch Loss = 0.2939\n",
            "Iteration 5, Batch 57/100: Batch Loss = 0.5533\n",
            "Iteration 5, Batch 58/100: Batch Loss = 0.2125\n",
            "Iteration 5, Batch 59/100: Batch Loss = 0.2171\n",
            "Iteration 5, Batch 60/100: Batch Loss = 0.2244\n",
            "Iteration 5, Batch 61/100: Batch Loss = 0.2963\n",
            "Iteration 5, Batch 62/100: Batch Loss = 0.2440\n",
            "Iteration 5, Batch 63/100: Batch Loss = 0.2297\n",
            "Iteration 5, Batch 64/100: Batch Loss = 0.2655\n",
            "Iteration 5, Batch 65/100: Batch Loss = 0.2960\n",
            "Iteration 5, Batch 66/100: Batch Loss = 0.3178\n",
            "Iteration 5, Batch 67/100: Batch Loss = 0.1904\n",
            "Iteration 5, Batch 68/100: Batch Loss = 0.4148\n",
            "Iteration 5, Batch 69/100: Batch Loss = 0.3002\n",
            "Iteration 5, Batch 70/100: Batch Loss = 0.1894\n",
            "Iteration 5, Batch 71/100: Batch Loss = 0.1588\n",
            "Iteration 5, Batch 72/100: Batch Loss = 0.2996\n",
            "Iteration 5, Batch 73/100: Batch Loss = 0.1342\n",
            "Iteration 5, Batch 74/100: Batch Loss = 0.4208\n",
            "Iteration 5, Batch 75/100: Batch Loss = 0.2791\n",
            "Iteration 5, Batch 76/100: Batch Loss = 0.3089\n",
            "Iteration 5, Batch 77/100: Batch Loss = 0.2912\n",
            "Iteration 5, Batch 78/100: Batch Loss = 0.3173\n",
            "Iteration 5, Batch 79/100: Batch Loss = 0.2948\n",
            "Iteration 5, Batch 80/100: Batch Loss = 0.3001\n",
            "Iteration 5, Batch 81/100: Batch Loss = 0.2130\n",
            "Iteration 5, Batch 82/100: Batch Loss = 0.1619\n",
            "Iteration 5, Batch 83/100: Batch Loss = 0.3316\n",
            "Iteration 5, Batch 84/100: Batch Loss = 0.3568\n",
            "Iteration 5, Batch 85/100: Batch Loss = 0.2702\n",
            "Iteration 5, Batch 86/100: Batch Loss = 0.2692\n",
            "Iteration 5, Batch 87/100: Batch Loss = 0.2368\n",
            "Iteration 5, Batch 88/100: Batch Loss = 0.2587\n",
            "Iteration 5, Batch 89/100: Batch Loss = 0.1521\n",
            "Iteration 5, Batch 90/100: Batch Loss = 0.3316\n",
            "Iteration 5, Batch 91/100: Batch Loss = 0.2232\n",
            "Iteration 5, Batch 92/100: Batch Loss = 0.1851\n",
            "Iteration 5, Batch 93/100: Batch Loss = 0.1627\n",
            "Iteration 5, Batch 94/100: Batch Loss = 0.3889\n",
            "Iteration 5, Batch 95/100: Batch Loss = 0.2930\n",
            "Iteration 5, Batch 96/100: Batch Loss = 0.2599\n",
            "Iteration 5, Batch 97/100: Batch Loss = 0.1819\n",
            "Iteration 5, Batch 98/100: Batch Loss = 0.2227\n",
            "Iteration 5, Batch 99/100: Batch Loss = 0.3047\n",
            "Iteration 5, Batch 100/100: Batch Loss = 0.2291\n",
            "Iteration 5: Train Loss = 0.2906, Val Loss = 0.2846\n",
            "Iteration 6, Batch 1/100: Batch Loss = 0.2135\n",
            "Iteration 6, Batch 2/100: Batch Loss = 0.3150\n",
            "Iteration 6, Batch 3/100: Batch Loss = 0.2889\n",
            "Iteration 6, Batch 4/100: Batch Loss = 0.3938\n",
            "Iteration 6, Batch 5/100: Batch Loss = 0.1842\n",
            "Iteration 6, Batch 6/100: Batch Loss = 0.2001\n",
            "Iteration 6, Batch 7/100: Batch Loss = 0.2289\n",
            "Iteration 6, Batch 8/100: Batch Loss = 0.1488\n",
            "Iteration 6, Batch 9/100: Batch Loss = 0.1649\n",
            "Iteration 6, Batch 10/100: Batch Loss = 0.2593\n",
            "Iteration 6, Batch 11/100: Batch Loss = 0.4627\n",
            "Iteration 6, Batch 12/100: Batch Loss = 0.2113\n",
            "Iteration 6, Batch 13/100: Batch Loss = 0.3000\n",
            "Iteration 6, Batch 14/100: Batch Loss = 0.3283\n",
            "Iteration 6, Batch 15/100: Batch Loss = 0.3125\n",
            "Iteration 6, Batch 16/100: Batch Loss = 0.3168\n",
            "Iteration 6, Batch 17/100: Batch Loss = 0.3287\n",
            "Iteration 6, Batch 18/100: Batch Loss = 0.3088\n",
            "Iteration 6, Batch 19/100: Batch Loss = 0.1903\n",
            "Iteration 6, Batch 20/100: Batch Loss = 0.2360\n",
            "Iteration 6, Batch 21/100: Batch Loss = 0.2274\n",
            "Iteration 6, Batch 22/100: Batch Loss = 0.2808\n",
            "Iteration 6, Batch 23/100: Batch Loss = 0.3686\n",
            "Iteration 6, Batch 24/100: Batch Loss = 0.3345\n",
            "Iteration 6, Batch 25/100: Batch Loss = 0.1740\n",
            "Iteration 6, Batch 26/100: Batch Loss = 0.1501\n",
            "Iteration 6, Batch 27/100: Batch Loss = 0.2609\n",
            "Iteration 6, Batch 28/100: Batch Loss = 0.2052\n",
            "Iteration 6, Batch 29/100: Batch Loss = 0.4249\n",
            "Iteration 6, Batch 30/100: Batch Loss = 0.3022\n",
            "Iteration 6, Batch 31/100: Batch Loss = 0.5208\n",
            "Iteration 6, Batch 32/100: Batch Loss = 0.2562\n",
            "Iteration 6, Batch 33/100: Batch Loss = 0.3085\n",
            "Iteration 6, Batch 34/100: Batch Loss = 0.3266\n",
            "Iteration 6, Batch 35/100: Batch Loss = 0.2190\n",
            "Iteration 6, Batch 36/100: Batch Loss = 0.3250\n",
            "Iteration 6, Batch 37/100: Batch Loss = 0.2929\n",
            "Iteration 6, Batch 38/100: Batch Loss = 0.2219\n",
            "Iteration 6, Batch 39/100: Batch Loss = 0.2027\n",
            "Iteration 6, Batch 40/100: Batch Loss = 0.1893\n",
            "Iteration 6, Batch 41/100: Batch Loss = 0.3542\n",
            "Iteration 6, Batch 42/100: Batch Loss = 0.1823\n",
            "Iteration 6, Batch 43/100: Batch Loss = 0.2313\n",
            "Iteration 6, Batch 44/100: Batch Loss = 0.1885\n",
            "Iteration 6, Batch 45/100: Batch Loss = 0.1104\n",
            "Iteration 6, Batch 46/100: Batch Loss = 0.2082\n",
            "Iteration 6, Batch 47/100: Batch Loss = 0.3662\n",
            "Iteration 6, Batch 48/100: Batch Loss = 0.1845\n",
            "Iteration 6, Batch 49/100: Batch Loss = 0.2327\n",
            "Iteration 6, Batch 50/100: Batch Loss = 0.2369\n",
            "Iteration 6, Batch 51/100: Batch Loss = 0.1785\n",
            "Iteration 6, Batch 52/100: Batch Loss = 0.2500\n",
            "Iteration 6, Batch 53/100: Batch Loss = 0.1952\n",
            "Iteration 6, Batch 54/100: Batch Loss = 0.2855\n",
            "Iteration 6, Batch 55/100: Batch Loss = 0.1744\n",
            "Iteration 6, Batch 56/100: Batch Loss = 0.2707\n",
            "Iteration 6, Batch 57/100: Batch Loss = 0.1212\n",
            "Iteration 6, Batch 58/100: Batch Loss = 0.2977\n",
            "Iteration 6, Batch 59/100: Batch Loss = 0.3197\n",
            "Iteration 6, Batch 60/100: Batch Loss = 0.1924\n",
            "Iteration 6, Batch 61/100: Batch Loss = 0.2503\n",
            "Iteration 6, Batch 62/100: Batch Loss = 0.2338\n",
            "Iteration 6, Batch 63/100: Batch Loss = 0.3125\n",
            "Iteration 6, Batch 64/100: Batch Loss = 0.2782\n",
            "Iteration 6, Batch 65/100: Batch Loss = 0.1585\n",
            "Iteration 6, Batch 66/100: Batch Loss = 0.2216\n",
            "Iteration 6, Batch 67/100: Batch Loss = 0.2482\n",
            "Iteration 6, Batch 68/100: Batch Loss = 0.3814\n",
            "Iteration 6, Batch 69/100: Batch Loss = 0.1976\n",
            "Iteration 6, Batch 70/100: Batch Loss = 0.2021\n",
            "Iteration 6, Batch 71/100: Batch Loss = 0.2647\n",
            "Iteration 6, Batch 72/100: Batch Loss = 0.7470\n",
            "Iteration 6, Batch 73/100: Batch Loss = 0.2711\n",
            "Iteration 6, Batch 74/100: Batch Loss = 0.1501\n",
            "Iteration 6, Batch 75/100: Batch Loss = 0.2621\n",
            "Iteration 6, Batch 76/100: Batch Loss = 0.1923\n",
            "Iteration 6, Batch 77/100: Batch Loss = 0.2461\n",
            "Iteration 6, Batch 78/100: Batch Loss = 0.2316\n",
            "Iteration 6, Batch 79/100: Batch Loss = 0.1632\n",
            "Iteration 6, Batch 80/100: Batch Loss = 0.1925\n",
            "Iteration 6, Batch 81/100: Batch Loss = 0.1319\n",
            "Iteration 6, Batch 82/100: Batch Loss = 0.2564\n",
            "Iteration 6, Batch 83/100: Batch Loss = 0.1897\n",
            "Iteration 6, Batch 84/100: Batch Loss = 0.2032\n",
            "Iteration 6, Batch 85/100: Batch Loss = 0.3189\n",
            "Iteration 6, Batch 86/100: Batch Loss = 0.2397\n",
            "Iteration 6, Batch 87/100: Batch Loss = 0.2410\n",
            "Iteration 6, Batch 88/100: Batch Loss = 0.1829\n",
            "Iteration 6, Batch 89/100: Batch Loss = 0.3832\n",
            "Iteration 6, Batch 90/100: Batch Loss = 0.3639\n",
            "Iteration 6, Batch 91/100: Batch Loss = 0.2876\n",
            "Iteration 6, Batch 92/100: Batch Loss = 0.1666\n",
            "Iteration 6, Batch 93/100: Batch Loss = 0.2241\n",
            "Iteration 6, Batch 94/100: Batch Loss = 0.1630\n",
            "Iteration 6, Batch 95/100: Batch Loss = 0.2148\n",
            "Iteration 6, Batch 96/100: Batch Loss = 0.3132\n",
            "Iteration 6, Batch 97/100: Batch Loss = 0.1656\n",
            "Iteration 6, Batch 98/100: Batch Loss = 0.2551\n",
            "Iteration 6, Batch 99/100: Batch Loss = 0.2567\n",
            "Iteration 6, Batch 100/100: Batch Loss = 0.1737\n",
            "Iteration 6: Train Loss = 0.2550, Val Loss = 0.2583\n",
            "Iteration 7, Batch 1/100: Batch Loss = 0.2877\n",
            "Iteration 7, Batch 2/100: Batch Loss = 0.2841\n",
            "Iteration 7, Batch 3/100: Batch Loss = 0.1302\n",
            "Iteration 7, Batch 4/100: Batch Loss = 0.2754\n",
            "Iteration 7, Batch 5/100: Batch Loss = 0.1667\n",
            "Iteration 7, Batch 6/100: Batch Loss = 0.2442\n",
            "Iteration 7, Batch 7/100: Batch Loss = 0.1971\n",
            "Iteration 7, Batch 8/100: Batch Loss = 0.2647\n",
            "Iteration 7, Batch 9/100: Batch Loss = 0.2313\n",
            "Iteration 7, Batch 10/100: Batch Loss = 0.2241\n",
            "Iteration 7, Batch 11/100: Batch Loss = 0.1917\n",
            "Iteration 7, Batch 12/100: Batch Loss = 0.1862\n",
            "Iteration 7, Batch 13/100: Batch Loss = 0.1504\n",
            "Iteration 7, Batch 14/100: Batch Loss = 0.1379\n",
            "Iteration 7, Batch 15/100: Batch Loss = 0.1621\n",
            "Iteration 7, Batch 16/100: Batch Loss = 0.2020\n",
            "Iteration 7, Batch 17/100: Batch Loss = 0.4061\n",
            "Iteration 7, Batch 18/100: Batch Loss = 0.2093\n",
            "Iteration 7, Batch 19/100: Batch Loss = 0.3004\n",
            "Iteration 7, Batch 20/100: Batch Loss = 0.1273\n",
            "Iteration 7, Batch 21/100: Batch Loss = 0.2438\n",
            "Iteration 7, Batch 22/100: Batch Loss = 0.1547\n",
            "Iteration 7, Batch 23/100: Batch Loss = 0.1267\n",
            "Iteration 7, Batch 24/100: Batch Loss = 0.2143\n",
            "Iteration 7, Batch 25/100: Batch Loss = 0.3314\n",
            "Iteration 7, Batch 26/100: Batch Loss = 0.1685\n",
            "Iteration 7, Batch 27/100: Batch Loss = 0.1946\n",
            "Iteration 7, Batch 28/100: Batch Loss = 0.2931\n",
            "Iteration 7, Batch 29/100: Batch Loss = 0.4096\n",
            "Iteration 7, Batch 30/100: Batch Loss = 0.2381\n",
            "Iteration 7, Batch 31/100: Batch Loss = 0.1442\n",
            "Iteration 7, Batch 32/100: Batch Loss = 0.3082\n",
            "Iteration 7, Batch 33/100: Batch Loss = 0.2525\n",
            "Iteration 7, Batch 34/100: Batch Loss = 0.2054\n",
            "Iteration 7, Batch 35/100: Batch Loss = 0.2914\n",
            "Iteration 7, Batch 36/100: Batch Loss = 0.1327\n",
            "Iteration 7, Batch 37/100: Batch Loss = 0.2796\n",
            "Iteration 7, Batch 38/100: Batch Loss = 0.1865\n",
            "Iteration 7, Batch 39/100: Batch Loss = 0.2886\n",
            "Iteration 7, Batch 40/100: Batch Loss = 0.1978\n",
            "Iteration 7, Batch 41/100: Batch Loss = 0.2019\n",
            "Iteration 7, Batch 42/100: Batch Loss = 0.1649\n",
            "Iteration 7, Batch 43/100: Batch Loss = 0.2392\n",
            "Iteration 7, Batch 44/100: Batch Loss = 0.2973\n",
            "Iteration 7, Batch 45/100: Batch Loss = 0.1511\n",
            "Iteration 7, Batch 46/100: Batch Loss = 0.3073\n",
            "Iteration 7, Batch 47/100: Batch Loss = 0.1451\n",
            "Iteration 7, Batch 48/100: Batch Loss = 0.3521\n",
            "Iteration 7, Batch 49/100: Batch Loss = 0.1972\n",
            "Iteration 7, Batch 50/100: Batch Loss = 0.1503\n",
            "Iteration 7, Batch 51/100: Batch Loss = 0.1717\n",
            "Iteration 7, Batch 52/100: Batch Loss = 0.2176\n",
            "Iteration 7, Batch 53/100: Batch Loss = 0.2697\n",
            "Iteration 7, Batch 54/100: Batch Loss = 0.2986\n",
            "Iteration 7, Batch 55/100: Batch Loss = 0.1175\n",
            "Iteration 7, Batch 56/100: Batch Loss = 0.2698\n",
            "Iteration 7, Batch 57/100: Batch Loss = 0.1718\n",
            "Iteration 7, Batch 58/100: Batch Loss = 0.2459\n",
            "Iteration 7, Batch 59/100: Batch Loss = 0.1793\n",
            "Iteration 7, Batch 60/100: Batch Loss = 0.2073\n",
            "Iteration 7, Batch 61/100: Batch Loss = 0.2313\n",
            "Iteration 7, Batch 62/100: Batch Loss = 0.1769\n",
            "Iteration 7, Batch 63/100: Batch Loss = 0.2615\n",
            "Iteration 7, Batch 64/100: Batch Loss = 0.1508\n",
            "Iteration 7, Batch 65/100: Batch Loss = 0.1610\n",
            "Iteration 7, Batch 66/100: Batch Loss = 0.2329\n",
            "Iteration 7, Batch 67/100: Batch Loss = 0.1896\n",
            "Iteration 7, Batch 68/100: Batch Loss = 0.1878\n",
            "Iteration 7, Batch 69/100: Batch Loss = 0.0871\n",
            "Iteration 7, Batch 70/100: Batch Loss = 0.1381\n",
            "Iteration 7, Batch 71/100: Batch Loss = 0.2125\n",
            "Iteration 7, Batch 72/100: Batch Loss = 0.2276\n",
            "Iteration 7, Batch 73/100: Batch Loss = 0.1911\n",
            "Iteration 7, Batch 74/100: Batch Loss = 0.1780\n",
            "Iteration 7, Batch 75/100: Batch Loss = 0.3117\n",
            "Iteration 7, Batch 76/100: Batch Loss = 0.2757\n",
            "Iteration 7, Batch 77/100: Batch Loss = 0.3314\n",
            "Iteration 7, Batch 78/100: Batch Loss = 0.1361\n",
            "Iteration 7, Batch 79/100: Batch Loss = 0.1264\n",
            "Iteration 7, Batch 80/100: Batch Loss = 0.2202\n",
            "Iteration 7, Batch 81/100: Batch Loss = 0.2241\n",
            "Iteration 7, Batch 82/100: Batch Loss = 0.1725\n",
            "Iteration 7, Batch 83/100: Batch Loss = 0.1658\n",
            "Iteration 7, Batch 84/100: Batch Loss = 0.1435\n",
            "Iteration 7, Batch 85/100: Batch Loss = 0.2166\n",
            "Iteration 7, Batch 86/100: Batch Loss = 0.1878\n",
            "Iteration 7, Batch 87/100: Batch Loss = 0.2960\n",
            "Iteration 7, Batch 88/100: Batch Loss = 0.2169\n",
            "Iteration 7, Batch 89/100: Batch Loss = 0.2229\n",
            "Iteration 7, Batch 90/100: Batch Loss = 0.0687\n",
            "Iteration 7, Batch 91/100: Batch Loss = 0.1856\n",
            "Iteration 7, Batch 92/100: Batch Loss = 0.1851\n",
            "Iteration 7, Batch 93/100: Batch Loss = 0.1879\n",
            "Iteration 7, Batch 94/100: Batch Loss = 0.2004\n",
            "Iteration 7, Batch 95/100: Batch Loss = 0.2069\n",
            "Iteration 7, Batch 96/100: Batch Loss = 0.1353\n",
            "Iteration 7, Batch 97/100: Batch Loss = 0.0962\n",
            "Iteration 7, Batch 98/100: Batch Loss = 0.1184\n",
            "Iteration 7, Batch 99/100: Batch Loss = 0.3108\n",
            "Iteration 7, Batch 100/100: Batch Loss = 0.2224\n",
            "Iteration 7: Train Loss = 0.2119, Val Loss = 0.2341\n",
            "Iteration 8, Batch 1/100: Batch Loss = 0.2716\n",
            "Iteration 8, Batch 2/100: Batch Loss = 0.1529\n",
            "Iteration 8, Batch 3/100: Batch Loss = 0.1563\n",
            "Iteration 8, Batch 4/100: Batch Loss = 0.2443\n",
            "Iteration 8, Batch 5/100: Batch Loss = 0.2918\n",
            "Iteration 8, Batch 6/100: Batch Loss = 0.2978\n",
            "Iteration 8, Batch 7/100: Batch Loss = 0.1581\n",
            "Iteration 8, Batch 8/100: Batch Loss = 0.1346\n",
            "Iteration 8, Batch 9/100: Batch Loss = 0.1352\n",
            "Iteration 8, Batch 10/100: Batch Loss = 0.2327\n",
            "Iteration 8, Batch 11/100: Batch Loss = 0.0968\n",
            "Iteration 8, Batch 12/100: Batch Loss = 0.1484\n",
            "Iteration 8, Batch 13/100: Batch Loss = 0.1381\n",
            "Iteration 8, Batch 14/100: Batch Loss = 0.1877\n",
            "Iteration 8, Batch 15/100: Batch Loss = 0.2044\n",
            "Iteration 8, Batch 16/100: Batch Loss = 0.0982\n",
            "Iteration 8, Batch 17/100: Batch Loss = 0.1345\n",
            "Iteration 8, Batch 18/100: Batch Loss = 0.1752\n",
            "Iteration 8, Batch 19/100: Batch Loss = 0.3325\n",
            "Iteration 8, Batch 20/100: Batch Loss = 0.4088\n",
            "Iteration 8, Batch 21/100: Batch Loss = 0.2839\n",
            "Iteration 8, Batch 22/100: Batch Loss = 0.1510\n",
            "Iteration 8, Batch 23/100: Batch Loss = 0.1956\n",
            "Iteration 8, Batch 24/100: Batch Loss = 0.2624\n",
            "Iteration 8, Batch 25/100: Batch Loss = 0.1007\n",
            "Iteration 8, Batch 26/100: Batch Loss = 0.1063\n",
            "Iteration 8, Batch 27/100: Batch Loss = 0.2224\n",
            "Iteration 8, Batch 28/100: Batch Loss = 0.1777\n",
            "Iteration 8, Batch 29/100: Batch Loss = 0.2707\n",
            "Iteration 8, Batch 30/100: Batch Loss = 0.1861\n",
            "Iteration 8, Batch 31/100: Batch Loss = 0.2785\n",
            "Iteration 8, Batch 32/100: Batch Loss = 0.0941\n",
            "Iteration 8, Batch 33/100: Batch Loss = 0.1699\n",
            "Iteration 8, Batch 34/100: Batch Loss = 0.2923\n",
            "Iteration 8, Batch 35/100: Batch Loss = 0.1528\n",
            "Iteration 8, Batch 36/100: Batch Loss = 0.1732\n",
            "Iteration 8, Batch 37/100: Batch Loss = 0.1346\n",
            "Iteration 8, Batch 38/100: Batch Loss = 0.1483\n",
            "Iteration 8, Batch 39/100: Batch Loss = 0.2770\n",
            "Iteration 8, Batch 40/100: Batch Loss = 0.1376\n",
            "Iteration 8, Batch 41/100: Batch Loss = 0.1312\n",
            "Iteration 8, Batch 42/100: Batch Loss = 0.1714\n",
            "Iteration 8, Batch 43/100: Batch Loss = 0.0870\n",
            "Iteration 8, Batch 44/100: Batch Loss = 0.2446\n",
            "Iteration 8, Batch 45/100: Batch Loss = 0.2345\n",
            "Iteration 8, Batch 46/100: Batch Loss = 0.2880\n",
            "Iteration 8, Batch 47/100: Batch Loss = 0.1454\n",
            "Iteration 8, Batch 48/100: Batch Loss = 0.1252\n",
            "Iteration 8, Batch 49/100: Batch Loss = 0.1706\n",
            "Iteration 8, Batch 50/100: Batch Loss = 0.2055\n",
            "Iteration 8, Batch 51/100: Batch Loss = 0.2564\n",
            "Iteration 8, Batch 52/100: Batch Loss = 0.2925\n",
            "Iteration 8, Batch 53/100: Batch Loss = 0.4100\n",
            "Iteration 8, Batch 54/100: Batch Loss = 0.1386\n",
            "Iteration 8, Batch 55/100: Batch Loss = 0.2869\n",
            "Iteration 8, Batch 56/100: Batch Loss = 0.1252\n",
            "Iteration 8, Batch 57/100: Batch Loss = 0.1637\n",
            "Iteration 8, Batch 58/100: Batch Loss = 0.3096\n",
            "Iteration 8, Batch 59/100: Batch Loss = 0.1314\n",
            "Iteration 8, Batch 60/100: Batch Loss = 0.2520\n",
            "Iteration 8, Batch 61/100: Batch Loss = 0.2129\n",
            "Iteration 8, Batch 62/100: Batch Loss = 0.2053\n",
            "Iteration 8, Batch 63/100: Batch Loss = 0.3486\n",
            "Iteration 8, Batch 64/100: Batch Loss = 0.1795\n",
            "Iteration 8, Batch 65/100: Batch Loss = 0.1787\n",
            "Iteration 8, Batch 66/100: Batch Loss = 0.1522\n",
            "Iteration 8, Batch 67/100: Batch Loss = 0.2751\n",
            "Iteration 8, Batch 68/100: Batch Loss = 0.1472\n",
            "Iteration 8, Batch 69/100: Batch Loss = 0.2884\n",
            "Iteration 8, Batch 70/100: Batch Loss = 0.3264\n",
            "Iteration 8, Batch 71/100: Batch Loss = 0.2044\n",
            "Iteration 8, Batch 72/100: Batch Loss = 0.1628\n",
            "Iteration 8, Batch 73/100: Batch Loss = 0.0813\n",
            "Iteration 8, Batch 74/100: Batch Loss = 0.1107\n",
            "Iteration 8, Batch 75/100: Batch Loss = 0.1615\n",
            "Iteration 8, Batch 76/100: Batch Loss = 0.2505\n",
            "Iteration 8, Batch 77/100: Batch Loss = 0.1420\n",
            "Iteration 8, Batch 78/100: Batch Loss = 0.4190\n",
            "Iteration 8, Batch 79/100: Batch Loss = 0.2041\n",
            "Iteration 8, Batch 80/100: Batch Loss = 0.1832\n",
            "Iteration 8, Batch 81/100: Batch Loss = 0.3015\n",
            "Iteration 8, Batch 82/100: Batch Loss = 0.1987\n",
            "Iteration 8, Batch 83/100: Batch Loss = 0.2684\n",
            "Iteration 8, Batch 84/100: Batch Loss = 0.1884\n",
            "Iteration 8, Batch 85/100: Batch Loss = 0.1526\n",
            "Iteration 8, Batch 86/100: Batch Loss = 0.1134\n",
            "Iteration 8, Batch 87/100: Batch Loss = 0.1798\n",
            "Iteration 8, Batch 88/100: Batch Loss = 0.2003\n",
            "Iteration 8, Batch 89/100: Batch Loss = 0.2100\n",
            "Iteration 8, Batch 90/100: Batch Loss = 0.1343\n",
            "Iteration 8, Batch 91/100: Batch Loss = 0.1728\n",
            "Iteration 8, Batch 92/100: Batch Loss = 0.1535\n",
            "Iteration 8, Batch 93/100: Batch Loss = 0.1680\n",
            "Iteration 8, Batch 94/100: Batch Loss = 0.1145\n",
            "Iteration 8, Batch 95/100: Batch Loss = 0.2447\n",
            "Iteration 8, Batch 96/100: Batch Loss = 0.2992\n",
            "Iteration 8, Batch 97/100: Batch Loss = 0.1260\n",
            "Iteration 8, Batch 98/100: Batch Loss = 0.1190\n",
            "Iteration 8, Batch 99/100: Batch Loss = 0.2164\n",
            "Iteration 8, Batch 100/100: Batch Loss = 0.0917\n",
            "Iteration 8: Train Loss = 0.1987, Val Loss = 0.2081\n",
            "Iteration 9, Batch 1/100: Batch Loss = 0.1727\n",
            "Iteration 9, Batch 2/100: Batch Loss = 0.0996\n",
            "Iteration 9, Batch 3/100: Batch Loss = 0.1810\n",
            "Iteration 9, Batch 4/100: Batch Loss = 0.1659\n",
            "Iteration 9, Batch 5/100: Batch Loss = 0.3042\n",
            "Iteration 9, Batch 6/100: Batch Loss = 0.2163\n",
            "Iteration 9, Batch 7/100: Batch Loss = 0.1701\n",
            "Iteration 9, Batch 8/100: Batch Loss = 0.0847\n",
            "Iteration 9, Batch 9/100: Batch Loss = 0.1584\n",
            "Iteration 9, Batch 10/100: Batch Loss = 0.1158\n",
            "Iteration 9, Batch 11/100: Batch Loss = 0.1969\n",
            "Iteration 9, Batch 12/100: Batch Loss = 0.3309\n",
            "Iteration 9, Batch 13/100: Batch Loss = 0.1749\n",
            "Iteration 9, Batch 14/100: Batch Loss = 0.2018\n",
            "Iteration 9, Batch 15/100: Batch Loss = 0.1973\n",
            "Iteration 9, Batch 16/100: Batch Loss = 0.1333\n",
            "Iteration 9, Batch 17/100: Batch Loss = 0.1813\n",
            "Iteration 9, Batch 18/100: Batch Loss = 0.1288\n",
            "Iteration 9, Batch 19/100: Batch Loss = 0.1447\n",
            "Iteration 9, Batch 20/100: Batch Loss = 0.4027\n",
            "Iteration 9, Batch 21/100: Batch Loss = 0.1954\n",
            "Iteration 9, Batch 22/100: Batch Loss = 0.0922\n",
            "Iteration 9, Batch 23/100: Batch Loss = 0.1762\n",
            "Iteration 9, Batch 24/100: Batch Loss = 0.2468\n",
            "Iteration 9, Batch 25/100: Batch Loss = 0.2007\n",
            "Iteration 9, Batch 26/100: Batch Loss = 0.2177\n",
            "Iteration 9, Batch 27/100: Batch Loss = 0.1636\n",
            "Iteration 9, Batch 28/100: Batch Loss = 0.1081\n",
            "Iteration 9, Batch 29/100: Batch Loss = 0.2054\n",
            "Iteration 9, Batch 30/100: Batch Loss = 0.0953\n",
            "Iteration 9, Batch 31/100: Batch Loss = 0.0825\n",
            "Iteration 9, Batch 32/100: Batch Loss = 0.2201\n",
            "Iteration 9, Batch 33/100: Batch Loss = 0.1237\n",
            "Iteration 9, Batch 34/100: Batch Loss = 0.3396\n",
            "Iteration 9, Batch 35/100: Batch Loss = 0.1732\n",
            "Iteration 9, Batch 36/100: Batch Loss = 0.2644\n",
            "Iteration 9, Batch 37/100: Batch Loss = 0.1500\n",
            "Iteration 9, Batch 38/100: Batch Loss = 0.0923\n",
            "Iteration 9, Batch 39/100: Batch Loss = 0.2234\n",
            "Iteration 9, Batch 40/100: Batch Loss = 0.2327\n",
            "Iteration 9, Batch 41/100: Batch Loss = 0.1483\n",
            "Iteration 9, Batch 42/100: Batch Loss = 0.2613\n",
            "Iteration 9, Batch 43/100: Batch Loss = 0.2623\n",
            "Iteration 9, Batch 44/100: Batch Loss = 0.1158\n",
            "Iteration 9, Batch 45/100: Batch Loss = 0.1632\n",
            "Iteration 9, Batch 46/100: Batch Loss = 0.1224\n",
            "Iteration 9, Batch 47/100: Batch Loss = 0.1471\n",
            "Iteration 9, Batch 48/100: Batch Loss = 0.1963\n",
            "Iteration 9, Batch 49/100: Batch Loss = 0.2655\n",
            "Iteration 9, Batch 50/100: Batch Loss = 0.2441\n",
            "Iteration 9, Batch 51/100: Batch Loss = 0.1706\n",
            "Iteration 9, Batch 52/100: Batch Loss = 0.2295\n",
            "Iteration 9, Batch 53/100: Batch Loss = 0.2974\n",
            "Iteration 9, Batch 54/100: Batch Loss = 0.2441\n",
            "Iteration 9, Batch 55/100: Batch Loss = 0.0857\n",
            "Iteration 9, Batch 56/100: Batch Loss = 0.3218\n",
            "Iteration 9, Batch 57/100: Batch Loss = 0.3138\n",
            "Iteration 9, Batch 58/100: Batch Loss = 0.1106\n",
            "Iteration 9, Batch 59/100: Batch Loss = 0.1110\n",
            "Iteration 9, Batch 60/100: Batch Loss = 0.1638\n",
            "Iteration 9, Batch 61/100: Batch Loss = 0.0906\n",
            "Iteration 9, Batch 62/100: Batch Loss = 0.0718\n",
            "Iteration 9, Batch 63/100: Batch Loss = 0.1702\n",
            "Iteration 9, Batch 64/100: Batch Loss = 0.1669\n",
            "Iteration 9, Batch 65/100: Batch Loss = 0.1708\n",
            "Iteration 9, Batch 66/100: Batch Loss = 0.2098\n",
            "Iteration 9, Batch 67/100: Batch Loss = 0.1387\n",
            "Iteration 9, Batch 68/100: Batch Loss = 0.0868\n",
            "Iteration 9, Batch 69/100: Batch Loss = 0.1529\n",
            "Iteration 9, Batch 70/100: Batch Loss = 0.1206\n",
            "Iteration 9, Batch 71/100: Batch Loss = 0.3010\n",
            "Iteration 9, Batch 72/100: Batch Loss = 0.1724\n",
            "Iteration 9, Batch 73/100: Batch Loss = 0.1623\n",
            "Iteration 9, Batch 74/100: Batch Loss = 0.1258\n",
            "Iteration 9, Batch 75/100: Batch Loss = 0.1236\n",
            "Iteration 9, Batch 76/100: Batch Loss = 0.1684\n",
            "Iteration 9, Batch 77/100: Batch Loss = 0.2333\n",
            "Iteration 9, Batch 78/100: Batch Loss = 0.1772\n",
            "Iteration 9, Batch 79/100: Batch Loss = 0.1579\n",
            "Iteration 9, Batch 80/100: Batch Loss = 0.1437\n",
            "Iteration 9, Batch 81/100: Batch Loss = 0.2957\n",
            "Iteration 9, Batch 82/100: Batch Loss = 0.1927\n",
            "Iteration 9, Batch 83/100: Batch Loss = 0.0675\n",
            "Iteration 9, Batch 84/100: Batch Loss = 0.1111\n",
            "Iteration 9, Batch 85/100: Batch Loss = 0.1966\n",
            "Iteration 9, Batch 86/100: Batch Loss = 0.1012\n",
            "Iteration 9, Batch 87/100: Batch Loss = 0.2433\n",
            "Iteration 9, Batch 88/100: Batch Loss = 0.2557\n",
            "Iteration 9, Batch 89/100: Batch Loss = 0.1948\n",
            "Iteration 9, Batch 90/100: Batch Loss = 0.1406\n",
            "Iteration 9, Batch 91/100: Batch Loss = 0.2151\n",
            "Iteration 9, Batch 92/100: Batch Loss = 0.1926\n",
            "Iteration 9, Batch 93/100: Batch Loss = 0.1507\n",
            "Iteration 9, Batch 94/100: Batch Loss = 0.2228\n",
            "Iteration 9, Batch 95/100: Batch Loss = 0.2485\n",
            "Iteration 9, Batch 96/100: Batch Loss = 0.1655\n",
            "Iteration 9, Batch 97/100: Batch Loss = 0.2683\n",
            "Iteration 9, Batch 98/100: Batch Loss = 0.0956\n",
            "Iteration 9, Batch 99/100: Batch Loss = 0.1198\n",
            "Iteration 9, Batch 100/100: Batch Loss = 0.3815\n",
            "Iteration 9: Train Loss = 0.1834, Val Loss = 0.2076\n",
            "Iteration 10, Batch 1/100: Batch Loss = 0.1860\n",
            "Iteration 10, Batch 2/100: Batch Loss = 0.1009\n",
            "Iteration 10, Batch 3/100: Batch Loss = 0.0781\n",
            "Iteration 10, Batch 4/100: Batch Loss = 0.1365\n",
            "Iteration 10, Batch 5/100: Batch Loss = 0.0830\n",
            "Iteration 10, Batch 6/100: Batch Loss = 0.3871\n",
            "Iteration 10, Batch 7/100: Batch Loss = 0.2677\n",
            "Iteration 10, Batch 8/100: Batch Loss = 0.1613\n",
            "Iteration 10, Batch 9/100: Batch Loss = 0.1054\n",
            "Iteration 10, Batch 10/100: Batch Loss = 0.1902\n",
            "Iteration 10, Batch 11/100: Batch Loss = 0.2327\n",
            "Iteration 10, Batch 12/100: Batch Loss = 0.1467\n",
            "Iteration 10, Batch 13/100: Batch Loss = 0.1243\n",
            "Iteration 10, Batch 14/100: Batch Loss = 0.1523\n",
            "Iteration 10, Batch 15/100: Batch Loss = 0.1872\n",
            "Iteration 10, Batch 16/100: Batch Loss = 0.1649\n",
            "Iteration 10, Batch 17/100: Batch Loss = 0.1207\n",
            "Iteration 10, Batch 18/100: Batch Loss = 0.2252\n",
            "Iteration 10, Batch 19/100: Batch Loss = 0.1664\n",
            "Iteration 10, Batch 20/100: Batch Loss = 0.1868\n",
            "Iteration 10, Batch 21/100: Batch Loss = 0.0928\n",
            "Iteration 10, Batch 22/100: Batch Loss = 0.2463\n",
            "Iteration 10, Batch 23/100: Batch Loss = 0.1335\n",
            "Iteration 10, Batch 24/100: Batch Loss = 0.2777\n",
            "Iteration 10, Batch 25/100: Batch Loss = 0.0705\n",
            "Iteration 10, Batch 26/100: Batch Loss = 0.0924\n",
            "Iteration 10, Batch 27/100: Batch Loss = 0.1299\n",
            "Iteration 10, Batch 28/100: Batch Loss = 0.2500\n",
            "Iteration 10, Batch 29/100: Batch Loss = 0.1408\n",
            "Iteration 10, Batch 30/100: Batch Loss = 0.1956\n",
            "Iteration 10, Batch 31/100: Batch Loss = 0.1081\n",
            "Iteration 10, Batch 32/100: Batch Loss = 0.3470\n",
            "Iteration 10, Batch 33/100: Batch Loss = 0.2004\n",
            "Iteration 10, Batch 34/100: Batch Loss = 0.0883\n",
            "Iteration 10, Batch 35/100: Batch Loss = 0.0932\n",
            "Iteration 10, Batch 36/100: Batch Loss = 0.2951\n",
            "Iteration 10, Batch 37/100: Batch Loss = 0.2480\n",
            "Iteration 10, Batch 38/100: Batch Loss = 0.3215\n",
            "Iteration 10, Batch 39/100: Batch Loss = 0.2017\n",
            "Iteration 10, Batch 40/100: Batch Loss = 0.2504\n",
            "Iteration 10, Batch 41/100: Batch Loss = 0.1789\n",
            "Iteration 10, Batch 42/100: Batch Loss = 0.4147\n",
            "Iteration 10, Batch 43/100: Batch Loss = 0.2904\n",
            "Iteration 10, Batch 44/100: Batch Loss = 0.1056\n",
            "Iteration 10, Batch 45/100: Batch Loss = 0.1254\n",
            "Iteration 10, Batch 46/100: Batch Loss = 0.1974\n",
            "Iteration 10, Batch 47/100: Batch Loss = 0.0886\n",
            "Iteration 10, Batch 48/100: Batch Loss = 0.1265\n",
            "Iteration 10, Batch 49/100: Batch Loss = 0.1662\n",
            "Iteration 10, Batch 50/100: Batch Loss = 0.2108\n",
            "Iteration 10, Batch 51/100: Batch Loss = 0.1314\n",
            "Iteration 10, Batch 52/100: Batch Loss = 0.1202\n",
            "Iteration 10, Batch 53/100: Batch Loss = 0.1098\n",
            "Iteration 10, Batch 54/100: Batch Loss = 0.4863\n",
            "Iteration 10, Batch 55/100: Batch Loss = 0.1062\n",
            "Iteration 10, Batch 56/100: Batch Loss = 0.1876\n",
            "Iteration 10, Batch 57/100: Batch Loss = 0.2666\n",
            "Iteration 10, Batch 58/100: Batch Loss = 0.1836\n",
            "Iteration 10, Batch 59/100: Batch Loss = 0.3151\n",
            "Iteration 10, Batch 60/100: Batch Loss = 0.1570\n",
            "Iteration 10, Batch 61/100: Batch Loss = 0.1243\n",
            "Iteration 10, Batch 62/100: Batch Loss = 0.1088\n",
            "Iteration 10, Batch 63/100: Batch Loss = 0.1365\n",
            "Iteration 10, Batch 64/100: Batch Loss = 0.0971\n",
            "Iteration 10, Batch 65/100: Batch Loss = 0.2316\n",
            "Iteration 10, Batch 66/100: Batch Loss = 0.1673\n",
            "Iteration 10, Batch 67/100: Batch Loss = 0.2453\n",
            "Iteration 10, Batch 68/100: Batch Loss = 0.1614\n",
            "Iteration 10, Batch 69/100: Batch Loss = 0.1585\n",
            "Iteration 10, Batch 70/100: Batch Loss = 0.1018\n",
            "Iteration 10, Batch 71/100: Batch Loss = 0.2277\n",
            "Iteration 10, Batch 72/100: Batch Loss = 0.0825\n",
            "Iteration 10, Batch 73/100: Batch Loss = 0.0786\n",
            "Iteration 10, Batch 74/100: Batch Loss = 0.1705\n",
            "Iteration 10, Batch 75/100: Batch Loss = 0.1179\n",
            "Iteration 10, Batch 76/100: Batch Loss = 0.1556\n",
            "Iteration 10, Batch 77/100: Batch Loss = 0.1898\n",
            "Iteration 10, Batch 78/100: Batch Loss = 0.0948\n",
            "Iteration 10, Batch 79/100: Batch Loss = 0.0875\n",
            "Iteration 10, Batch 80/100: Batch Loss = 0.2400\n",
            "Iteration 10, Batch 81/100: Batch Loss = 0.1073\n",
            "Iteration 10, Batch 82/100: Batch Loss = 0.3622\n",
            "Iteration 10, Batch 83/100: Batch Loss = 0.1073\n",
            "Iteration 10, Batch 84/100: Batch Loss = 0.1090\n",
            "Iteration 10, Batch 85/100: Batch Loss = 0.2075\n",
            "Iteration 10, Batch 86/100: Batch Loss = 0.1527\n",
            "Iteration 10, Batch 87/100: Batch Loss = 0.2217\n",
            "Iteration 10, Batch 88/100: Batch Loss = 0.1325\n",
            "Iteration 10, Batch 89/100: Batch Loss = 0.1558\n",
            "Iteration 10, Batch 90/100: Batch Loss = 0.2700\n",
            "Iteration 10, Batch 91/100: Batch Loss = 0.1317\n",
            "Iteration 10, Batch 92/100: Batch Loss = 0.3166\n",
            "Iteration 10, Batch 93/100: Batch Loss = 0.1789\n",
            "Iteration 10, Batch 94/100: Batch Loss = 0.0785\n",
            "Iteration 10, Batch 95/100: Batch Loss = 0.3002\n",
            "Iteration 10, Batch 96/100: Batch Loss = 0.1813\n",
            "Iteration 10, Batch 97/100: Batch Loss = 0.1221\n",
            "Iteration 10, Batch 98/100: Batch Loss = 0.2826\n",
            "Iteration 10, Batch 99/100: Batch Loss = 0.1535\n",
            "Iteration 10, Batch 100/100: Batch Loss = 0.1101\n",
            "Iteration 10: Train Loss = 0.1781, Val Loss = 0.2065\n",
            "Iteration 11, Batch 1/100: Batch Loss = 0.0812\n",
            "Iteration 11, Batch 2/100: Batch Loss = 0.2102\n",
            "Iteration 11, Batch 3/100: Batch Loss = 0.2360\n",
            "Iteration 11, Batch 4/100: Batch Loss = 0.1002\n",
            "Iteration 11, Batch 5/100: Batch Loss = 0.1056\n",
            "Iteration 11, Batch 6/100: Batch Loss = 0.0877\n",
            "Iteration 11, Batch 7/100: Batch Loss = 0.1427\n",
            "Iteration 11, Batch 8/100: Batch Loss = 0.0718\n",
            "Iteration 11, Batch 9/100: Batch Loss = 0.4262\n",
            "Iteration 11, Batch 10/100: Batch Loss = 0.0851\n",
            "Iteration 11, Batch 11/100: Batch Loss = 0.1060\n",
            "Iteration 11, Batch 12/100: Batch Loss = 0.1391\n",
            "Iteration 11, Batch 13/100: Batch Loss = 0.0793\n",
            "Iteration 11, Batch 14/100: Batch Loss = 0.0824\n",
            "Iteration 11, Batch 15/100: Batch Loss = 0.1907\n",
            "Iteration 11, Batch 16/100: Batch Loss = 0.3298\n",
            "Iteration 11, Batch 17/100: Batch Loss = 0.1979\n",
            "Iteration 11, Batch 18/100: Batch Loss = 0.1717\n",
            "Iteration 11, Batch 19/100: Batch Loss = 0.3229\n",
            "Iteration 11, Batch 20/100: Batch Loss = 0.1158\n",
            "Iteration 11, Batch 21/100: Batch Loss = 0.2104\n",
            "Iteration 11, Batch 22/100: Batch Loss = 0.2034\n",
            "Iteration 11, Batch 23/100: Batch Loss = 0.0784\n",
            "Iteration 11, Batch 24/100: Batch Loss = 0.1386\n",
            "Iteration 11, Batch 25/100: Batch Loss = 0.2013\n",
            "Iteration 11, Batch 26/100: Batch Loss = 0.2797\n",
            "Iteration 11, Batch 27/100: Batch Loss = 0.1255\n",
            "Iteration 11, Batch 28/100: Batch Loss = 0.2256\n",
            "Iteration 11, Batch 29/100: Batch Loss = 0.1721\n",
            "Iteration 11, Batch 30/100: Batch Loss = 0.0519\n",
            "Iteration 11, Batch 31/100: Batch Loss = 0.1042\n",
            "Iteration 11, Batch 32/100: Batch Loss = 0.2610\n",
            "Iteration 11, Batch 33/100: Batch Loss = 0.0918\n",
            "Iteration 11, Batch 34/100: Batch Loss = 0.1865\n",
            "Iteration 11, Batch 35/100: Batch Loss = 0.1443\n",
            "Iteration 11, Batch 36/100: Batch Loss = 0.1698\n",
            "Iteration 11, Batch 37/100: Batch Loss = 0.1400\n",
            "Iteration 11, Batch 38/100: Batch Loss = 0.0922\n",
            "Iteration 11, Batch 39/100: Batch Loss = 0.1873\n",
            "Iteration 11, Batch 40/100: Batch Loss = 0.0617\n",
            "Iteration 11, Batch 41/100: Batch Loss = 0.0665\n",
            "Iteration 11, Batch 42/100: Batch Loss = 0.2266\n",
            "Iteration 11, Batch 43/100: Batch Loss = 0.2002\n",
            "Iteration 11, Batch 44/100: Batch Loss = 0.1672\n",
            "Iteration 11, Batch 45/100: Batch Loss = 0.1419\n",
            "Iteration 11, Batch 46/100: Batch Loss = 0.2490\n",
            "Iteration 11, Batch 47/100: Batch Loss = 0.1353\n",
            "Iteration 11, Batch 48/100: Batch Loss = 0.1785\n",
            "Iteration 11, Batch 49/100: Batch Loss = 0.1945\n",
            "Iteration 11, Batch 50/100: Batch Loss = 0.1378\n",
            "Iteration 11, Batch 51/100: Batch Loss = 0.1704\n",
            "Iteration 11, Batch 52/100: Batch Loss = 0.1997\n",
            "Iteration 11, Batch 53/100: Batch Loss = 0.1414\n",
            "Iteration 11, Batch 54/100: Batch Loss = 0.1909\n",
            "Iteration 11, Batch 55/100: Batch Loss = 0.0890\n",
            "Iteration 11, Batch 56/100: Batch Loss = 0.0745\n",
            "Iteration 11, Batch 57/100: Batch Loss = 0.1428\n",
            "Iteration 11, Batch 58/100: Batch Loss = 0.2372\n",
            "Iteration 11, Batch 59/100: Batch Loss = 0.0840\n",
            "Iteration 11, Batch 60/100: Batch Loss = 0.1692\n",
            "Iteration 11, Batch 61/100: Batch Loss = 0.0912\n",
            "Iteration 11, Batch 62/100: Batch Loss = 0.1112\n",
            "Iteration 11, Batch 63/100: Batch Loss = 0.1588\n",
            "Iteration 11, Batch 64/100: Batch Loss = 0.1660\n",
            "Iteration 11, Batch 65/100: Batch Loss = 0.0897\n",
            "Iteration 11, Batch 66/100: Batch Loss = 0.1523\n",
            "Iteration 11, Batch 67/100: Batch Loss = 0.1638\n",
            "Iteration 11, Batch 68/100: Batch Loss = 0.2752\n",
            "Iteration 11, Batch 69/100: Batch Loss = 0.2159\n",
            "Iteration 11, Batch 70/100: Batch Loss = 0.0683\n",
            "Iteration 11, Batch 71/100: Batch Loss = 0.2727\n",
            "Iteration 11, Batch 72/100: Batch Loss = 0.2254\n",
            "Iteration 11, Batch 73/100: Batch Loss = 0.0723\n",
            "Iteration 11, Batch 74/100: Batch Loss = 0.3646\n",
            "Iteration 11, Batch 75/100: Batch Loss = 0.0860\n",
            "Iteration 11, Batch 76/100: Batch Loss = 0.2392\n",
            "Iteration 11, Batch 77/100: Batch Loss = 0.2003\n",
            "Iteration 11, Batch 78/100: Batch Loss = 0.2736\n",
            "Iteration 11, Batch 79/100: Batch Loss = 0.1342\n",
            "Iteration 11, Batch 80/100: Batch Loss = 0.1866\n",
            "Iteration 11, Batch 81/100: Batch Loss = 0.1616\n",
            "Iteration 11, Batch 82/100: Batch Loss = 0.2113\n",
            "Iteration 11, Batch 83/100: Batch Loss = 0.2715\n",
            "Iteration 11, Batch 84/100: Batch Loss = 0.0936\n",
            "Iteration 11, Batch 85/100: Batch Loss = 0.0530\n",
            "Iteration 11, Batch 86/100: Batch Loss = 0.2735\n",
            "Iteration 11, Batch 87/100: Batch Loss = 0.1545\n",
            "Iteration 11, Batch 88/100: Batch Loss = 0.2707\n",
            "Iteration 11, Batch 89/100: Batch Loss = 0.2179\n",
            "Iteration 11, Batch 90/100: Batch Loss = 0.2768\n",
            "Iteration 11, Batch 91/100: Batch Loss = 0.2312\n",
            "Iteration 11, Batch 92/100: Batch Loss = 0.0642\n",
            "Iteration 11, Batch 93/100: Batch Loss = 0.3207\n",
            "Iteration 11, Batch 94/100: Batch Loss = 0.1357\n",
            "Iteration 11, Batch 95/100: Batch Loss = 0.2067\n",
            "Iteration 11, Batch 96/100: Batch Loss = 0.2106\n",
            "Iteration 11, Batch 97/100: Batch Loss = 0.1233\n",
            "Iteration 11, Batch 98/100: Batch Loss = 0.1956\n",
            "Iteration 11, Batch 99/100: Batch Loss = 0.0999\n",
            "Iteration 11, Batch 100/100: Batch Loss = 0.0625\n",
            "Iteration 11: Train Loss = 0.1679, Val Loss = 0.1894\n",
            "Iteration 12, Batch 1/100: Batch Loss = 0.2088\n",
            "Iteration 12, Batch 2/100: Batch Loss = 0.1122\n",
            "Iteration 12, Batch 3/100: Batch Loss = 0.1115\n",
            "Iteration 12, Batch 4/100: Batch Loss = 0.1238\n",
            "Iteration 12, Batch 5/100: Batch Loss = 0.1534\n",
            "Iteration 12, Batch 6/100: Batch Loss = 0.1490\n",
            "Iteration 12, Batch 7/100: Batch Loss = 0.1925\n",
            "Iteration 12, Batch 8/100: Batch Loss = 0.1706\n",
            "Iteration 12, Batch 9/100: Batch Loss = 0.1527\n",
            "Iteration 12, Batch 10/100: Batch Loss = 0.1631\n",
            "Iteration 12, Batch 11/100: Batch Loss = 0.1273\n",
            "Iteration 12, Batch 12/100: Batch Loss = 0.2626\n",
            "Iteration 12, Batch 13/100: Batch Loss = 0.0560\n",
            "Iteration 12, Batch 14/100: Batch Loss = 0.0956\n",
            "Iteration 12, Batch 15/100: Batch Loss = 0.2098\n",
            "Iteration 12, Batch 16/100: Batch Loss = 0.2077\n",
            "Iteration 12, Batch 17/100: Batch Loss = 0.1235\n",
            "Iteration 12, Batch 18/100: Batch Loss = 0.2724\n",
            "Iteration 12, Batch 19/100: Batch Loss = 0.1141\n",
            "Iteration 12, Batch 20/100: Batch Loss = 0.2008\n",
            "Iteration 12, Batch 21/100: Batch Loss = 0.2891\n",
            "Iteration 12, Batch 22/100: Batch Loss = 0.1343\n",
            "Iteration 12, Batch 23/100: Batch Loss = 0.1840\n",
            "Iteration 12, Batch 24/100: Batch Loss = 0.1452\n",
            "Iteration 12, Batch 25/100: Batch Loss = 0.1330\n",
            "Iteration 12, Batch 26/100: Batch Loss = 0.2442\n",
            "Iteration 12, Batch 27/100: Batch Loss = 0.1938\n",
            "Iteration 12, Batch 28/100: Batch Loss = 0.2889\n",
            "Iteration 12, Batch 29/100: Batch Loss = 0.2037\n",
            "Iteration 12, Batch 30/100: Batch Loss = 0.2577\n",
            "Iteration 12, Batch 31/100: Batch Loss = 0.1694\n",
            "Iteration 12, Batch 32/100: Batch Loss = 0.1840\n",
            "Iteration 12, Batch 33/100: Batch Loss = 0.1469\n",
            "Iteration 12, Batch 34/100: Batch Loss = 0.0363\n",
            "Iteration 12, Batch 35/100: Batch Loss = 0.0549\n",
            "Iteration 12, Batch 36/100: Batch Loss = 0.2136\n",
            "Iteration 12, Batch 37/100: Batch Loss = 0.2339\n",
            "Iteration 12, Batch 38/100: Batch Loss = 0.0719\n",
            "Iteration 12, Batch 39/100: Batch Loss = 0.3355\n",
            "Iteration 12, Batch 40/100: Batch Loss = 0.2279\n",
            "Iteration 12, Batch 41/100: Batch Loss = 0.1177\n",
            "Iteration 12, Batch 42/100: Batch Loss = 0.1464\n",
            "Iteration 12, Batch 43/100: Batch Loss = 0.1402\n",
            "Iteration 12, Batch 44/100: Batch Loss = 0.1056\n",
            "Iteration 12, Batch 45/100: Batch Loss = 0.1165\n",
            "Iteration 12, Batch 46/100: Batch Loss = 0.1276\n",
            "Iteration 12, Batch 47/100: Batch Loss = 0.2299\n",
            "Iteration 12, Batch 48/100: Batch Loss = 0.1694\n",
            "Iteration 12, Batch 49/100: Batch Loss = 0.1082\n",
            "Iteration 12, Batch 50/100: Batch Loss = 0.2315\n",
            "Iteration 12, Batch 51/100: Batch Loss = 0.0497\n",
            "Iteration 12, Batch 52/100: Batch Loss = 0.1993\n",
            "Iteration 12, Batch 53/100: Batch Loss = 0.1595\n",
            "Iteration 12, Batch 54/100: Batch Loss = 0.1227\n",
            "Iteration 12, Batch 55/100: Batch Loss = 0.1311\n",
            "Iteration 12, Batch 56/100: Batch Loss = 0.2008\n",
            "Iteration 12, Batch 57/100: Batch Loss = 0.1800\n",
            "Iteration 12, Batch 58/100: Batch Loss = 0.1349\n",
            "Iteration 12, Batch 59/100: Batch Loss = 0.2446\n",
            "Iteration 12, Batch 60/100: Batch Loss = 0.2460\n",
            "Iteration 12, Batch 61/100: Batch Loss = 0.2293\n",
            "Iteration 12, Batch 62/100: Batch Loss = 0.3183\n",
            "Iteration 12, Batch 63/100: Batch Loss = 0.3000\n",
            "Iteration 12, Batch 64/100: Batch Loss = 0.1404\n",
            "Iteration 12, Batch 65/100: Batch Loss = 0.1089\n",
            "Iteration 12, Batch 66/100: Batch Loss = 0.1417\n",
            "Iteration 12, Batch 67/100: Batch Loss = 0.1479\n",
            "Iteration 12, Batch 68/100: Batch Loss = 0.0891\n",
            "Iteration 12, Batch 69/100: Batch Loss = 0.2086\n",
            "Iteration 12, Batch 70/100: Batch Loss = 0.1157\n",
            "Iteration 12, Batch 71/100: Batch Loss = 0.2228\n",
            "Iteration 12, Batch 72/100: Batch Loss = 0.1116\n",
            "Iteration 12, Batch 73/100: Batch Loss = 0.1941\n",
            "Iteration 12, Batch 74/100: Batch Loss = 0.1616\n",
            "Iteration 12, Batch 75/100: Batch Loss = 0.1649\n",
            "Iteration 12, Batch 76/100: Batch Loss = 0.1043\n",
            "Iteration 12, Batch 77/100: Batch Loss = 0.1871\n",
            "Iteration 12, Batch 78/100: Batch Loss = 0.1665\n",
            "Iteration 12, Batch 79/100: Batch Loss = 0.1253\n",
            "Iteration 12, Batch 80/100: Batch Loss = 0.2364\n",
            "Iteration 12, Batch 81/100: Batch Loss = 0.1573\n",
            "Iteration 12, Batch 82/100: Batch Loss = 0.2043\n",
            "Iteration 12, Batch 83/100: Batch Loss = 0.2927\n",
            "Iteration 12, Batch 84/100: Batch Loss = 0.1684\n",
            "Iteration 12, Batch 85/100: Batch Loss = 0.0885\n",
            "Iteration 12, Batch 86/100: Batch Loss = 0.2116\n",
            "Iteration 12, Batch 87/100: Batch Loss = 0.0715\n",
            "Iteration 12, Batch 88/100: Batch Loss = 0.1179\n",
            "Iteration 12, Batch 89/100: Batch Loss = 0.1282\n",
            "Iteration 12, Batch 90/100: Batch Loss = 0.0452\n",
            "Iteration 12, Batch 91/100: Batch Loss = 0.1783\n",
            "Iteration 12, Batch 92/100: Batch Loss = 0.1592\n",
            "Iteration 12, Batch 93/100: Batch Loss = 0.1947\n",
            "Iteration 12, Batch 94/100: Batch Loss = 0.1351\n",
            "Iteration 12, Batch 95/100: Batch Loss = 0.1101\n",
            "Iteration 12, Batch 96/100: Batch Loss = 0.2403\n",
            "Iteration 12, Batch 97/100: Batch Loss = 0.0746\n",
            "Iteration 12, Batch 98/100: Batch Loss = 0.0787\n",
            "Iteration 12, Batch 99/100: Batch Loss = 0.1869\n",
            "Iteration 12, Batch 100/100: Batch Loss = 0.0576\n",
            "Iteration 12: Train Loss = 0.1649, Val Loss = 0.1764\n",
            "Iteration 13, Batch 1/100: Batch Loss = 0.1916\n",
            "Iteration 13, Batch 2/100: Batch Loss = 0.0972\n",
            "Iteration 13, Batch 3/100: Batch Loss = 0.2147\n",
            "Iteration 13, Batch 4/100: Batch Loss = 0.2555\n",
            "Iteration 13, Batch 5/100: Batch Loss = 0.0666\n",
            "Iteration 13, Batch 6/100: Batch Loss = 0.1608\n",
            "Iteration 13, Batch 7/100: Batch Loss = 0.0879\n",
            "Iteration 13, Batch 8/100: Batch Loss = 0.0563\n",
            "Iteration 13, Batch 9/100: Batch Loss = 0.1881\n",
            "Iteration 13, Batch 10/100: Batch Loss = 0.0627\n",
            "Iteration 13, Batch 11/100: Batch Loss = 0.0760\n",
            "Iteration 13, Batch 12/100: Batch Loss = 0.1477\n",
            "Iteration 13, Batch 13/100: Batch Loss = 0.1208\n",
            "Iteration 13, Batch 14/100: Batch Loss = 0.0620\n",
            "Iteration 13, Batch 15/100: Batch Loss = 0.0907\n",
            "Iteration 13, Batch 16/100: Batch Loss = 0.0744\n",
            "Iteration 13, Batch 17/100: Batch Loss = 0.0644\n",
            "Iteration 13, Batch 18/100: Batch Loss = 0.2506\n",
            "Iteration 13, Batch 19/100: Batch Loss = 0.1686\n",
            "Iteration 13, Batch 20/100: Batch Loss = 0.0976\n",
            "Iteration 13, Batch 21/100: Batch Loss = 0.0958\n",
            "Iteration 13, Batch 22/100: Batch Loss = 0.3991\n",
            "Iteration 13, Batch 23/100: Batch Loss = 0.2596\n",
            "Iteration 13, Batch 24/100: Batch Loss = 0.1189\n",
            "Iteration 13, Batch 25/100: Batch Loss = 0.0669\n",
            "Iteration 13, Batch 26/100: Batch Loss = 0.0660\n",
            "Iteration 13, Batch 27/100: Batch Loss = 0.2036\n",
            "Iteration 13, Batch 28/100: Batch Loss = 0.2411\n",
            "Iteration 13, Batch 29/100: Batch Loss = 0.0899\n",
            "Iteration 13, Batch 30/100: Batch Loss = 0.1326\n",
            "Iteration 13, Batch 31/100: Batch Loss = 0.2212\n",
            "Iteration 13, Batch 32/100: Batch Loss = 0.1003\n",
            "Iteration 13, Batch 33/100: Batch Loss = 0.0742\n",
            "Iteration 13, Batch 34/100: Batch Loss = 0.0584\n",
            "Iteration 13, Batch 35/100: Batch Loss = 0.1488\n",
            "Iteration 13, Batch 36/100: Batch Loss = 0.1271\n",
            "Iteration 13, Batch 37/100: Batch Loss = 0.0546\n",
            "Iteration 13, Batch 38/100: Batch Loss = 0.2601\n",
            "Iteration 13, Batch 39/100: Batch Loss = 0.1301\n",
            "Iteration 13, Batch 40/100: Batch Loss = 0.1677\n",
            "Iteration 13, Batch 41/100: Batch Loss = 0.1076\n",
            "Iteration 13, Batch 42/100: Batch Loss = 0.1049\n",
            "Iteration 13, Batch 43/100: Batch Loss = 0.2213\n",
            "Iteration 13, Batch 44/100: Batch Loss = 0.1334\n",
            "Iteration 13, Batch 45/100: Batch Loss = 0.2777\n",
            "Iteration 13, Batch 46/100: Batch Loss = 0.2200\n",
            "Iteration 13, Batch 47/100: Batch Loss = 0.1524\n",
            "Iteration 13, Batch 48/100: Batch Loss = 0.3409\n",
            "Iteration 13, Batch 49/100: Batch Loss = 0.1928\n",
            "Iteration 13, Batch 50/100: Batch Loss = 0.2224\n",
            "Iteration 13, Batch 51/100: Batch Loss = 0.0931\n",
            "Iteration 13, Batch 52/100: Batch Loss = 0.1579\n",
            "Iteration 13, Batch 53/100: Batch Loss = 0.1925\n",
            "Iteration 13, Batch 54/100: Batch Loss = 0.2219\n",
            "Iteration 13, Batch 55/100: Batch Loss = 0.1342\n",
            "Iteration 13, Batch 56/100: Batch Loss = 0.0676\n",
            "Iteration 13, Batch 57/100: Batch Loss = 0.0825\n",
            "Iteration 13, Batch 58/100: Batch Loss = 0.2587\n",
            "Iteration 13, Batch 59/100: Batch Loss = 0.2440\n",
            "Iteration 13, Batch 60/100: Batch Loss = 0.7043\n",
            "Iteration 13, Batch 61/100: Batch Loss = 0.0752\n",
            "Iteration 13, Batch 62/100: Batch Loss = 0.2960\n",
            "Iteration 13, Batch 63/100: Batch Loss = 0.1111\n",
            "Iteration 13, Batch 64/100: Batch Loss = 0.1129\n",
            "Iteration 13, Batch 65/100: Batch Loss = 0.1522\n",
            "Iteration 13, Batch 66/100: Batch Loss = 0.1069\n",
            "Iteration 13, Batch 67/100: Batch Loss = 0.1465\n",
            "Iteration 13, Batch 68/100: Batch Loss = 0.2936\n",
            "Iteration 13, Batch 69/100: Batch Loss = 0.1332\n",
            "Iteration 13, Batch 70/100: Batch Loss = 0.1597\n",
            "Iteration 13, Batch 71/100: Batch Loss = 0.2866\n",
            "Iteration 13, Batch 72/100: Batch Loss = 0.1939\n",
            "Iteration 13, Batch 73/100: Batch Loss = 0.1591\n",
            "Iteration 13, Batch 74/100: Batch Loss = 0.1536\n",
            "Iteration 13, Batch 75/100: Batch Loss = 0.1596\n",
            "Iteration 13, Batch 76/100: Batch Loss = 0.2043\n",
            "Iteration 13, Batch 77/100: Batch Loss = 0.0794\n",
            "Iteration 13, Batch 78/100: Batch Loss = 0.1308\n",
            "Iteration 13, Batch 79/100: Batch Loss = 0.2372\n",
            "Iteration 13, Batch 80/100: Batch Loss = 0.1790\n",
            "Iteration 13, Batch 81/100: Batch Loss = 0.1897\n",
            "Iteration 13, Batch 82/100: Batch Loss = 0.2017\n",
            "Iteration 13, Batch 83/100: Batch Loss = 0.2446\n",
            "Iteration 13, Batch 84/100: Batch Loss = 0.2716\n",
            "Iteration 13, Batch 85/100: Batch Loss = 0.1731\n",
            "Iteration 13, Batch 86/100: Batch Loss = 0.0344\n",
            "Iteration 13, Batch 87/100: Batch Loss = 0.1504\n",
            "Iteration 13, Batch 88/100: Batch Loss = 0.0827\n",
            "Iteration 13, Batch 89/100: Batch Loss = 0.1676\n",
            "Iteration 13, Batch 90/100: Batch Loss = 0.2474\n",
            "Iteration 13, Batch 91/100: Batch Loss = 0.1553\n",
            "Iteration 13, Batch 92/100: Batch Loss = 0.1170\n",
            "Iteration 13, Batch 93/100: Batch Loss = 0.0734\n",
            "Iteration 13, Batch 94/100: Batch Loss = 0.0555\n",
            "Iteration 13, Batch 95/100: Batch Loss = 0.1803\n",
            "Iteration 13, Batch 96/100: Batch Loss = 0.1492\n",
            "Iteration 13, Batch 97/100: Batch Loss = 0.1445\n",
            "Iteration 13, Batch 98/100: Batch Loss = 0.1250\n",
            "Iteration 13, Batch 99/100: Batch Loss = 0.1365\n",
            "Iteration 13, Batch 100/100: Batch Loss = 0.1600\n",
            "Iteration 13: Train Loss = 0.1611, Val Loss = 0.1816\n",
            "Iteration 14, Batch 1/100: Batch Loss = 0.0678\n",
            "Iteration 14, Batch 2/100: Batch Loss = 0.2201\n",
            "Iteration 14, Batch 3/100: Batch Loss = 0.0692\n",
            "Iteration 14, Batch 4/100: Batch Loss = 0.0909\n",
            "Iteration 14, Batch 5/100: Batch Loss = 0.1783\n",
            "Iteration 14, Batch 6/100: Batch Loss = 0.2828\n",
            "Iteration 14, Batch 7/100: Batch Loss = 0.3425\n",
            "Iteration 14, Batch 8/100: Batch Loss = 0.2165\n",
            "Iteration 14, Batch 9/100: Batch Loss = 0.1633\n",
            "Iteration 14, Batch 10/100: Batch Loss = 0.1389\n",
            "Iteration 14, Batch 11/100: Batch Loss = 0.2068\n",
            "Iteration 14, Batch 12/100: Batch Loss = 0.0692\n",
            "Iteration 14, Batch 13/100: Batch Loss = 0.0997\n",
            "Iteration 14, Batch 14/100: Batch Loss = 0.2025\n",
            "Iteration 14, Batch 15/100: Batch Loss = 0.1900\n",
            "Iteration 14, Batch 16/100: Batch Loss = 0.0831\n",
            "Iteration 14, Batch 17/100: Batch Loss = 0.1781\n",
            "Iteration 14, Batch 18/100: Batch Loss = 0.0911\n",
            "Iteration 14, Batch 19/100: Batch Loss = 0.1561\n",
            "Iteration 14, Batch 20/100: Batch Loss = 0.1507\n",
            "Iteration 14, Batch 21/100: Batch Loss = 0.0909\n",
            "Iteration 14, Batch 22/100: Batch Loss = 0.3151\n",
            "Iteration 14, Batch 23/100: Batch Loss = 0.1507\n",
            "Iteration 14, Batch 24/100: Batch Loss = 0.1257\n",
            "Iteration 14, Batch 25/100: Batch Loss = 0.1390\n",
            "Iteration 14, Batch 26/100: Batch Loss = 0.0892\n",
            "Iteration 14, Batch 27/100: Batch Loss = 0.1545\n",
            "Iteration 14, Batch 28/100: Batch Loss = 0.1532\n",
            "Iteration 14, Batch 29/100: Batch Loss = 0.2041\n",
            "Iteration 14, Batch 30/100: Batch Loss = 0.0486\n",
            "Iteration 14, Batch 31/100: Batch Loss = 0.1350\n",
            "Iteration 14, Batch 32/100: Batch Loss = 0.1231\n",
            "Iteration 14, Batch 33/100: Batch Loss = 0.1124\n",
            "Iteration 14, Batch 34/100: Batch Loss = 0.2412\n",
            "Iteration 14, Batch 35/100: Batch Loss = 0.1915\n",
            "Iteration 14, Batch 36/100: Batch Loss = 0.3632\n",
            "Iteration 14, Batch 37/100: Batch Loss = 0.1769\n",
            "Iteration 14, Batch 38/100: Batch Loss = 0.2202\n",
            "Iteration 14, Batch 39/100: Batch Loss = 0.2482\n",
            "Iteration 14, Batch 40/100: Batch Loss = 0.1033\n",
            "Iteration 14, Batch 41/100: Batch Loss = 0.0923\n",
            "Iteration 14, Batch 42/100: Batch Loss = 0.1705\n",
            "Iteration 14, Batch 43/100: Batch Loss = 0.0919\n",
            "Iteration 14, Batch 44/100: Batch Loss = 0.1147\n",
            "Iteration 14, Batch 45/100: Batch Loss = 0.1289\n",
            "Iteration 14, Batch 46/100: Batch Loss = 0.2370\n",
            "Iteration 14, Batch 47/100: Batch Loss = 0.0806\n",
            "Iteration 14, Batch 48/100: Batch Loss = 0.1769\n",
            "Iteration 14, Batch 49/100: Batch Loss = 0.3532\n",
            "Iteration 14, Batch 50/100: Batch Loss = 0.1276\n",
            "Iteration 14, Batch 51/100: Batch Loss = 0.2492\n",
            "Iteration 14, Batch 52/100: Batch Loss = 0.1311\n",
            "Iteration 14, Batch 53/100: Batch Loss = 0.2102\n",
            "Iteration 14, Batch 54/100: Batch Loss = 0.1341\n",
            "Iteration 14, Batch 55/100: Batch Loss = 0.1856\n",
            "Iteration 14, Batch 56/100: Batch Loss = 0.1665\n",
            "Iteration 14, Batch 57/100: Batch Loss = 0.1521\n",
            "Iteration 14, Batch 58/100: Batch Loss = 0.0871\n",
            "Iteration 14, Batch 59/100: Batch Loss = 0.1103\n",
            "Iteration 14, Batch 60/100: Batch Loss = 0.1226\n",
            "Iteration 14, Batch 61/100: Batch Loss = 0.1299\n",
            "Iteration 14, Batch 62/100: Batch Loss = 0.0826\n",
            "Iteration 14, Batch 63/100: Batch Loss = 0.1381\n",
            "Iteration 14, Batch 64/100: Batch Loss = 0.1734\n",
            "Iteration 14, Batch 65/100: Batch Loss = 0.1079\n",
            "Iteration 14, Batch 66/100: Batch Loss = 0.2285\n",
            "Iteration 14, Batch 67/100: Batch Loss = 0.0844\n",
            "Iteration 14, Batch 68/100: Batch Loss = 0.1203\n",
            "Iteration 14, Batch 69/100: Batch Loss = 0.3795\n",
            "Iteration 14, Batch 70/100: Batch Loss = 0.2410\n",
            "Iteration 14, Batch 71/100: Batch Loss = 0.0819\n",
            "Iteration 14, Batch 72/100: Batch Loss = 0.1621\n",
            "Iteration 14, Batch 73/100: Batch Loss = 0.1238\n",
            "Iteration 14, Batch 74/100: Batch Loss = 0.0966\n",
            "Iteration 14, Batch 75/100: Batch Loss = 0.2088\n",
            "Iteration 14, Batch 76/100: Batch Loss = 0.0900\n",
            "Iteration 14, Batch 77/100: Batch Loss = 0.1652\n",
            "Iteration 14, Batch 78/100: Batch Loss = 0.1099\n",
            "Iteration 14, Batch 79/100: Batch Loss = 0.2367\n",
            "Iteration 14, Batch 80/100: Batch Loss = 0.1252\n",
            "Iteration 14, Batch 81/100: Batch Loss = 0.0726\n",
            "Iteration 14, Batch 82/100: Batch Loss = 0.1868\n",
            "Iteration 14, Batch 83/100: Batch Loss = 0.1421\n",
            "Iteration 14, Batch 84/100: Batch Loss = 0.1156\n",
            "Iteration 14, Batch 85/100: Batch Loss = 0.0797\n",
            "Iteration 14, Batch 86/100: Batch Loss = 0.2731\n",
            "Iteration 14, Batch 87/100: Batch Loss = 0.1009\n",
            "Iteration 14, Batch 88/100: Batch Loss = 0.0711\n",
            "Iteration 14, Batch 89/100: Batch Loss = 0.2125\n",
            "Iteration 14, Batch 90/100: Batch Loss = 0.1489\n",
            "Iteration 14, Batch 91/100: Batch Loss = 0.1288\n",
            "Iteration 14, Batch 92/100: Batch Loss = 0.1125\n",
            "Iteration 14, Batch 93/100: Batch Loss = 0.1744\n",
            "Iteration 14, Batch 94/100: Batch Loss = 0.1765\n",
            "Iteration 14, Batch 95/100: Batch Loss = 0.0555\n",
            "Iteration 14, Batch 96/100: Batch Loss = 0.2032\n",
            "Iteration 14, Batch 97/100: Batch Loss = 0.1181\n",
            "Iteration 14, Batch 98/100: Batch Loss = 0.1228\n",
            "Iteration 14, Batch 99/100: Batch Loss = 0.0814\n",
            "Iteration 14, Batch 100/100: Batch Loss = 0.1437\n",
            "Iteration 14: Train Loss = 0.1551, Val Loss = 0.1716\n",
            "Iteration 15, Batch 1/100: Batch Loss = 0.0691\n",
            "Iteration 15, Batch 2/100: Batch Loss = 0.0874\n",
            "Iteration 15, Batch 3/100: Batch Loss = 0.1392\n",
            "Iteration 15, Batch 4/100: Batch Loss = 0.0323\n",
            "Iteration 15, Batch 5/100: Batch Loss = 0.1521\n",
            "Iteration 15, Batch 6/100: Batch Loss = 0.0856\n",
            "Iteration 15, Batch 7/100: Batch Loss = 0.0644\n",
            "Iteration 15, Batch 8/100: Batch Loss = 0.2525\n",
            "Iteration 15, Batch 9/100: Batch Loss = 0.1299\n",
            "Iteration 15, Batch 10/100: Batch Loss = 0.3394\n",
            "Iteration 15, Batch 11/100: Batch Loss = 0.1805\n",
            "Iteration 15, Batch 12/100: Batch Loss = 0.1144\n",
            "Iteration 15, Batch 13/100: Batch Loss = 0.0703\n",
            "Iteration 15, Batch 14/100: Batch Loss = 0.1324\n",
            "Iteration 15, Batch 15/100: Batch Loss = 0.2434\n",
            "Iteration 15, Batch 16/100: Batch Loss = 0.2432\n",
            "Iteration 15, Batch 17/100: Batch Loss = 0.1551\n",
            "Iteration 15, Batch 18/100: Batch Loss = 0.2562\n",
            "Iteration 15, Batch 19/100: Batch Loss = 0.1337\n",
            "Iteration 15, Batch 20/100: Batch Loss = 0.1242\n",
            "Iteration 15, Batch 21/100: Batch Loss = 0.1082\n",
            "Iteration 15, Batch 22/100: Batch Loss = 0.0549\n",
            "Iteration 15, Batch 23/100: Batch Loss = 0.2481\n",
            "Iteration 15, Batch 24/100: Batch Loss = 0.1663\n",
            "Iteration 15, Batch 25/100: Batch Loss = 0.2037\n",
            "Iteration 15, Batch 26/100: Batch Loss = 0.1697\n",
            "Iteration 15, Batch 27/100: Batch Loss = 0.0911\n",
            "Iteration 15, Batch 28/100: Batch Loss = 0.1810\n",
            "Iteration 15, Batch 29/100: Batch Loss = 0.1234\n",
            "Iteration 15, Batch 30/100: Batch Loss = 0.0523\n",
            "Iteration 15, Batch 31/100: Batch Loss = 0.0898\n",
            "Iteration 15, Batch 32/100: Batch Loss = 0.1644\n",
            "Iteration 15, Batch 33/100: Batch Loss = 0.1371\n",
            "Iteration 15, Batch 34/100: Batch Loss = 0.2425\n",
            "Iteration 15, Batch 35/100: Batch Loss = 0.1181\n",
            "Iteration 15, Batch 36/100: Batch Loss = 0.0822\n",
            "Iteration 15, Batch 37/100: Batch Loss = 0.2073\n",
            "Iteration 15, Batch 38/100: Batch Loss = 0.0714\n",
            "Iteration 15, Batch 39/100: Batch Loss = 0.3132\n",
            "Iteration 15, Batch 40/100: Batch Loss = 0.0795\n",
            "Iteration 15, Batch 41/100: Batch Loss = 0.1485\n",
            "Iteration 15, Batch 42/100: Batch Loss = 0.3260\n",
            "Iteration 15, Batch 43/100: Batch Loss = 0.2569\n",
            "Iteration 15, Batch 44/100: Batch Loss = 0.0551\n",
            "Iteration 15, Batch 45/100: Batch Loss = 0.0378\n",
            "Iteration 15, Batch 46/100: Batch Loss = 0.0836\n",
            "Iteration 15, Batch 47/100: Batch Loss = 0.1481\n",
            "Iteration 15, Batch 48/100: Batch Loss = 0.0553\n",
            "Iteration 15, Batch 49/100: Batch Loss = 0.0857\n",
            "Iteration 15, Batch 50/100: Batch Loss = 0.0728\n",
            "Iteration 15, Batch 51/100: Batch Loss = 0.1849\n",
            "Iteration 15, Batch 52/100: Batch Loss = 0.0722\n",
            "Iteration 15, Batch 53/100: Batch Loss = 0.2012\n",
            "Iteration 15, Batch 54/100: Batch Loss = 0.1451\n",
            "Iteration 15, Batch 55/100: Batch Loss = 0.0847\n",
            "Iteration 15, Batch 56/100: Batch Loss = 0.2168\n",
            "Iteration 15, Batch 57/100: Batch Loss = 0.3460\n",
            "Iteration 15, Batch 58/100: Batch Loss = 0.0904\n",
            "Iteration 15, Batch 59/100: Batch Loss = 0.0841\n",
            "Iteration 15, Batch 60/100: Batch Loss = 0.1820\n",
            "Iteration 15, Batch 61/100: Batch Loss = 0.1133\n",
            "Iteration 15, Batch 62/100: Batch Loss = 0.0692\n",
            "Iteration 15, Batch 63/100: Batch Loss = 0.3145\n",
            "Iteration 15, Batch 64/100: Batch Loss = 0.1689\n",
            "Iteration 15, Batch 65/100: Batch Loss = 0.0517\n",
            "Iteration 15, Batch 66/100: Batch Loss = 0.0729\n",
            "Iteration 15, Batch 67/100: Batch Loss = 0.2568\n",
            "Iteration 15, Batch 68/100: Batch Loss = 0.1267\n",
            "Iteration 15, Batch 69/100: Batch Loss = 0.1027\n",
            "Iteration 15, Batch 70/100: Batch Loss = 0.2701\n",
            "Iteration 15, Batch 71/100: Batch Loss = 0.1993\n",
            "Iteration 15, Batch 72/100: Batch Loss = 0.1197\n",
            "Iteration 15, Batch 73/100: Batch Loss = 0.1016\n",
            "Iteration 15, Batch 74/100: Batch Loss = 0.1236\n",
            "Iteration 15, Batch 75/100: Batch Loss = 0.2734\n",
            "Iteration 15, Batch 76/100: Batch Loss = 0.1977\n",
            "Iteration 15, Batch 77/100: Batch Loss = 0.1159\n",
            "Iteration 15, Batch 78/100: Batch Loss = 0.3201\n",
            "Iteration 15, Batch 79/100: Batch Loss = 0.1727\n",
            "Iteration 15, Batch 80/100: Batch Loss = 0.1775\n",
            "Iteration 15, Batch 81/100: Batch Loss = 0.1463\n",
            "Iteration 15, Batch 82/100: Batch Loss = 0.0718\n",
            "Iteration 15, Batch 83/100: Batch Loss = 0.1315\n",
            "Iteration 15, Batch 84/100: Batch Loss = 0.1808\n",
            "Iteration 15, Batch 85/100: Batch Loss = 0.0963\n",
            "Iteration 15, Batch 86/100: Batch Loss = 0.1581\n",
            "Iteration 15, Batch 87/100: Batch Loss = 0.1639\n",
            "Iteration 15, Batch 88/100: Batch Loss = 0.0749\n",
            "Iteration 15, Batch 89/100: Batch Loss = 0.3135\n",
            "Iteration 15, Batch 90/100: Batch Loss = 0.2016\n",
            "Iteration 15, Batch 91/100: Batch Loss = 0.0733\n",
            "Iteration 15, Batch 92/100: Batch Loss = 0.3230\n",
            "Iteration 15, Batch 93/100: Batch Loss = 0.2013\n",
            "Iteration 15, Batch 94/100: Batch Loss = 0.1483\n",
            "Iteration 15, Batch 95/100: Batch Loss = 0.0656\n",
            "Iteration 15, Batch 96/100: Batch Loss = 0.0702\n",
            "Iteration 15, Batch 97/100: Batch Loss = 0.0524\n",
            "Iteration 15, Batch 98/100: Batch Loss = 0.0545\n",
            "Iteration 15, Batch 99/100: Batch Loss = 0.1575\n",
            "Iteration 15, Batch 100/100: Batch Loss = 0.0874\n",
            "Iteration 15: Train Loss = 0.1491, Val Loss = 0.1792\n",
            "Iteration 16, Batch 1/100: Batch Loss = 0.2643\n",
            "Iteration 16, Batch 2/100: Batch Loss = 0.1527\n",
            "Iteration 16, Batch 3/100: Batch Loss = 0.2037\n",
            "Iteration 16, Batch 4/100: Batch Loss = 0.2142\n",
            "Iteration 16, Batch 5/100: Batch Loss = 0.2471\n",
            "Iteration 16, Batch 6/100: Batch Loss = 0.1751\n",
            "Iteration 16, Batch 7/100: Batch Loss = 0.0575\n",
            "Iteration 16, Batch 8/100: Batch Loss = 0.1898\n",
            "Iteration 16, Batch 9/100: Batch Loss = 0.1094\n",
            "Iteration 16, Batch 10/100: Batch Loss = 0.1914\n",
            "Iteration 16, Batch 11/100: Batch Loss = 0.1334\n",
            "Iteration 16, Batch 12/100: Batch Loss = 0.2367\n",
            "Iteration 16, Batch 13/100: Batch Loss = 0.1220\n",
            "Iteration 16, Batch 14/100: Batch Loss = 0.1264\n",
            "Iteration 16, Batch 15/100: Batch Loss = 0.1055\n",
            "Iteration 16, Batch 16/100: Batch Loss = 0.0786\n",
            "Iteration 16, Batch 17/100: Batch Loss = 0.1347\n",
            "Iteration 16, Batch 18/100: Batch Loss = 0.0408\n",
            "Iteration 16, Batch 19/100: Batch Loss = 0.1174\n",
            "Iteration 16, Batch 20/100: Batch Loss = 0.0969\n",
            "Iteration 16, Batch 21/100: Batch Loss = 0.0237\n",
            "Iteration 16, Batch 22/100: Batch Loss = 0.1385\n",
            "Iteration 16, Batch 23/100: Batch Loss = 0.0343\n",
            "Iteration 16, Batch 24/100: Batch Loss = 0.0457\n",
            "Iteration 16, Batch 25/100: Batch Loss = 0.1894\n",
            "Iteration 16, Batch 26/100: Batch Loss = 0.1164\n",
            "Iteration 16, Batch 27/100: Batch Loss = 0.2563\n",
            "Iteration 16, Batch 28/100: Batch Loss = 0.1305\n",
            "Iteration 16, Batch 29/100: Batch Loss = 0.1029\n",
            "Iteration 16, Batch 30/100: Batch Loss = 0.2430\n",
            "Iteration 16, Batch 31/100: Batch Loss = 0.1820\n",
            "Iteration 16, Batch 32/100: Batch Loss = 0.1618\n",
            "Iteration 16, Batch 33/100: Batch Loss = 0.1701\n",
            "Iteration 16, Batch 34/100: Batch Loss = 0.0704\n",
            "Iteration 16, Batch 35/100: Batch Loss = 0.2723\n",
            "Iteration 16, Batch 36/100: Batch Loss = 0.1314\n",
            "Iteration 16, Batch 37/100: Batch Loss = 0.1257\n",
            "Iteration 16, Batch 38/100: Batch Loss = 0.1974\n",
            "Iteration 16, Batch 39/100: Batch Loss = 0.0697\n",
            "Iteration 16, Batch 40/100: Batch Loss = 0.2286\n",
            "Iteration 16, Batch 41/100: Batch Loss = 0.1331\n",
            "Iteration 16, Batch 42/100: Batch Loss = 0.1588\n",
            "Iteration 16, Batch 43/100: Batch Loss = 0.1394\n",
            "Iteration 16, Batch 44/100: Batch Loss = 0.2365\n",
            "Iteration 16, Batch 45/100: Batch Loss = 0.1188\n",
            "Iteration 16, Batch 46/100: Batch Loss = 0.1516\n",
            "Iteration 16, Batch 47/100: Batch Loss = 0.0937\n",
            "Iteration 16, Batch 48/100: Batch Loss = 0.0563\n",
            "Iteration 16, Batch 49/100: Batch Loss = 0.1048\n",
            "Iteration 16, Batch 50/100: Batch Loss = 0.0567\n",
            "Iteration 16, Batch 51/100: Batch Loss = 0.2387\n",
            "Iteration 16, Batch 52/100: Batch Loss = 0.0495\n",
            "Iteration 16, Batch 53/100: Batch Loss = 0.2208\n",
            "Iteration 16, Batch 54/100: Batch Loss = 0.0881\n",
            "Iteration 16, Batch 55/100: Batch Loss = 0.0710\n",
            "Iteration 16, Batch 56/100: Batch Loss = 0.1664\n",
            "Iteration 16, Batch 57/100: Batch Loss = 0.0623\n",
            "Iteration 16, Batch 58/100: Batch Loss = 0.1359\n",
            "Iteration 16, Batch 59/100: Batch Loss = 0.0444\n",
            "Iteration 16, Batch 60/100: Batch Loss = 0.1806\n",
            "Iteration 16, Batch 61/100: Batch Loss = 0.3029\n",
            "Iteration 16, Batch 62/100: Batch Loss = 0.1045\n",
            "Iteration 16, Batch 63/100: Batch Loss = 0.0426\n",
            "Iteration 16, Batch 64/100: Batch Loss = 0.3148\n",
            "Iteration 16, Batch 65/100: Batch Loss = 0.1781\n",
            "Iteration 16, Batch 66/100: Batch Loss = 0.3206\n",
            "Iteration 16, Batch 67/100: Batch Loss = 0.1497\n",
            "Iteration 16, Batch 68/100: Batch Loss = 0.0464\n",
            "Iteration 16, Batch 69/100: Batch Loss = 0.0924\n",
            "Iteration 16, Batch 70/100: Batch Loss = 0.2203\n",
            "Iteration 16, Batch 71/100: Batch Loss = 0.2020\n",
            "Iteration 16, Batch 72/100: Batch Loss = 0.1630\n",
            "Iteration 16, Batch 73/100: Batch Loss = 0.1684\n",
            "Iteration 16, Batch 74/100: Batch Loss = 0.1180\n",
            "Iteration 16, Batch 75/100: Batch Loss = 0.1263\n",
            "Iteration 16, Batch 76/100: Batch Loss = 0.1206\n",
            "Iteration 16, Batch 77/100: Batch Loss = 0.2440\n",
            "Iteration 16, Batch 78/100: Batch Loss = 0.1848\n",
            "Iteration 16, Batch 79/100: Batch Loss = 0.1920\n",
            "Iteration 16, Batch 80/100: Batch Loss = 0.0504\n",
            "Iteration 16, Batch 81/100: Batch Loss = 0.2031\n",
            "Iteration 16, Batch 82/100: Batch Loss = 0.1344\n",
            "Iteration 16, Batch 83/100: Batch Loss = 0.0812\n",
            "Iteration 16, Batch 84/100: Batch Loss = 0.3514\n",
            "Iteration 16, Batch 85/100: Batch Loss = 0.1493\n",
            "Iteration 16, Batch 86/100: Batch Loss = 0.1428\n",
            "Iteration 16, Batch 87/100: Batch Loss = 0.1475\n",
            "Iteration 16, Batch 88/100: Batch Loss = 0.0458\n",
            "Iteration 16, Batch 89/100: Batch Loss = 0.0519\n",
            "Iteration 16, Batch 90/100: Batch Loss = 0.2419\n",
            "Iteration 16, Batch 91/100: Batch Loss = 0.1591\n",
            "Iteration 16, Batch 92/100: Batch Loss = 0.0679\n",
            "Iteration 16, Batch 93/100: Batch Loss = 0.1249\n",
            "Iteration 16, Batch 94/100: Batch Loss = 0.1418\n",
            "Iteration 16, Batch 95/100: Batch Loss = 0.1956\n",
            "Iteration 16, Batch 96/100: Batch Loss = 0.0572\n",
            "Iteration 16, Batch 97/100: Batch Loss = 0.1577\n",
            "Iteration 16, Batch 98/100: Batch Loss = 0.2222\n",
            "Iteration 16, Batch 99/100: Batch Loss = 0.2678\n",
            "Iteration 16, Batch 100/100: Batch Loss = 0.0866\n",
            "Iteration 16: Train Loss = 0.1477, Val Loss = 0.1593\n",
            "Iteration 17, Batch 1/100: Batch Loss = 0.0758\n",
            "Iteration 17, Batch 2/100: Batch Loss = 0.1207\n",
            "Iteration 17, Batch 3/100: Batch Loss = 0.1166\n",
            "Iteration 17, Batch 4/100: Batch Loss = 0.1041\n",
            "Iteration 17, Batch 5/100: Batch Loss = 0.1730\n",
            "Iteration 17, Batch 6/100: Batch Loss = 0.0981\n",
            "Iteration 17, Batch 7/100: Batch Loss = 0.0657\n",
            "Iteration 17, Batch 8/100: Batch Loss = 0.1092\n",
            "Iteration 17, Batch 9/100: Batch Loss = 0.1496\n",
            "Iteration 17, Batch 10/100: Batch Loss = 0.0948\n",
            "Iteration 17, Batch 11/100: Batch Loss = 0.0863\n",
            "Iteration 17, Batch 12/100: Batch Loss = 0.1144\n",
            "Iteration 17, Batch 13/100: Batch Loss = 0.0735\n",
            "Iteration 17, Batch 14/100: Batch Loss = 0.3018\n",
            "Iteration 17, Batch 15/100: Batch Loss = 0.1527\n",
            "Iteration 17, Batch 16/100: Batch Loss = 0.0896\n",
            "Iteration 17, Batch 17/100: Batch Loss = 0.1282\n",
            "Iteration 17, Batch 18/100: Batch Loss = 0.2766\n",
            "Iteration 17, Batch 19/100: Batch Loss = 0.0862\n",
            "Iteration 17, Batch 20/100: Batch Loss = 0.1586\n",
            "Iteration 17, Batch 21/100: Batch Loss = 0.2107\n",
            "Iteration 17, Batch 22/100: Batch Loss = 0.0561\n",
            "Iteration 17, Batch 23/100: Batch Loss = 0.1555\n",
            "Iteration 17, Batch 24/100: Batch Loss = 0.2887\n",
            "Iteration 17, Batch 25/100: Batch Loss = 0.0306\n",
            "Iteration 17, Batch 26/100: Batch Loss = 0.2504\n",
            "Iteration 17, Batch 27/100: Batch Loss = 0.1357\n",
            "Iteration 17, Batch 28/100: Batch Loss = 0.1933\n",
            "Iteration 17, Batch 29/100: Batch Loss = 0.0282\n",
            "Iteration 17, Batch 30/100: Batch Loss = 0.0617\n",
            "Iteration 17, Batch 31/100: Batch Loss = 0.1066\n",
            "Iteration 17, Batch 32/100: Batch Loss = 0.1764\n",
            "Iteration 17, Batch 33/100: Batch Loss = 0.1249\n",
            "Iteration 17, Batch 34/100: Batch Loss = 0.0792\n",
            "Iteration 17, Batch 35/100: Batch Loss = 0.0823\n",
            "Iteration 17, Batch 36/100: Batch Loss = 0.0915\n",
            "Iteration 17, Batch 37/100: Batch Loss = 0.1154\n",
            "Iteration 17, Batch 38/100: Batch Loss = 0.1032\n",
            "Iteration 17, Batch 39/100: Batch Loss = 0.0306\n",
            "Iteration 17, Batch 40/100: Batch Loss = 0.0564\n",
            "Iteration 17, Batch 41/100: Batch Loss = 0.1495\n",
            "Iteration 17, Batch 42/100: Batch Loss = 0.0546\n",
            "Iteration 17, Batch 43/100: Batch Loss = 0.3105\n",
            "Iteration 17, Batch 44/100: Batch Loss = 0.1072\n",
            "Iteration 17, Batch 45/100: Batch Loss = 0.4048\n",
            "Iteration 17, Batch 46/100: Batch Loss = 0.1457\n",
            "Iteration 17, Batch 47/100: Batch Loss = 0.1087\n",
            "Iteration 17, Batch 48/100: Batch Loss = 0.1270\n",
            "Iteration 17, Batch 49/100: Batch Loss = 0.0724\n",
            "Iteration 17, Batch 50/100: Batch Loss = 0.0913\n",
            "Iteration 17, Batch 51/100: Batch Loss = 0.1020\n",
            "Iteration 17, Batch 52/100: Batch Loss = 0.2146\n",
            "Iteration 17, Batch 53/100: Batch Loss = 0.2417\n",
            "Iteration 17, Batch 54/100: Batch Loss = 0.1435\n",
            "Iteration 17, Batch 55/100: Batch Loss = 0.3453\n",
            "Iteration 17, Batch 56/100: Batch Loss = 0.0700\n",
            "Iteration 17, Batch 57/100: Batch Loss = 0.1569\n",
            "Iteration 17, Batch 58/100: Batch Loss = 0.1116\n",
            "Iteration 17, Batch 59/100: Batch Loss = 0.0427\n",
            "Iteration 17, Batch 60/100: Batch Loss = 0.0866\n",
            "Iteration 17, Batch 61/100: Batch Loss = 0.1909\n",
            "Iteration 17, Batch 62/100: Batch Loss = 0.1156\n",
            "Iteration 17, Batch 63/100: Batch Loss = 0.1457\n",
            "Iteration 17, Batch 64/100: Batch Loss = 0.3447\n",
            "Iteration 17, Batch 65/100: Batch Loss = 0.1310\n",
            "Iteration 17, Batch 66/100: Batch Loss = 0.1328\n",
            "Iteration 17, Batch 67/100: Batch Loss = 0.0971\n",
            "Iteration 17, Batch 68/100: Batch Loss = 0.1911\n",
            "Iteration 17, Batch 69/100: Batch Loss = 0.1345\n",
            "Iteration 17, Batch 70/100: Batch Loss = 0.1728\n",
            "Iteration 17, Batch 71/100: Batch Loss = 0.0574\n",
            "Iteration 17, Batch 72/100: Batch Loss = 0.1077\n",
            "Iteration 17, Batch 73/100: Batch Loss = 0.1726\n",
            "Iteration 17, Batch 74/100: Batch Loss = 0.1948\n",
            "Iteration 17, Batch 75/100: Batch Loss = 0.3575\n",
            "Iteration 17, Batch 76/100: Batch Loss = 0.0768\n",
            "Iteration 17, Batch 77/100: Batch Loss = 0.0495\n",
            "Iteration 17, Batch 78/100: Batch Loss = 0.1091\n",
            "Iteration 17, Batch 79/100: Batch Loss = 0.1832\n",
            "Iteration 17, Batch 80/100: Batch Loss = 0.1409\n",
            "Iteration 17, Batch 81/100: Batch Loss = 0.1492\n",
            "Iteration 17, Batch 82/100: Batch Loss = 0.2096\n",
            "Iteration 17, Batch 83/100: Batch Loss = 0.0749\n",
            "Iteration 17, Batch 84/100: Batch Loss = 0.1818\n",
            "Iteration 17, Batch 85/100: Batch Loss = 0.0840\n",
            "Iteration 17, Batch 86/100: Batch Loss = 0.0715\n",
            "Iteration 17, Batch 87/100: Batch Loss = 0.1004\n",
            "Iteration 17, Batch 88/100: Batch Loss = 0.1421\n",
            "Iteration 17, Batch 89/100: Batch Loss = 0.4798\n",
            "Iteration 17, Batch 90/100: Batch Loss = 0.2823\n",
            "Iteration 17, Batch 91/100: Batch Loss = 0.0909\n",
            "Iteration 17, Batch 92/100: Batch Loss = 0.1409\n",
            "Iteration 17, Batch 93/100: Batch Loss = 0.4826\n",
            "Iteration 17, Batch 94/100: Batch Loss = 0.1477\n",
            "Iteration 17, Batch 95/100: Batch Loss = 0.2564\n",
            "Iteration 17, Batch 96/100: Batch Loss = 0.0889\n",
            "Iteration 17, Batch 97/100: Batch Loss = 0.1119\n",
            "Iteration 17, Batch 98/100: Batch Loss = 0.1057\n",
            "Iteration 17, Batch 99/100: Batch Loss = 0.1354\n",
            "Iteration 17, Batch 100/100: Batch Loss = 0.0718\n",
            "Iteration 17: Train Loss = 0.1455, Val Loss = 0.1608\n",
            "Iteration 18, Batch 1/100: Batch Loss = 0.0932\n",
            "Iteration 18, Batch 2/100: Batch Loss = 0.2056\n",
            "Iteration 18, Batch 3/100: Batch Loss = 0.1058\n",
            "Iteration 18, Batch 4/100: Batch Loss = 0.0283\n",
            "Iteration 18, Batch 5/100: Batch Loss = 0.0962\n",
            "Iteration 18, Batch 6/100: Batch Loss = 0.2809\n",
            "Iteration 18, Batch 7/100: Batch Loss = 0.0559\n",
            "Iteration 18, Batch 8/100: Batch Loss = 0.0939\n",
            "Iteration 18, Batch 9/100: Batch Loss = 0.1131\n",
            "Iteration 18, Batch 10/100: Batch Loss = 0.0875\n",
            "Iteration 18, Batch 11/100: Batch Loss = 0.0822\n",
            "Iteration 18, Batch 12/100: Batch Loss = 0.0777\n",
            "Iteration 18, Batch 13/100: Batch Loss = 0.2915\n",
            "Iteration 18, Batch 14/100: Batch Loss = 0.1736\n",
            "Iteration 18, Batch 15/100: Batch Loss = 0.1038\n",
            "Iteration 18, Batch 16/100: Batch Loss = 0.0496\n",
            "Iteration 18, Batch 17/100: Batch Loss = 0.2373\n",
            "Iteration 18, Batch 18/100: Batch Loss = 0.0470\n",
            "Iteration 18, Batch 19/100: Batch Loss = 0.0694\n",
            "Iteration 18, Batch 20/100: Batch Loss = 0.1149\n",
            "Iteration 18, Batch 21/100: Batch Loss = 0.0496\n",
            "Iteration 18, Batch 22/100: Batch Loss = 0.0264\n",
            "Iteration 18, Batch 23/100: Batch Loss = 0.2136\n",
            "Iteration 18, Batch 24/100: Batch Loss = 0.1127\n",
            "Iteration 18, Batch 25/100: Batch Loss = 0.1278\n",
            "Iteration 18, Batch 26/100: Batch Loss = 0.2228\n",
            "Iteration 18, Batch 27/100: Batch Loss = 0.1258\n",
            "Iteration 18, Batch 28/100: Batch Loss = 0.1347\n",
            "Iteration 18, Batch 29/100: Batch Loss = 0.1754\n",
            "Iteration 18, Batch 30/100: Batch Loss = 0.1515\n",
            "Iteration 18, Batch 31/100: Batch Loss = 0.0984\n",
            "Iteration 18, Batch 32/100: Batch Loss = 0.0534\n",
            "Iteration 18, Batch 33/100: Batch Loss = 0.1598\n",
            "Iteration 18, Batch 34/100: Batch Loss = 0.1814\n",
            "Iteration 18, Batch 35/100: Batch Loss = 0.1144\n",
            "Iteration 18, Batch 36/100: Batch Loss = 0.1986\n",
            "Iteration 18, Batch 37/100: Batch Loss = 0.1752\n",
            "Iteration 18, Batch 38/100: Batch Loss = 0.1166\n",
            "Iteration 18, Batch 39/100: Batch Loss = 0.3767\n",
            "Iteration 18, Batch 40/100: Batch Loss = 0.1200\n",
            "Iteration 18, Batch 41/100: Batch Loss = 0.1504\n",
            "Iteration 18, Batch 42/100: Batch Loss = 0.0691\n",
            "Iteration 18, Batch 43/100: Batch Loss = 0.1163\n",
            "Iteration 18, Batch 44/100: Batch Loss = 0.2009\n",
            "Iteration 18, Batch 45/100: Batch Loss = 0.1041\n",
            "Iteration 18, Batch 46/100: Batch Loss = 0.1221\n",
            "Iteration 18, Batch 47/100: Batch Loss = 0.0814\n",
            "Iteration 18, Batch 48/100: Batch Loss = 0.1700\n",
            "Iteration 18, Batch 49/100: Batch Loss = 0.1069\n",
            "Iteration 18, Batch 50/100: Batch Loss = 0.2167\n",
            "Iteration 18, Batch 51/100: Batch Loss = 0.2519\n",
            "Iteration 18, Batch 52/100: Batch Loss = 0.3013\n",
            "Iteration 18, Batch 53/100: Batch Loss = 0.0863\n",
            "Iteration 18, Batch 54/100: Batch Loss = 0.1219\n",
            "Iteration 18, Batch 55/100: Batch Loss = 0.2366\n",
            "Iteration 18, Batch 56/100: Batch Loss = 0.0602\n",
            "Iteration 18, Batch 57/100: Batch Loss = 0.1342\n",
            "Iteration 18, Batch 58/100: Batch Loss = 0.0405\n",
            "Iteration 18, Batch 59/100: Batch Loss = 0.1442\n",
            "Iteration 18, Batch 60/100: Batch Loss = 0.1083\n",
            "Iteration 18, Batch 61/100: Batch Loss = 0.1396\n",
            "Iteration 18, Batch 62/100: Batch Loss = 0.0998\n",
            "Iteration 18, Batch 63/100: Batch Loss = 0.1462\n",
            "Iteration 18, Batch 64/100: Batch Loss = 0.1921\n",
            "Iteration 18, Batch 65/100: Batch Loss = 0.0972\n",
            "Iteration 18, Batch 66/100: Batch Loss = 0.0983\n",
            "Iteration 18, Batch 67/100: Batch Loss = 0.1097\n",
            "Iteration 18, Batch 68/100: Batch Loss = 0.1551\n",
            "Iteration 18, Batch 69/100: Batch Loss = 0.0292\n",
            "Iteration 18, Batch 70/100: Batch Loss = 0.2548\n",
            "Iteration 18, Batch 71/100: Batch Loss = 0.0461\n",
            "Iteration 18, Batch 72/100: Batch Loss = 0.0317\n",
            "Iteration 18, Batch 73/100: Batch Loss = 0.1479\n",
            "Iteration 18, Batch 74/100: Batch Loss = 0.0661\n",
            "Iteration 18, Batch 75/100: Batch Loss = 0.3369\n",
            "Iteration 18, Batch 76/100: Batch Loss = 0.3279\n",
            "Iteration 18, Batch 77/100: Batch Loss = 0.1197\n",
            "Iteration 18, Batch 78/100: Batch Loss = 0.1809\n",
            "Iteration 18, Batch 79/100: Batch Loss = 0.1307\n",
            "Iteration 18, Batch 80/100: Batch Loss = 0.0781\n",
            "Iteration 18, Batch 81/100: Batch Loss = 0.0566\n",
            "Iteration 18, Batch 82/100: Batch Loss = 0.2068\n",
            "Iteration 18, Batch 83/100: Batch Loss = 0.0928\n",
            "Iteration 18, Batch 84/100: Batch Loss = 0.3997\n",
            "Iteration 18, Batch 85/100: Batch Loss = 0.1981\n",
            "Iteration 18, Batch 86/100: Batch Loss = 0.1521\n",
            "Iteration 18, Batch 87/100: Batch Loss = 0.2532\n",
            "Iteration 18, Batch 88/100: Batch Loss = 0.0331\n",
            "Iteration 18, Batch 89/100: Batch Loss = 0.0444\n",
            "Iteration 18, Batch 90/100: Batch Loss = 0.1574\n",
            "Iteration 18, Batch 91/100: Batch Loss = 0.0947\n",
            "Iteration 18, Batch 92/100: Batch Loss = 0.1251\n",
            "Iteration 18, Batch 93/100: Batch Loss = 0.1951\n",
            "Iteration 18, Batch 94/100: Batch Loss = 0.2889\n",
            "Iteration 18, Batch 95/100: Batch Loss = 0.0936\n",
            "Iteration 18, Batch 96/100: Batch Loss = 0.1971\n",
            "Iteration 18, Batch 97/100: Batch Loss = 0.2568\n",
            "Iteration 18, Batch 98/100: Batch Loss = 0.1639\n",
            "Iteration 18, Batch 99/100: Batch Loss = 0.0795\n",
            "Iteration 18, Batch 100/100: Batch Loss = 0.1449\n",
            "Iteration 18: Train Loss = 0.1419, Val Loss = 0.1654\n",
            "Iteration 19, Batch 1/100: Batch Loss = 0.0707\n",
            "Iteration 19, Batch 2/100: Batch Loss = 0.2939\n",
            "Iteration 19, Batch 3/100: Batch Loss = 0.0945\n",
            "Iteration 19, Batch 4/100: Batch Loss = 0.0551\n",
            "Iteration 19, Batch 5/100: Batch Loss = 0.0701\n",
            "Iteration 19, Batch 6/100: Batch Loss = 0.1043\n",
            "Iteration 19, Batch 7/100: Batch Loss = 0.2144\n",
            "Iteration 19, Batch 8/100: Batch Loss = 0.2560\n",
            "Iteration 19, Batch 9/100: Batch Loss = 0.0961\n",
            "Iteration 19, Batch 10/100: Batch Loss = 0.2413\n",
            "Iteration 19, Batch 11/100: Batch Loss = 0.1781\n",
            "Iteration 19, Batch 12/100: Batch Loss = 0.0792\n",
            "Iteration 19, Batch 13/100: Batch Loss = 0.1016\n",
            "Iteration 19, Batch 14/100: Batch Loss = 0.0409\n",
            "Iteration 19, Batch 15/100: Batch Loss = 0.0679\n",
            "Iteration 19, Batch 16/100: Batch Loss = 0.1824\n",
            "Iteration 19, Batch 17/100: Batch Loss = 0.2116\n",
            "Iteration 19, Batch 18/100: Batch Loss = 0.1467\n",
            "Iteration 19, Batch 19/100: Batch Loss = 0.0904\n",
            "Iteration 19, Batch 20/100: Batch Loss = 0.0620\n",
            "Iteration 19, Batch 21/100: Batch Loss = 0.1169\n",
            "Iteration 19, Batch 22/100: Batch Loss = 0.0763\n",
            "Iteration 19, Batch 23/100: Batch Loss = 0.1174\n",
            "Iteration 19, Batch 24/100: Batch Loss = 0.1227\n",
            "Iteration 19, Batch 25/100: Batch Loss = 0.1519\n",
            "Iteration 19, Batch 26/100: Batch Loss = 0.2442\n",
            "Iteration 19, Batch 27/100: Batch Loss = 0.1018\n",
            "Iteration 19, Batch 28/100: Batch Loss = 0.0215\n",
            "Iteration 19, Batch 29/100: Batch Loss = 0.1347\n",
            "Iteration 19, Batch 30/100: Batch Loss = 0.0324\n",
            "Iteration 19, Batch 31/100: Batch Loss = 0.0608\n",
            "Iteration 19, Batch 32/100: Batch Loss = 0.2944\n",
            "Iteration 19, Batch 33/100: Batch Loss = 0.0974\n",
            "Iteration 19, Batch 34/100: Batch Loss = 0.0245\n",
            "Iteration 19, Batch 35/100: Batch Loss = 0.0406\n",
            "Iteration 19, Batch 36/100: Batch Loss = 0.2459\n",
            "Iteration 19, Batch 37/100: Batch Loss = 0.0280\n",
            "Iteration 19, Batch 38/100: Batch Loss = 0.1309\n",
            "Iteration 19, Batch 39/100: Batch Loss = 0.1189\n",
            "Iteration 19, Batch 40/100: Batch Loss = 0.1632\n",
            "Iteration 19, Batch 41/100: Batch Loss = 0.1105\n",
            "Iteration 19, Batch 42/100: Batch Loss = 0.2480\n",
            "Iteration 19, Batch 43/100: Batch Loss = 0.1920\n",
            "Iteration 19, Batch 44/100: Batch Loss = 0.1450\n",
            "Iteration 19, Batch 45/100: Batch Loss = 0.2346\n",
            "Iteration 19, Batch 46/100: Batch Loss = 0.1589\n",
            "Iteration 19, Batch 47/100: Batch Loss = 0.0528\n",
            "Iteration 19, Batch 48/100: Batch Loss = 0.2774\n",
            "Iteration 19, Batch 49/100: Batch Loss = 0.0850\n",
            "Iteration 19, Batch 50/100: Batch Loss = 0.1612\n",
            "Iteration 19, Batch 51/100: Batch Loss = 0.0849\n",
            "Iteration 19, Batch 52/100: Batch Loss = 0.0757\n",
            "Iteration 19, Batch 53/100: Batch Loss = 0.2000\n",
            "Iteration 19, Batch 54/100: Batch Loss = 0.1137\n",
            "Iteration 19, Batch 55/100: Batch Loss = 0.1387\n",
            "Iteration 19, Batch 56/100: Batch Loss = 0.0503\n",
            "Iteration 19, Batch 57/100: Batch Loss = 0.1239\n",
            "Iteration 19, Batch 58/100: Batch Loss = 0.1274\n",
            "Iteration 19, Batch 59/100: Batch Loss = 0.0660\n",
            "Iteration 19, Batch 60/100: Batch Loss = 0.1368\n",
            "Iteration 19, Batch 61/100: Batch Loss = 0.0741\n",
            "Iteration 19, Batch 62/100: Batch Loss = 0.1755\n",
            "Iteration 19, Batch 63/100: Batch Loss = 0.1133\n",
            "Iteration 19, Batch 64/100: Batch Loss = 0.0911\n",
            "Iteration 19, Batch 65/100: Batch Loss = 0.0824\n",
            "Iteration 19, Batch 66/100: Batch Loss = 0.2296\n",
            "Iteration 19, Batch 67/100: Batch Loss = 0.1386\n",
            "Iteration 19, Batch 68/100: Batch Loss = 0.1673\n",
            "Iteration 19, Batch 69/100: Batch Loss = 0.1681\n",
            "Iteration 19, Batch 70/100: Batch Loss = 0.2026\n",
            "Iteration 19, Batch 71/100: Batch Loss = 0.0971\n",
            "Iteration 19, Batch 72/100: Batch Loss = 0.2307\n",
            "Iteration 19, Batch 73/100: Batch Loss = 0.3374\n",
            "Iteration 19, Batch 74/100: Batch Loss = 0.3838\n",
            "Iteration 19, Batch 75/100: Batch Loss = 0.1269\n",
            "Iteration 19, Batch 76/100: Batch Loss = 0.1156\n",
            "Iteration 19, Batch 77/100: Batch Loss = 0.1792\n",
            "Iteration 19, Batch 78/100: Batch Loss = 0.0899\n",
            "Iteration 19, Batch 79/100: Batch Loss = 0.0794\n",
            "Iteration 19, Batch 80/100: Batch Loss = 0.1361\n",
            "Iteration 19, Batch 81/100: Batch Loss = 0.2196\n",
            "Iteration 19, Batch 82/100: Batch Loss = 0.0791\n",
            "Iteration 19, Batch 83/100: Batch Loss = 0.0427\n",
            "Iteration 19, Batch 84/100: Batch Loss = 0.0821\n",
            "Iteration 19, Batch 85/100: Batch Loss = 0.1426\n",
            "Iteration 19, Batch 86/100: Batch Loss = 0.0966\n",
            "Iteration 19, Batch 87/100: Batch Loss = 0.1457\n",
            "Iteration 19, Batch 88/100: Batch Loss = 0.0636\n",
            "Iteration 19, Batch 89/100: Batch Loss = 0.1001\n",
            "Iteration 19, Batch 90/100: Batch Loss = 0.1050\n",
            "Iteration 19, Batch 91/100: Batch Loss = 0.2682\n",
            "Iteration 19, Batch 92/100: Batch Loss = 0.2254\n",
            "Iteration 19, Batch 93/100: Batch Loss = 0.1106\n",
            "Iteration 19, Batch 94/100: Batch Loss = 0.0970\n",
            "Iteration 19, Batch 95/100: Batch Loss = 0.2448\n",
            "Iteration 19, Batch 96/100: Batch Loss = 0.2901\n",
            "Iteration 19, Batch 97/100: Batch Loss = 0.0382\n",
            "Iteration 19, Batch 98/100: Batch Loss = 0.0808\n",
            "Iteration 19, Batch 99/100: Batch Loss = 0.2607\n",
            "Iteration 19, Batch 100/100: Batch Loss = 0.0788\n",
            "Iteration 19: Train Loss = 0.1375, Val Loss = 0.1763\n",
            "Iteration 20, Batch 1/100: Batch Loss = 0.0924\n",
            "Iteration 20, Batch 2/100: Batch Loss = 0.1049\n",
            "Iteration 20, Batch 3/100: Batch Loss = 0.1643\n",
            "Iteration 20, Batch 4/100: Batch Loss = 0.1133\n",
            "Iteration 20, Batch 5/100: Batch Loss = 0.0598\n",
            "Iteration 20, Batch 6/100: Batch Loss = 0.0793\n",
            "Iteration 20, Batch 7/100: Batch Loss = 0.1383\n",
            "Iteration 20, Batch 8/100: Batch Loss = 0.3323\n",
            "Iteration 20, Batch 9/100: Batch Loss = 0.2143\n",
            "Iteration 20, Batch 10/100: Batch Loss = 0.0763\n",
            "Iteration 20, Batch 11/100: Batch Loss = 0.2026\n",
            "Iteration 20, Batch 12/100: Batch Loss = 0.0986\n",
            "Iteration 20, Batch 13/100: Batch Loss = 0.1965\n",
            "Iteration 20, Batch 14/100: Batch Loss = 0.2999\n",
            "Iteration 20, Batch 15/100: Batch Loss = 0.2415\n",
            "Iteration 20, Batch 16/100: Batch Loss = 0.1498\n",
            "Iteration 20, Batch 17/100: Batch Loss = 0.1136\n",
            "Iteration 20, Batch 18/100: Batch Loss = 0.1546\n",
            "Iteration 20, Batch 19/100: Batch Loss = 0.1046\n",
            "Iteration 20, Batch 20/100: Batch Loss = 0.1576\n",
            "Iteration 20, Batch 21/100: Batch Loss = 0.1379\n",
            "Iteration 20, Batch 22/100: Batch Loss = 0.1125\n",
            "Iteration 20, Batch 23/100: Batch Loss = 0.1383\n",
            "Iteration 20, Batch 24/100: Batch Loss = 0.2625\n",
            "Iteration 20, Batch 25/100: Batch Loss = 0.2038\n",
            "Iteration 20, Batch 26/100: Batch Loss = 0.1075\n",
            "Iteration 20, Batch 27/100: Batch Loss = 0.2383\n",
            "Iteration 20, Batch 28/100: Batch Loss = 0.1295\n",
            "Iteration 20, Batch 29/100: Batch Loss = 0.0605\n",
            "Iteration 20, Batch 30/100: Batch Loss = 0.1349\n",
            "Iteration 20, Batch 31/100: Batch Loss = 0.0885\n",
            "Iteration 20, Batch 32/100: Batch Loss = 0.0384\n",
            "Iteration 20, Batch 33/100: Batch Loss = 0.1060\n",
            "Iteration 20, Batch 34/100: Batch Loss = 0.1829\n",
            "Iteration 20, Batch 35/100: Batch Loss = 0.1079\n",
            "Iteration 20, Batch 36/100: Batch Loss = 0.1565\n",
            "Iteration 20, Batch 37/100: Batch Loss = 0.1481\n",
            "Iteration 20, Batch 38/100: Batch Loss = 0.0951\n",
            "Iteration 20, Batch 39/100: Batch Loss = 0.1389\n",
            "Iteration 20, Batch 40/100: Batch Loss = 0.1223\n",
            "Iteration 20, Batch 41/100: Batch Loss = 0.2145\n",
            "Iteration 20, Batch 42/100: Batch Loss = 0.0584\n",
            "Iteration 20, Batch 43/100: Batch Loss = 0.0793\n",
            "Iteration 20, Batch 44/100: Batch Loss = 0.1064\n",
            "Iteration 20, Batch 45/100: Batch Loss = 0.0752\n",
            "Iteration 20, Batch 46/100: Batch Loss = 0.0916\n",
            "Iteration 20, Batch 47/100: Batch Loss = 0.1190\n",
            "Iteration 20, Batch 48/100: Batch Loss = 0.1369\n",
            "Iteration 20, Batch 49/100: Batch Loss = 0.0412\n",
            "Iteration 20, Batch 50/100: Batch Loss = 0.0980\n",
            "Iteration 20, Batch 51/100: Batch Loss = 0.0873\n",
            "Iteration 20, Batch 52/100: Batch Loss = 0.1381\n",
            "Iteration 20, Batch 53/100: Batch Loss = 0.1455\n",
            "Iteration 20, Batch 54/100: Batch Loss = 0.1793\n",
            "Iteration 20, Batch 55/100: Batch Loss = 0.1220\n",
            "Iteration 20, Batch 56/100: Batch Loss = 0.5354\n",
            "Iteration 20, Batch 57/100: Batch Loss = 0.1241\n",
            "Iteration 20, Batch 58/100: Batch Loss = 0.1213\n",
            "Iteration 20, Batch 59/100: Batch Loss = 0.1680\n",
            "Iteration 20, Batch 60/100: Batch Loss = 0.0643\n",
            "Iteration 20, Batch 61/100: Batch Loss = 0.1884\n",
            "Iteration 20, Batch 62/100: Batch Loss = 0.1926\n",
            "Iteration 20, Batch 63/100: Batch Loss = 0.1034\n",
            "Iteration 20, Batch 64/100: Batch Loss = 0.0923\n",
            "Iteration 20, Batch 65/100: Batch Loss = 0.0998\n",
            "Iteration 20, Batch 66/100: Batch Loss = 0.1074\n",
            "Iteration 20, Batch 67/100: Batch Loss = 0.1847\n",
            "Iteration 20, Batch 68/100: Batch Loss = 0.0667\n",
            "Iteration 20, Batch 69/100: Batch Loss = 0.1659\n",
            "Iteration 20, Batch 70/100: Batch Loss = 0.0525\n",
            "Iteration 20, Batch 71/100: Batch Loss = 0.1447\n",
            "Iteration 20, Batch 72/100: Batch Loss = 0.1719\n",
            "Iteration 20, Batch 73/100: Batch Loss = 0.2140\n",
            "Iteration 20, Batch 74/100: Batch Loss = 0.3947\n",
            "Iteration 20, Batch 75/100: Batch Loss = 0.2710\n",
            "Iteration 20, Batch 76/100: Batch Loss = 0.1099\n",
            "Iteration 20, Batch 77/100: Batch Loss = 0.0402\n",
            "Iteration 20, Batch 78/100: Batch Loss = 0.0609\n",
            "Iteration 20, Batch 79/100: Batch Loss = 0.1526\n",
            "Iteration 20, Batch 80/100: Batch Loss = 0.0855\n",
            "Iteration 20, Batch 81/100: Batch Loss = 0.0424\n",
            "Iteration 20, Batch 82/100: Batch Loss = 0.1290\n",
            "Iteration 20, Batch 83/100: Batch Loss = 0.2553\n",
            "Iteration 20, Batch 84/100: Batch Loss = 0.1259\n",
            "Iteration 20, Batch 85/100: Batch Loss = 0.2669\n",
            "Iteration 20, Batch 86/100: Batch Loss = 0.0528\n",
            "Iteration 20, Batch 87/100: Batch Loss = 0.2632\n",
            "Iteration 20, Batch 88/100: Batch Loss = 0.0480\n",
            "Iteration 20, Batch 89/100: Batch Loss = 0.0627\n",
            "Iteration 20, Batch 90/100: Batch Loss = 0.0509\n",
            "Iteration 20, Batch 91/100: Batch Loss = 0.0261\n",
            "Iteration 20, Batch 92/100: Batch Loss = 0.2029\n",
            "Iteration 20, Batch 93/100: Batch Loss = 0.1015\n",
            "Iteration 20, Batch 94/100: Batch Loss = 0.0702\n",
            "Iteration 20, Batch 95/100: Batch Loss = 0.4475\n",
            "Iteration 20, Batch 96/100: Batch Loss = 0.1277\n",
            "Iteration 20, Batch 97/100: Batch Loss = 0.1306\n",
            "Iteration 20, Batch 98/100: Batch Loss = 0.1474\n",
            "Iteration 20, Batch 99/100: Batch Loss = 0.0841\n",
            "Iteration 20, Batch 100/100: Batch Loss = 0.0717\n",
            "Iteration 20: Train Loss = 0.1412, Val Loss = 0.1648\n",
            "Iteration 21, Batch 1/100: Batch Loss = 0.0830\n",
            "Iteration 21, Batch 2/100: Batch Loss = 0.1253\n",
            "Iteration 21, Batch 3/100: Batch Loss = 0.1107\n",
            "Iteration 21, Batch 4/100: Batch Loss = 0.1108\n",
            "Iteration 21, Batch 5/100: Batch Loss = 0.1060\n",
            "Iteration 21, Batch 6/100: Batch Loss = 0.0686\n",
            "Iteration 21, Batch 7/100: Batch Loss = 0.0965\n",
            "Iteration 21, Batch 8/100: Batch Loss = 0.3631\n",
            "Iteration 21, Batch 9/100: Batch Loss = 0.0915\n",
            "Iteration 21, Batch 10/100: Batch Loss = 0.1653\n",
            "Iteration 21, Batch 11/100: Batch Loss = 0.0570\n",
            "Iteration 21, Batch 12/100: Batch Loss = 0.0804\n",
            "Iteration 21, Batch 13/100: Batch Loss = 0.2291\n",
            "Iteration 21, Batch 14/100: Batch Loss = 0.3291\n",
            "Iteration 21, Batch 15/100: Batch Loss = 0.3069\n",
            "Iteration 21, Batch 16/100: Batch Loss = 0.0775\n",
            "Iteration 21, Batch 17/100: Batch Loss = 0.1517\n",
            "Iteration 21, Batch 18/100: Batch Loss = 0.1137\n",
            "Iteration 21, Batch 19/100: Batch Loss = 0.3095\n",
            "Iteration 21, Batch 20/100: Batch Loss = 0.1521\n",
            "Iteration 21, Batch 21/100: Batch Loss = 0.1281\n",
            "Iteration 21, Batch 22/100: Batch Loss = 0.0982\n",
            "Iteration 21, Batch 23/100: Batch Loss = 0.1324\n",
            "Iteration 21, Batch 24/100: Batch Loss = 0.1186\n",
            "Iteration 21, Batch 25/100: Batch Loss = 0.1094\n",
            "Iteration 21, Batch 26/100: Batch Loss = 0.1182\n",
            "Iteration 21, Batch 27/100: Batch Loss = 0.1732\n",
            "Iteration 21, Batch 28/100: Batch Loss = 0.1094\n",
            "Iteration 21, Batch 29/100: Batch Loss = 0.1326\n",
            "Iteration 21, Batch 30/100: Batch Loss = 0.2198\n",
            "Iteration 21, Batch 31/100: Batch Loss = 0.0490\n",
            "Iteration 21, Batch 32/100: Batch Loss = 0.2096\n",
            "Iteration 21, Batch 33/100: Batch Loss = 0.0392\n",
            "Iteration 21, Batch 34/100: Batch Loss = 0.0975\n",
            "Iteration 21, Batch 35/100: Batch Loss = 0.1436\n",
            "Iteration 21, Batch 36/100: Batch Loss = 0.5283\n",
            "Iteration 21, Batch 37/100: Batch Loss = 0.1457\n",
            "Iteration 21, Batch 38/100: Batch Loss = 0.1496\n",
            "Iteration 21, Batch 39/100: Batch Loss = 0.1653\n",
            "Iteration 21, Batch 40/100: Batch Loss = 0.1106\n",
            "Iteration 21, Batch 41/100: Batch Loss = 0.2930\n",
            "Iteration 21, Batch 42/100: Batch Loss = 0.1151\n",
            "Iteration 21, Batch 43/100: Batch Loss = 0.1894\n",
            "Iteration 21, Batch 44/100: Batch Loss = 0.0889\n",
            "Iteration 21, Batch 45/100: Batch Loss = 0.1223\n",
            "Iteration 21, Batch 46/100: Batch Loss = 0.1826\n",
            "Iteration 21, Batch 47/100: Batch Loss = 0.0956\n",
            "Iteration 21, Batch 48/100: Batch Loss = 0.0900\n",
            "Iteration 21, Batch 49/100: Batch Loss = 0.1698\n",
            "Iteration 21, Batch 50/100: Batch Loss = 0.0452\n",
            "Iteration 21, Batch 51/100: Batch Loss = 0.0656\n",
            "Iteration 21, Batch 52/100: Batch Loss = 0.0860\n",
            "Iteration 21, Batch 53/100: Batch Loss = 0.0428\n",
            "Iteration 21, Batch 54/100: Batch Loss = 0.1146\n",
            "Iteration 21, Batch 55/100: Batch Loss = 0.1099\n",
            "Iteration 21, Batch 56/100: Batch Loss = 0.3079\n",
            "Iteration 21, Batch 57/100: Batch Loss = 0.0528\n",
            "Iteration 21, Batch 58/100: Batch Loss = 0.1252\n",
            "Iteration 21, Batch 59/100: Batch Loss = 0.1106\n",
            "Iteration 21, Batch 60/100: Batch Loss = 0.2224\n",
            "Iteration 21, Batch 61/100: Batch Loss = 0.0766\n",
            "Iteration 21, Batch 62/100: Batch Loss = 0.1791\n",
            "Iteration 21, Batch 63/100: Batch Loss = 0.2698\n",
            "Iteration 21, Batch 64/100: Batch Loss = 0.2451\n",
            "Iteration 21, Batch 65/100: Batch Loss = 0.2942\n",
            "Iteration 21, Batch 66/100: Batch Loss = 0.1894\n",
            "Iteration 21, Batch 67/100: Batch Loss = 0.1525\n",
            "Iteration 21, Batch 68/100: Batch Loss = 0.0302\n",
            "Iteration 21, Batch 69/100: Batch Loss = 0.1487\n",
            "Iteration 21, Batch 70/100: Batch Loss = 0.1036\n",
            "Iteration 21, Batch 71/100: Batch Loss = 0.0923\n",
            "Iteration 21, Batch 72/100: Batch Loss = 0.0948\n",
            "Iteration 21, Batch 73/100: Batch Loss = 0.0639\n",
            "Iteration 21, Batch 74/100: Batch Loss = 0.1601\n",
            "Iteration 21, Batch 75/100: Batch Loss = 0.0429\n",
            "Iteration 21, Batch 76/100: Batch Loss = 0.1283\n",
            "Iteration 21, Batch 77/100: Batch Loss = 0.1960\n",
            "Iteration 21, Batch 78/100: Batch Loss = 0.0504\n",
            "Iteration 21, Batch 79/100: Batch Loss = 0.1103\n",
            "Iteration 21, Batch 80/100: Batch Loss = 0.1662\n",
            "Iteration 21, Batch 81/100: Batch Loss = 0.2403\n",
            "Iteration 21, Batch 82/100: Batch Loss = 0.2024\n",
            "Iteration 21, Batch 83/100: Batch Loss = 0.2059\n",
            "Iteration 21, Batch 84/100: Batch Loss = 0.0274\n",
            "Iteration 21, Batch 85/100: Batch Loss = 0.1144\n",
            "Iteration 21, Batch 86/100: Batch Loss = 0.0672\n",
            "Iteration 21, Batch 87/100: Batch Loss = 0.1125\n",
            "Iteration 21, Batch 88/100: Batch Loss = 0.0759\n",
            "Iteration 21, Batch 89/100: Batch Loss = 0.1172\n",
            "Iteration 21, Batch 90/100: Batch Loss = 0.0697\n",
            "Iteration 21, Batch 91/100: Batch Loss = 0.1704\n",
            "Iteration 21, Batch 92/100: Batch Loss = 0.1161\n",
            "Iteration 21, Batch 93/100: Batch Loss = 0.0832\n",
            "Iteration 21, Batch 94/100: Batch Loss = 0.0976\n",
            "Iteration 21, Batch 95/100: Batch Loss = 0.1935\n",
            "Iteration 21, Batch 96/100: Batch Loss = 0.2025\n",
            "Iteration 21, Batch 97/100: Batch Loss = 0.0781\n",
            "Iteration 21, Batch 98/100: Batch Loss = 0.0986\n",
            "Iteration 21, Batch 99/100: Batch Loss = 0.1013\n",
            "Iteration 21, Batch 100/100: Batch Loss = 0.1244\n",
            "Iteration 21: Train Loss = 0.1403, Val Loss = 0.1584\n",
            "Iteration 22, Batch 1/100: Batch Loss = 0.1154\n",
            "Iteration 22, Batch 2/100: Batch Loss = 0.2060\n",
            "Iteration 22, Batch 3/100: Batch Loss = 0.0816\n",
            "Iteration 22, Batch 4/100: Batch Loss = 0.0868\n",
            "Iteration 22, Batch 5/100: Batch Loss = 0.1073\n",
            "Iteration 22, Batch 6/100: Batch Loss = 0.1578\n",
            "Iteration 22, Batch 7/100: Batch Loss = 0.1374\n",
            "Iteration 22, Batch 8/100: Batch Loss = 0.1047\n",
            "Iteration 22, Batch 9/100: Batch Loss = 0.0901\n",
            "Iteration 22, Batch 10/100: Batch Loss = 0.0733\n",
            "Iteration 22, Batch 11/100: Batch Loss = 0.0934\n",
            "Iteration 22, Batch 12/100: Batch Loss = 0.0483\n",
            "Iteration 22, Batch 13/100: Batch Loss = 0.1076\n",
            "Iteration 22, Batch 14/100: Batch Loss = 0.0302\n",
            "Iteration 22, Batch 15/100: Batch Loss = 0.0668\n",
            "Iteration 22, Batch 16/100: Batch Loss = 0.1467\n",
            "Iteration 22, Batch 17/100: Batch Loss = 0.1267\n",
            "Iteration 22, Batch 18/100: Batch Loss = 0.2114\n",
            "Iteration 22, Batch 19/100: Batch Loss = 0.1073\n",
            "Iteration 22, Batch 20/100: Batch Loss = 0.0708\n",
            "Iteration 22, Batch 21/100: Batch Loss = 0.0468\n",
            "Iteration 22, Batch 22/100: Batch Loss = 0.0971\n",
            "Iteration 22, Batch 23/100: Batch Loss = 0.1408\n",
            "Iteration 22, Batch 24/100: Batch Loss = 0.0676\n",
            "Iteration 22, Batch 25/100: Batch Loss = 0.1385\n",
            "Iteration 22, Batch 26/100: Batch Loss = 0.1690\n",
            "Iteration 22, Batch 27/100: Batch Loss = 0.0603\n",
            "Iteration 22, Batch 28/100: Batch Loss = 0.2003\n",
            "Iteration 22, Batch 29/100: Batch Loss = 0.0495\n",
            "Iteration 22, Batch 30/100: Batch Loss = 0.1119\n",
            "Iteration 22, Batch 31/100: Batch Loss = 0.0897\n",
            "Iteration 22, Batch 32/100: Batch Loss = 0.3583\n",
            "Iteration 22, Batch 33/100: Batch Loss = 0.1791\n",
            "Iteration 22, Batch 34/100: Batch Loss = 0.0254\n",
            "Iteration 22, Batch 35/100: Batch Loss = 0.0524\n",
            "Iteration 22, Batch 36/100: Batch Loss = 0.1097\n",
            "Iteration 22, Batch 37/100: Batch Loss = 0.1676\n",
            "Iteration 22, Batch 38/100: Batch Loss = 0.2342\n",
            "Iteration 22, Batch 39/100: Batch Loss = 0.2472\n",
            "Iteration 22, Batch 40/100: Batch Loss = 0.0352\n",
            "Iteration 22, Batch 41/100: Batch Loss = 0.0463\n",
            "Iteration 22, Batch 42/100: Batch Loss = 0.0726\n",
            "Iteration 22, Batch 43/100: Batch Loss = 0.0497\n",
            "Iteration 22, Batch 44/100: Batch Loss = 0.1039\n",
            "Iteration 22, Batch 45/100: Batch Loss = 0.0726\n",
            "Iteration 22, Batch 46/100: Batch Loss = 0.1808\n",
            "Iteration 22, Batch 47/100: Batch Loss = 0.0904\n",
            "Iteration 22, Batch 48/100: Batch Loss = 0.0630\n",
            "Iteration 22, Batch 49/100: Batch Loss = 0.1778\n",
            "Iteration 22, Batch 50/100: Batch Loss = 0.0580\n",
            "Iteration 22, Batch 51/100: Batch Loss = 0.0394\n",
            "Iteration 22, Batch 52/100: Batch Loss = 0.4217\n",
            "Iteration 22, Batch 53/100: Batch Loss = 0.0947\n",
            "Iteration 22, Batch 54/100: Batch Loss = 0.2079\n",
            "Iteration 22, Batch 55/100: Batch Loss = 0.1383\n",
            "Iteration 22, Batch 56/100: Batch Loss = 0.3446\n",
            "Iteration 22, Batch 57/100: Batch Loss = 0.1296\n",
            "Iteration 22, Batch 58/100: Batch Loss = 0.0509\n",
            "Iteration 22, Batch 59/100: Batch Loss = 0.2212\n",
            "Iteration 22, Batch 60/100: Batch Loss = 0.0441\n",
            "Iteration 22, Batch 61/100: Batch Loss = 0.0572\n",
            "Iteration 22, Batch 62/100: Batch Loss = 0.0601\n",
            "Iteration 22, Batch 63/100: Batch Loss = 0.1070\n",
            "Iteration 22, Batch 64/100: Batch Loss = 0.1767\n",
            "Iteration 22, Batch 65/100: Batch Loss = 0.1736\n",
            "Iteration 22, Batch 66/100: Batch Loss = 0.1848\n",
            "Iteration 22, Batch 67/100: Batch Loss = 0.0849\n",
            "Iteration 22, Batch 68/100: Batch Loss = 0.1536\n",
            "Iteration 22, Batch 69/100: Batch Loss = 0.1106\n",
            "Iteration 22, Batch 70/100: Batch Loss = 0.2044\n",
            "Iteration 22, Batch 71/100: Batch Loss = 0.0726\n",
            "Iteration 22, Batch 72/100: Batch Loss = 0.0921\n",
            "Iteration 22, Batch 73/100: Batch Loss = 0.2544\n",
            "Iteration 22, Batch 74/100: Batch Loss = 0.0347\n",
            "Iteration 22, Batch 75/100: Batch Loss = 0.1916\n",
            "Iteration 22, Batch 76/100: Batch Loss = 0.1108\n",
            "Iteration 22, Batch 77/100: Batch Loss = 0.2913\n",
            "Iteration 22, Batch 78/100: Batch Loss = 0.1046\n",
            "Iteration 22, Batch 79/100: Batch Loss = 0.0295\n",
            "Iteration 22, Batch 80/100: Batch Loss = 0.1475\n",
            "Iteration 22, Batch 81/100: Batch Loss = 0.0958\n",
            "Iteration 22, Batch 82/100: Batch Loss = 0.0896\n",
            "Iteration 22, Batch 83/100: Batch Loss = 0.1959\n",
            "Iteration 22, Batch 84/100: Batch Loss = 0.2013\n",
            "Iteration 22, Batch 85/100: Batch Loss = 0.1793\n",
            "Iteration 22, Batch 86/100: Batch Loss = 0.0330\n",
            "Iteration 22, Batch 87/100: Batch Loss = 0.1764\n",
            "Iteration 22, Batch 88/100: Batch Loss = 0.3623\n",
            "Iteration 22, Batch 89/100: Batch Loss = 0.1566\n",
            "Iteration 22, Batch 90/100: Batch Loss = 0.2558\n",
            "Iteration 22, Batch 91/100: Batch Loss = 0.1186\n",
            "Iteration 22, Batch 92/100: Batch Loss = 0.1140\n",
            "Iteration 22, Batch 93/100: Batch Loss = 0.1003\n",
            "Iteration 22, Batch 94/100: Batch Loss = 0.1301\n",
            "Iteration 22, Batch 95/100: Batch Loss = 0.1829\n",
            "Iteration 22, Batch 96/100: Batch Loss = 0.3375\n",
            "Iteration 22, Batch 97/100: Batch Loss = 0.1888\n",
            "Iteration 22, Batch 98/100: Batch Loss = 0.2416\n",
            "Iteration 22, Batch 99/100: Batch Loss = 0.1571\n",
            "Iteration 22, Batch 100/100: Batch Loss = 0.2379\n",
            "Iteration 22: Train Loss = 0.1355, Val Loss = 0.1491\n",
            "Iteration 23, Batch 1/100: Batch Loss = 0.1565\n",
            "Iteration 23, Batch 2/100: Batch Loss = 0.1076\n",
            "Iteration 23, Batch 3/100: Batch Loss = 0.1545\n",
            "Iteration 23, Batch 4/100: Batch Loss = 0.0678\n",
            "Iteration 23, Batch 5/100: Batch Loss = 0.1797\n",
            "Iteration 23, Batch 6/100: Batch Loss = 0.2708\n",
            "Iteration 23, Batch 7/100: Batch Loss = 0.1025\n",
            "Iteration 23, Batch 8/100: Batch Loss = 0.0719\n",
            "Iteration 23, Batch 9/100: Batch Loss = 0.0320\n",
            "Iteration 23, Batch 10/100: Batch Loss = 0.1141\n",
            "Iteration 23, Batch 11/100: Batch Loss = 0.0507\n",
            "Iteration 23, Batch 12/100: Batch Loss = 0.1695\n",
            "Iteration 23, Batch 13/100: Batch Loss = 0.0656\n",
            "Iteration 23, Batch 14/100: Batch Loss = 0.1498\n",
            "Iteration 23, Batch 15/100: Batch Loss = 0.2097\n",
            "Iteration 23, Batch 16/100: Batch Loss = 0.1183\n",
            "Iteration 23, Batch 17/100: Batch Loss = 0.2673\n",
            "Iteration 23, Batch 18/100: Batch Loss = 0.1813\n",
            "Iteration 23, Batch 19/100: Batch Loss = 0.0599\n",
            "Iteration 23, Batch 20/100: Batch Loss = 0.0789\n",
            "Iteration 23, Batch 21/100: Batch Loss = 0.1147\n",
            "Iteration 23, Batch 22/100: Batch Loss = 0.1615\n",
            "Iteration 23, Batch 23/100: Batch Loss = 0.1376\n",
            "Iteration 23, Batch 24/100: Batch Loss = 0.1254\n",
            "Iteration 23, Batch 25/100: Batch Loss = 0.0978\n",
            "Iteration 23, Batch 26/100: Batch Loss = 0.1908\n",
            "Iteration 23, Batch 27/100: Batch Loss = 0.3654\n",
            "Iteration 23, Batch 28/100: Batch Loss = 0.1036\n",
            "Iteration 23, Batch 29/100: Batch Loss = 0.1313\n",
            "Iteration 23, Batch 30/100: Batch Loss = 0.0601\n",
            "Iteration 23, Batch 31/100: Batch Loss = 0.1789\n",
            "Iteration 23, Batch 32/100: Batch Loss = 0.0913\n",
            "Iteration 23, Batch 33/100: Batch Loss = 0.0553\n",
            "Iteration 23, Batch 34/100: Batch Loss = 0.0433\n",
            "Iteration 23, Batch 35/100: Batch Loss = 0.1241\n",
            "Iteration 23, Batch 36/100: Batch Loss = 0.2729\n",
            "Iteration 23, Batch 37/100: Batch Loss = 0.0176\n",
            "Iteration 23, Batch 38/100: Batch Loss = 0.0386\n",
            "Iteration 23, Batch 39/100: Batch Loss = 0.1927\n",
            "Iteration 23, Batch 40/100: Batch Loss = 0.1353\n",
            "Iteration 23, Batch 41/100: Batch Loss = 0.2445\n",
            "Iteration 23, Batch 42/100: Batch Loss = 0.2640\n",
            "Iteration 23, Batch 43/100: Batch Loss = 0.1844\n",
            "Iteration 23, Batch 44/100: Batch Loss = 0.3106\n",
            "Iteration 23, Batch 45/100: Batch Loss = 0.2716\n",
            "Iteration 23, Batch 46/100: Batch Loss = 0.1782\n",
            "Iteration 23, Batch 47/100: Batch Loss = 0.1272\n",
            "Iteration 23, Batch 48/100: Batch Loss = 0.0595\n",
            "Iteration 23, Batch 49/100: Batch Loss = 0.2012\n",
            "Iteration 23, Batch 50/100: Batch Loss = 0.0638\n",
            "Iteration 23, Batch 51/100: Batch Loss = 0.0330\n",
            "Iteration 23, Batch 52/100: Batch Loss = 0.0629\n",
            "Iteration 23, Batch 53/100: Batch Loss = 0.0369\n",
            "Iteration 23, Batch 54/100: Batch Loss = 0.0802\n",
            "Iteration 23, Batch 55/100: Batch Loss = 0.0772\n",
            "Iteration 23, Batch 56/100: Batch Loss = 0.2514\n",
            "Iteration 23, Batch 57/100: Batch Loss = 0.0736\n",
            "Iteration 23, Batch 58/100: Batch Loss = 0.0655\n",
            "Iteration 23, Batch 59/100: Batch Loss = 0.2484\n",
            "Iteration 23, Batch 60/100: Batch Loss = 0.0827\n",
            "Iteration 23, Batch 61/100: Batch Loss = 0.1304\n",
            "Iteration 23, Batch 62/100: Batch Loss = 0.0734\n",
            "Iteration 23, Batch 63/100: Batch Loss = 0.0891\n",
            "Iteration 23, Batch 64/100: Batch Loss = 0.1554\n",
            "Iteration 23, Batch 65/100: Batch Loss = 0.3201\n",
            "Iteration 23, Batch 66/100: Batch Loss = 0.1214\n",
            "Iteration 23, Batch 67/100: Batch Loss = 0.1696\n",
            "Iteration 23, Batch 68/100: Batch Loss = 0.0750\n",
            "Iteration 23, Batch 69/100: Batch Loss = 0.1608\n",
            "Iteration 23, Batch 70/100: Batch Loss = 0.0566\n",
            "Iteration 23, Batch 71/100: Batch Loss = 0.0742\n",
            "Iteration 23, Batch 72/100: Batch Loss = 0.0708\n",
            "Iteration 23, Batch 73/100: Batch Loss = 0.2178\n",
            "Iteration 23, Batch 74/100: Batch Loss = 0.0684\n",
            "Iteration 23, Batch 75/100: Batch Loss = 0.0973\n",
            "Iteration 23, Batch 76/100: Batch Loss = 0.1488\n",
            "Iteration 23, Batch 77/100: Batch Loss = 0.1893\n",
            "Iteration 23, Batch 78/100: Batch Loss = 0.2587\n",
            "Iteration 23, Batch 79/100: Batch Loss = 0.1122\n",
            "Iteration 23, Batch 80/100: Batch Loss = 0.1293\n",
            "Iteration 23, Batch 81/100: Batch Loss = 0.0981\n",
            "Iteration 23, Batch 82/100: Batch Loss = 0.0329\n",
            "Iteration 23, Batch 83/100: Batch Loss = 0.1442\n",
            "Iteration 23, Batch 84/100: Batch Loss = 0.1465\n",
            "Iteration 23, Batch 85/100: Batch Loss = 0.0813\n",
            "Iteration 23, Batch 86/100: Batch Loss = 0.0472\n",
            "Iteration 23, Batch 87/100: Batch Loss = 0.2946\n",
            "Iteration 23, Batch 88/100: Batch Loss = 0.1834\n",
            "Iteration 23, Batch 89/100: Batch Loss = 0.1319\n",
            "Iteration 23, Batch 90/100: Batch Loss = 0.1272\n",
            "Iteration 23, Batch 91/100: Batch Loss = 0.0440\n",
            "Iteration 23, Batch 92/100: Batch Loss = 0.1120\n",
            "Iteration 23, Batch 93/100: Batch Loss = 0.2676\n",
            "Iteration 23, Batch 94/100: Batch Loss = 0.1073\n",
            "Iteration 23, Batch 95/100: Batch Loss = 0.1949\n",
            "Iteration 23, Batch 96/100: Batch Loss = 0.0770\n",
            "Iteration 23, Batch 97/100: Batch Loss = 0.0384\n",
            "Iteration 23, Batch 98/100: Batch Loss = 0.0691\n",
            "Iteration 23, Batch 99/100: Batch Loss = 0.1136\n",
            "Iteration 23, Batch 100/100: Batch Loss = 0.0882\n",
            "Iteration 23: Train Loss = 0.1328, Val Loss = 0.1589\n",
            "Iteration 24, Batch 1/100: Batch Loss = 0.0355\n",
            "Iteration 24, Batch 2/100: Batch Loss = 0.3033\n",
            "Iteration 24, Batch 3/100: Batch Loss = 0.0406\n",
            "Iteration 24, Batch 4/100: Batch Loss = 0.1905\n",
            "Iteration 24, Batch 5/100: Batch Loss = 0.0635\n",
            "Iteration 24, Batch 6/100: Batch Loss = 0.0533\n",
            "Iteration 24, Batch 7/100: Batch Loss = 0.1985\n",
            "Iteration 24, Batch 8/100: Batch Loss = 0.0413\n",
            "Iteration 24, Batch 9/100: Batch Loss = 0.0982\n",
            "Iteration 24, Batch 10/100: Batch Loss = 0.1345\n",
            "Iteration 24, Batch 11/100: Batch Loss = 0.0791\n",
            "Iteration 24, Batch 12/100: Batch Loss = 0.1267\n",
            "Iteration 24, Batch 13/100: Batch Loss = 0.3519\n",
            "Iteration 24, Batch 14/100: Batch Loss = 0.1054\n",
            "Iteration 24, Batch 15/100: Batch Loss = 0.4411\n",
            "Iteration 24, Batch 16/100: Batch Loss = 0.1338\n",
            "Iteration 24, Batch 17/100: Batch Loss = 0.0888\n",
            "Iteration 24, Batch 18/100: Batch Loss = 0.2402\n",
            "Iteration 24, Batch 19/100: Batch Loss = 0.1206\n",
            "Iteration 24, Batch 20/100: Batch Loss = 0.1840\n",
            "Iteration 24, Batch 21/100: Batch Loss = 0.2086\n",
            "Iteration 24, Batch 22/100: Batch Loss = 0.1188\n",
            "Iteration 24, Batch 23/100: Batch Loss = 0.1112\n",
            "Iteration 24, Batch 24/100: Batch Loss = 0.0888\n",
            "Iteration 24, Batch 25/100: Batch Loss = 0.1551\n",
            "Iteration 24, Batch 26/100: Batch Loss = 0.1968\n",
            "Iteration 24, Batch 27/100: Batch Loss = 0.0700\n",
            "Iteration 24, Batch 28/100: Batch Loss = 0.2658\n",
            "Iteration 24, Batch 29/100: Batch Loss = 0.3442\n",
            "Iteration 24, Batch 30/100: Batch Loss = 0.1496\n",
            "Iteration 24, Batch 31/100: Batch Loss = 0.1129\n",
            "Iteration 24, Batch 32/100: Batch Loss = 0.0703\n",
            "Iteration 24, Batch 33/100: Batch Loss = 0.0318\n",
            "Iteration 24, Batch 34/100: Batch Loss = 0.0988\n",
            "Iteration 24, Batch 35/100: Batch Loss = 0.3254\n",
            "Iteration 24, Batch 36/100: Batch Loss = 0.0584\n",
            "Iteration 24, Batch 37/100: Batch Loss = 0.0568\n",
            "Iteration 24, Batch 38/100: Batch Loss = 0.2206\n",
            "Iteration 24, Batch 39/100: Batch Loss = 0.0986\n",
            "Iteration 24, Batch 40/100: Batch Loss = 0.0987\n",
            "Iteration 24, Batch 41/100: Batch Loss = 0.0894\n",
            "Iteration 24, Batch 42/100: Batch Loss = 0.1707\n",
            "Iteration 24, Batch 43/100: Batch Loss = 0.0398\n",
            "Iteration 24, Batch 44/100: Batch Loss = 0.0997\n",
            "Iteration 24, Batch 45/100: Batch Loss = 0.0580\n",
            "Iteration 24, Batch 46/100: Batch Loss = 0.0551\n",
            "Iteration 24, Batch 47/100: Batch Loss = 0.1024\n",
            "Iteration 24, Batch 48/100: Batch Loss = 0.1280\n",
            "Iteration 24, Batch 49/100: Batch Loss = 0.0840\n",
            "Iteration 24, Batch 50/100: Batch Loss = 0.0395\n",
            "Iteration 24, Batch 51/100: Batch Loss = 0.1966\n",
            "Iteration 24, Batch 52/100: Batch Loss = 0.1729\n",
            "Iteration 24, Batch 53/100: Batch Loss = 0.0341\n",
            "Iteration 24, Batch 54/100: Batch Loss = 0.0705\n",
            "Iteration 24, Batch 55/100: Batch Loss = 0.2619\n",
            "Iteration 24, Batch 56/100: Batch Loss = 0.1233\n",
            "Iteration 24, Batch 57/100: Batch Loss = 0.0356\n",
            "Iteration 24, Batch 58/100: Batch Loss = 0.0637\n",
            "Iteration 24, Batch 59/100: Batch Loss = 0.0819\n",
            "Iteration 24, Batch 60/100: Batch Loss = 0.0275\n",
            "Iteration 24, Batch 61/100: Batch Loss = 0.0259\n",
            "Iteration 24, Batch 62/100: Batch Loss = 0.0554\n",
            "Iteration 24, Batch 63/100: Batch Loss = 0.0712\n",
            "Iteration 24, Batch 64/100: Batch Loss = 0.2334\n",
            "Iteration 24, Batch 65/100: Batch Loss = 0.2278\n",
            "Iteration 24, Batch 66/100: Batch Loss = 0.0487\n",
            "Iteration 24, Batch 67/100: Batch Loss = 0.0359\n",
            "Iteration 24, Batch 68/100: Batch Loss = 0.0390\n",
            "Iteration 24, Batch 69/100: Batch Loss = 0.2460\n",
            "Iteration 24, Batch 70/100: Batch Loss = 0.1975\n",
            "Iteration 24, Batch 71/100: Batch Loss = 0.1460\n",
            "Iteration 24, Batch 72/100: Batch Loss = 0.1430\n",
            "Iteration 24, Batch 73/100: Batch Loss = 0.2029\n",
            "Iteration 24, Batch 74/100: Batch Loss = 0.2350\n",
            "Iteration 24, Batch 75/100: Batch Loss = 0.0714\n",
            "Iteration 24, Batch 76/100: Batch Loss = 0.1196\n",
            "Iteration 24, Batch 77/100: Batch Loss = 0.0926\n",
            "Iteration 24, Batch 78/100: Batch Loss = 0.1361\n",
            "Iteration 24, Batch 79/100: Batch Loss = 0.0831\n",
            "Iteration 24, Batch 80/100: Batch Loss = 0.0331\n",
            "Iteration 24, Batch 81/100: Batch Loss = 0.2384\n",
            "Iteration 24, Batch 82/100: Batch Loss = 0.0652\n",
            "Iteration 24, Batch 83/100: Batch Loss = 0.1617\n",
            "Iteration 24, Batch 84/100: Batch Loss = 0.1491\n",
            "Iteration 24, Batch 85/100: Batch Loss = 0.3219\n",
            "Iteration 24, Batch 86/100: Batch Loss = 0.0808\n",
            "Iteration 24, Batch 87/100: Batch Loss = 0.0996\n",
            "Iteration 24, Batch 88/100: Batch Loss = 0.2185\n",
            "Iteration 24, Batch 89/100: Batch Loss = 0.0994\n",
            "Iteration 24, Batch 90/100: Batch Loss = 0.2066\n",
            "Iteration 24, Batch 91/100: Batch Loss = 0.0360\n",
            "Iteration 24, Batch 92/100: Batch Loss = 0.3305\n",
            "Iteration 24, Batch 93/100: Batch Loss = 0.0515\n",
            "Iteration 24, Batch 94/100: Batch Loss = 0.2457\n",
            "Iteration 24, Batch 95/100: Batch Loss = 0.1887\n",
            "Iteration 24, Batch 96/100: Batch Loss = 0.0419\n",
            "Iteration 24, Batch 97/100: Batch Loss = 0.1233\n",
            "Iteration 24, Batch 98/100: Batch Loss = 0.1032\n",
            "Iteration 24, Batch 99/100: Batch Loss = 0.0617\n",
            "Iteration 24, Batch 100/100: Batch Loss = 0.0350\n",
            "Iteration 24: Train Loss = 0.1315, Val Loss = 0.1704\n",
            "Iteration 25, Batch 1/100: Batch Loss = 0.2264\n",
            "Iteration 25, Batch 2/100: Batch Loss = 0.1715\n",
            "Iteration 25, Batch 3/100: Batch Loss = 0.0414\n",
            "Iteration 25, Batch 4/100: Batch Loss = 0.0657\n",
            "Iteration 25, Batch 5/100: Batch Loss = 0.0774\n",
            "Iteration 25, Batch 6/100: Batch Loss = 0.1226\n",
            "Iteration 25, Batch 7/100: Batch Loss = 0.1665\n",
            "Iteration 25, Batch 8/100: Batch Loss = 0.0989\n",
            "Iteration 25, Batch 9/100: Batch Loss = 0.0753\n",
            "Iteration 25, Batch 10/100: Batch Loss = 0.0844\n",
            "Iteration 25, Batch 11/100: Batch Loss = 0.1039\n",
            "Iteration 25, Batch 12/100: Batch Loss = 0.0469\n",
            "Iteration 25, Batch 13/100: Batch Loss = 0.0722\n",
            "Iteration 25, Batch 14/100: Batch Loss = 0.1106\n",
            "Iteration 25, Batch 15/100: Batch Loss = 0.1054\n",
            "Iteration 25, Batch 16/100: Batch Loss = 0.1106\n",
            "Iteration 25, Batch 17/100: Batch Loss = 0.1174\n",
            "Iteration 25, Batch 18/100: Batch Loss = 0.2001\n",
            "Iteration 25, Batch 19/100: Batch Loss = 0.1500\n",
            "Iteration 25, Batch 20/100: Batch Loss = 0.2078\n",
            "Iteration 25, Batch 21/100: Batch Loss = 0.1411\n",
            "Iteration 25, Batch 22/100: Batch Loss = 0.2675\n",
            "Iteration 25, Batch 23/100: Batch Loss = 0.3639\n",
            "Iteration 25, Batch 24/100: Batch Loss = 0.0556\n",
            "Iteration 25, Batch 25/100: Batch Loss = 0.1093\n",
            "Iteration 25, Batch 26/100: Batch Loss = 0.1280\n",
            "Iteration 25, Batch 27/100: Batch Loss = 0.2414\n",
            "Iteration 25, Batch 28/100: Batch Loss = 0.0717\n",
            "Iteration 25, Batch 29/100: Batch Loss = 0.0810\n",
            "Iteration 25, Batch 30/100: Batch Loss = 0.1583\n",
            "Iteration 25, Batch 31/100: Batch Loss = 0.1188\n",
            "Iteration 25, Batch 32/100: Batch Loss = 0.0957\n",
            "Iteration 25, Batch 33/100: Batch Loss = 0.1475\n",
            "Iteration 25, Batch 34/100: Batch Loss = 0.1033\n",
            "Iteration 25, Batch 35/100: Batch Loss = 0.0438\n",
            "Iteration 25, Batch 36/100: Batch Loss = 0.2223\n",
            "Iteration 25, Batch 37/100: Batch Loss = 0.0394\n",
            "Iteration 25, Batch 38/100: Batch Loss = 0.1674\n",
            "Iteration 25, Batch 39/100: Batch Loss = 0.1564\n",
            "Iteration 25, Batch 40/100: Batch Loss = 0.1175\n",
            "Iteration 25, Batch 41/100: Batch Loss = 0.0949\n",
            "Iteration 25, Batch 42/100: Batch Loss = 0.1563\n",
            "Iteration 25, Batch 43/100: Batch Loss = 0.0778\n",
            "Iteration 25, Batch 44/100: Batch Loss = 0.1051\n",
            "Iteration 25, Batch 45/100: Batch Loss = 0.0885\n",
            "Iteration 25, Batch 46/100: Batch Loss = 0.0250\n",
            "Iteration 25, Batch 47/100: Batch Loss = 0.0562\n",
            "Iteration 25, Batch 48/100: Batch Loss = 0.1022\n",
            "Iteration 25, Batch 49/100: Batch Loss = 0.0791\n",
            "Iteration 25, Batch 50/100: Batch Loss = 0.1101\n",
            "Iteration 25, Batch 51/100: Batch Loss = 0.0528\n",
            "Iteration 25, Batch 52/100: Batch Loss = 0.1823\n",
            "Iteration 25, Batch 53/100: Batch Loss = 0.0411\n",
            "Iteration 25, Batch 54/100: Batch Loss = 0.0459\n",
            "Iteration 25, Batch 55/100: Batch Loss = 0.1218\n",
            "Iteration 25, Batch 56/100: Batch Loss = 0.1539\n",
            "Iteration 25, Batch 57/100: Batch Loss = 0.1921\n",
            "Iteration 25, Batch 58/100: Batch Loss = 0.3759\n",
            "Iteration 25, Batch 59/100: Batch Loss = 0.2450\n",
            "Iteration 25, Batch 60/100: Batch Loss = 0.1362\n",
            "Iteration 25, Batch 61/100: Batch Loss = 0.1275\n",
            "Iteration 25, Batch 62/100: Batch Loss = 0.1023\n",
            "Iteration 25, Batch 63/100: Batch Loss = 0.1485\n",
            "Iteration 25, Batch 64/100: Batch Loss = 0.1739\n",
            "Iteration 25, Batch 65/100: Batch Loss = 0.0501\n",
            "Iteration 25, Batch 66/100: Batch Loss = 0.0521\n",
            "Iteration 25, Batch 67/100: Batch Loss = 0.1620\n",
            "Iteration 25, Batch 68/100: Batch Loss = 0.0656\n",
            "Iteration 25, Batch 69/100: Batch Loss = 0.1009\n",
            "Iteration 25, Batch 70/100: Batch Loss = 0.0832\n",
            "Iteration 25, Batch 71/100: Batch Loss = 0.0835\n",
            "Iteration 25, Batch 72/100: Batch Loss = 0.1058\n",
            "Iteration 25, Batch 73/100: Batch Loss = 0.0980\n",
            "Iteration 25, Batch 74/100: Batch Loss = 0.1019\n",
            "Iteration 25, Batch 75/100: Batch Loss = 0.1624\n",
            "Iteration 25, Batch 76/100: Batch Loss = 0.1367\n",
            "Iteration 25, Batch 77/100: Batch Loss = 0.0807\n",
            "Iteration 25, Batch 78/100: Batch Loss = 0.1388\n",
            "Iteration 25, Batch 79/100: Batch Loss = 0.2247\n",
            "Iteration 25, Batch 80/100: Batch Loss = 0.2064\n",
            "Iteration 25, Batch 81/100: Batch Loss = 0.0954\n",
            "Iteration 25, Batch 82/100: Batch Loss = 0.2298\n",
            "Iteration 25, Batch 83/100: Batch Loss = 0.1315\n",
            "Iteration 25, Batch 84/100: Batch Loss = 0.3887\n",
            "Iteration 25, Batch 85/100: Batch Loss = 0.1555\n",
            "Iteration 25, Batch 86/100: Batch Loss = 0.0802\n",
            "Iteration 25, Batch 87/100: Batch Loss = 0.3598\n",
            "Iteration 25, Batch 88/100: Batch Loss = 0.1868\n",
            "Iteration 25, Batch 89/100: Batch Loss = 0.2378\n",
            "Iteration 25, Batch 90/100: Batch Loss = 0.1778\n",
            "Iteration 25, Batch 91/100: Batch Loss = 0.1303\n",
            "Iteration 25, Batch 92/100: Batch Loss = 0.1602\n",
            "Iteration 25, Batch 93/100: Batch Loss = 0.0624\n",
            "Iteration 25, Batch 94/100: Batch Loss = 0.0905\n",
            "Iteration 25, Batch 95/100: Batch Loss = 0.3775\n",
            "Iteration 25, Batch 96/100: Batch Loss = 0.0560\n",
            "Iteration 25, Batch 97/100: Batch Loss = 0.1032\n",
            "Iteration 25, Batch 98/100: Batch Loss = 0.0477\n",
            "Iteration 25, Batch 99/100: Batch Loss = 0.1285\n",
            "Iteration 25, Batch 100/100: Batch Loss = 0.1243\n",
            "Iteration 25: Train Loss = 0.1336, Val Loss = 0.1637\n",
            "Iteration 26, Batch 1/100: Batch Loss = 0.2821\n",
            "Iteration 26, Batch 2/100: Batch Loss = 0.2737\n",
            "Iteration 26, Batch 3/100: Batch Loss = 0.1210\n",
            "Iteration 26, Batch 4/100: Batch Loss = 0.1803\n",
            "Iteration 26, Batch 5/100: Batch Loss = 0.1238\n",
            "Iteration 26, Batch 6/100: Batch Loss = 0.0316\n",
            "Iteration 26, Batch 7/100: Batch Loss = 0.1169\n",
            "Iteration 26, Batch 8/100: Batch Loss = 0.1700\n",
            "Iteration 26, Batch 9/100: Batch Loss = 0.2691\n",
            "Iteration 26, Batch 10/100: Batch Loss = 0.0338\n",
            "Iteration 26, Batch 11/100: Batch Loss = 0.0749\n",
            "Iteration 26, Batch 12/100: Batch Loss = 0.2297\n",
            "Iteration 26, Batch 13/100: Batch Loss = 0.3246\n",
            "Iteration 26, Batch 14/100: Batch Loss = 0.1875\n",
            "Iteration 26, Batch 15/100: Batch Loss = 0.0409\n",
            "Iteration 26, Batch 16/100: Batch Loss = 0.0930\n",
            "Iteration 26, Batch 17/100: Batch Loss = 0.1687\n",
            "Iteration 26, Batch 18/100: Batch Loss = 0.0244\n",
            "Iteration 26, Batch 19/100: Batch Loss = 0.0717\n",
            "Iteration 26, Batch 20/100: Batch Loss = 0.0918\n",
            "Iteration 26, Batch 21/100: Batch Loss = 0.0365\n",
            "Iteration 26, Batch 22/100: Batch Loss = 0.0832\n",
            "Iteration 26, Batch 23/100: Batch Loss = 0.1038\n",
            "Iteration 26, Batch 24/100: Batch Loss = 0.3817\n",
            "Iteration 26, Batch 25/100: Batch Loss = 0.0445\n",
            "Iteration 26, Batch 26/100: Batch Loss = 0.0958\n",
            "Iteration 26, Batch 27/100: Batch Loss = 0.0659\n",
            "Iteration 26, Batch 28/100: Batch Loss = 0.1803\n",
            "Iteration 26, Batch 29/100: Batch Loss = 0.1405\n",
            "Iteration 26, Batch 30/100: Batch Loss = 0.0421\n",
            "Iteration 26, Batch 31/100: Batch Loss = 0.0637\n",
            "Iteration 26, Batch 32/100: Batch Loss = 0.2018\n",
            "Iteration 26, Batch 33/100: Batch Loss = 0.1591\n",
            "Iteration 26, Batch 34/100: Batch Loss = 0.0657\n",
            "Iteration 26, Batch 35/100: Batch Loss = 0.3699\n",
            "Iteration 26, Batch 36/100: Batch Loss = 0.0990\n",
            "Iteration 26, Batch 37/100: Batch Loss = 0.0836\n",
            "Iteration 26, Batch 38/100: Batch Loss = 0.1637\n",
            "Iteration 26, Batch 39/100: Batch Loss = 0.1335\n",
            "Iteration 26, Batch 40/100: Batch Loss = 0.1150\n",
            "Iteration 26, Batch 41/100: Batch Loss = 0.1137\n",
            "Iteration 26, Batch 42/100: Batch Loss = 0.1226\n",
            "Iteration 26, Batch 43/100: Batch Loss = 0.1976\n",
            "Iteration 26, Batch 44/100: Batch Loss = 0.0676\n",
            "Iteration 26, Batch 45/100: Batch Loss = 0.0771\n",
            "Iteration 26, Batch 46/100: Batch Loss = 0.1825\n",
            "Iteration 26, Batch 47/100: Batch Loss = 0.0541\n",
            "Iteration 26, Batch 48/100: Batch Loss = 0.0935\n",
            "Iteration 26, Batch 49/100: Batch Loss = 0.0917\n",
            "Iteration 26, Batch 50/100: Batch Loss = 0.2385\n",
            "Iteration 26, Batch 51/100: Batch Loss = 0.0492\n",
            "Iteration 26, Batch 52/100: Batch Loss = 0.2600\n",
            "Iteration 26, Batch 53/100: Batch Loss = 0.0758\n",
            "Iteration 26, Batch 54/100: Batch Loss = 0.0906\n",
            "Iteration 26, Batch 55/100: Batch Loss = 0.4761\n",
            "Iteration 26, Batch 56/100: Batch Loss = 0.0562\n",
            "Iteration 26, Batch 57/100: Batch Loss = 0.1170\n",
            "Iteration 26, Batch 58/100: Batch Loss = 0.1238\n",
            "Iteration 26, Batch 59/100: Batch Loss = 0.2427\n",
            "Iteration 26, Batch 60/100: Batch Loss = 0.0591\n",
            "Iteration 26, Batch 61/100: Batch Loss = 0.0743\n",
            "Iteration 26, Batch 62/100: Batch Loss = 0.3396\n",
            "Iteration 26, Batch 63/100: Batch Loss = 0.1227\n",
            "Iteration 26, Batch 64/100: Batch Loss = 0.0591\n",
            "Iteration 26, Batch 65/100: Batch Loss = 0.0419\n",
            "Iteration 26, Batch 66/100: Batch Loss = 0.0651\n",
            "Iteration 26, Batch 67/100: Batch Loss = 0.0419\n",
            "Iteration 26, Batch 68/100: Batch Loss = 0.1268\n",
            "Iteration 26, Batch 69/100: Batch Loss = 0.1218\n",
            "Iteration 26, Batch 70/100: Batch Loss = 0.0686\n",
            "Iteration 26, Batch 71/100: Batch Loss = 0.1147\n",
            "Iteration 26, Batch 72/100: Batch Loss = 0.1466\n",
            "Iteration 26, Batch 73/100: Batch Loss = 0.3160\n",
            "Iteration 26, Batch 74/100: Batch Loss = 0.1275\n",
            "Iteration 26, Batch 75/100: Batch Loss = 0.0327\n",
            "Iteration 26, Batch 76/100: Batch Loss = 0.1000\n",
            "Iteration 26, Batch 77/100: Batch Loss = 0.2186\n",
            "Iteration 26, Batch 78/100: Batch Loss = 0.1516\n",
            "Iteration 26, Batch 79/100: Batch Loss = 0.0942\n",
            "Iteration 26, Batch 80/100: Batch Loss = 0.0443\n",
            "Iteration 26, Batch 81/100: Batch Loss = 0.1339\n",
            "Iteration 26, Batch 82/100: Batch Loss = 0.0167\n",
            "Iteration 26, Batch 83/100: Batch Loss = 0.1920\n",
            "Iteration 26, Batch 84/100: Batch Loss = 0.1305\n",
            "Iteration 26, Batch 85/100: Batch Loss = 0.1988\n",
            "Iteration 26, Batch 86/100: Batch Loss = 0.2411\n",
            "Iteration 26, Batch 87/100: Batch Loss = 0.0516\n",
            "Iteration 26, Batch 88/100: Batch Loss = 0.0974\n",
            "Iteration 26, Batch 89/100: Batch Loss = 0.2856\n",
            "Iteration 26, Batch 90/100: Batch Loss = 0.0279\n",
            "Iteration 26, Batch 91/100: Batch Loss = 0.2409\n",
            "Iteration 26, Batch 92/100: Batch Loss = 0.2394\n",
            "Iteration 26, Batch 93/100: Batch Loss = 0.0565\n",
            "Iteration 26, Batch 94/100: Batch Loss = 0.1023\n",
            "Iteration 26, Batch 95/100: Batch Loss = 0.0821\n",
            "Iteration 26, Batch 96/100: Batch Loss = 0.0494\n",
            "Iteration 26, Batch 97/100: Batch Loss = 0.2255\n",
            "Iteration 26, Batch 98/100: Batch Loss = 0.1131\n",
            "Iteration 26, Batch 99/100: Batch Loss = 0.0881\n",
            "Iteration 26, Batch 100/100: Batch Loss = 0.0223\n",
            "Iteration 26: Train Loss = 0.1333, Val Loss = 0.1576\n",
            "Iteration 27, Batch 1/100: Batch Loss = 0.0808\n",
            "Iteration 27, Batch 2/100: Batch Loss = 0.3489\n",
            "Iteration 27, Batch 3/100: Batch Loss = 0.0563\n",
            "Iteration 27, Batch 4/100: Batch Loss = 0.0529\n",
            "Iteration 27, Batch 5/100: Batch Loss = 0.0356\n",
            "Iteration 27, Batch 6/100: Batch Loss = 0.0777\n",
            "Iteration 27, Batch 7/100: Batch Loss = 0.0992\n",
            "Iteration 27, Batch 8/100: Batch Loss = 0.1990\n",
            "Iteration 27, Batch 9/100: Batch Loss = 0.1136\n",
            "Iteration 27, Batch 10/100: Batch Loss = 0.1288\n",
            "Iteration 27, Batch 11/100: Batch Loss = 0.2868\n",
            "Iteration 27, Batch 12/100: Batch Loss = 0.0816\n",
            "Iteration 27, Batch 13/100: Batch Loss = 0.1529\n",
            "Iteration 27, Batch 14/100: Batch Loss = 0.1792\n",
            "Iteration 27, Batch 15/100: Batch Loss = 0.0496\n",
            "Iteration 27, Batch 16/100: Batch Loss = 0.1908\n",
            "Iteration 27, Batch 17/100: Batch Loss = 0.1700\n",
            "Iteration 27, Batch 18/100: Batch Loss = 0.0310\n",
            "Iteration 27, Batch 19/100: Batch Loss = 0.0670\n",
            "Iteration 27, Batch 20/100: Batch Loss = 0.0489\n",
            "Iteration 27, Batch 21/100: Batch Loss = 0.1258\n",
            "Iteration 27, Batch 22/100: Batch Loss = 0.1738\n",
            "Iteration 27, Batch 23/100: Batch Loss = 0.0562\n",
            "Iteration 27, Batch 24/100: Batch Loss = 0.0728\n",
            "Iteration 27, Batch 25/100: Batch Loss = 0.0584\n",
            "Iteration 27, Batch 26/100: Batch Loss = 0.0911\n",
            "Iteration 27, Batch 27/100: Batch Loss = 0.1676\n",
            "Iteration 27, Batch 28/100: Batch Loss = 0.1685\n",
            "Iteration 27, Batch 29/100: Batch Loss = 0.1746\n",
            "Iteration 27, Batch 30/100: Batch Loss = 0.1851\n",
            "Iteration 27, Batch 31/100: Batch Loss = 0.1288\n",
            "Iteration 27, Batch 32/100: Batch Loss = 0.1195\n",
            "Iteration 27, Batch 33/100: Batch Loss = 0.0828\n",
            "Iteration 27, Batch 34/100: Batch Loss = 0.0564\n",
            "Iteration 27, Batch 35/100: Batch Loss = 0.1408\n",
            "Iteration 27, Batch 36/100: Batch Loss = 0.0916\n",
            "Iteration 27, Batch 37/100: Batch Loss = 0.1030\n",
            "Iteration 27, Batch 38/100: Batch Loss = 0.0554\n",
            "Iteration 27, Batch 39/100: Batch Loss = 0.1451\n",
            "Iteration 27, Batch 40/100: Batch Loss = 0.0514\n",
            "Iteration 27, Batch 41/100: Batch Loss = 0.1953\n",
            "Iteration 27, Batch 42/100: Batch Loss = 0.0695\n",
            "Iteration 27, Batch 43/100: Batch Loss = 0.0362\n",
            "Iteration 27, Batch 44/100: Batch Loss = 0.0427\n",
            "Iteration 27, Batch 45/100: Batch Loss = 0.0769\n",
            "Iteration 27, Batch 46/100: Batch Loss = 0.1933\n",
            "Iteration 27, Batch 47/100: Batch Loss = 0.2933\n",
            "Iteration 27, Batch 48/100: Batch Loss = 0.2080\n",
            "Iteration 27, Batch 49/100: Batch Loss = 0.0710\n",
            "Iteration 27, Batch 50/100: Batch Loss = 0.1039\n",
            "Iteration 27, Batch 51/100: Batch Loss = 0.0476\n",
            "Iteration 27, Batch 52/100: Batch Loss = 0.2480\n",
            "Iteration 27, Batch 53/100: Batch Loss = 0.0476\n",
            "Iteration 27, Batch 54/100: Batch Loss = 0.1882\n",
            "Iteration 27, Batch 55/100: Batch Loss = 0.2125\n",
            "Iteration 27, Batch 56/100: Batch Loss = 0.1839\n",
            "Iteration 27, Batch 57/100: Batch Loss = 0.2650\n",
            "Iteration 27, Batch 58/100: Batch Loss = 0.1415\n",
            "Iteration 27, Batch 59/100: Batch Loss = 0.2600\n",
            "Iteration 27, Batch 60/100: Batch Loss = 0.1158\n",
            "Iteration 27, Batch 61/100: Batch Loss = 0.0960\n",
            "Iteration 27, Batch 62/100: Batch Loss = 0.0497\n",
            "Iteration 27, Batch 63/100: Batch Loss = 0.1048\n",
            "Iteration 27, Batch 64/100: Batch Loss = 0.0675\n",
            "Iteration 27, Batch 65/100: Batch Loss = 0.1268\n",
            "Iteration 27, Batch 66/100: Batch Loss = 0.1160\n",
            "Iteration 27, Batch 67/100: Batch Loss = 0.1148\n",
            "Iteration 27, Batch 68/100: Batch Loss = 0.1524\n",
            "Iteration 27, Batch 69/100: Batch Loss = 0.1278\n",
            "Iteration 27, Batch 70/100: Batch Loss = 0.2037\n",
            "Iteration 27, Batch 71/100: Batch Loss = 0.1364\n",
            "Iteration 27, Batch 72/100: Batch Loss = 0.1424\n",
            "Iteration 27, Batch 73/100: Batch Loss = 0.0674\n",
            "Iteration 27, Batch 74/100: Batch Loss = 0.2308\n",
            "Iteration 27, Batch 75/100: Batch Loss = 0.0303\n",
            "Iteration 27, Batch 76/100: Batch Loss = 0.0728\n",
            "Iteration 27, Batch 77/100: Batch Loss = 0.0577\n",
            "Iteration 27, Batch 78/100: Batch Loss = 0.2551\n",
            "Iteration 27, Batch 79/100: Batch Loss = 0.0997\n",
            "Iteration 27, Batch 80/100: Batch Loss = 0.1492\n",
            "Iteration 27, Batch 81/100: Batch Loss = 0.0504\n",
            "Iteration 27, Batch 82/100: Batch Loss = 0.0715\n",
            "Iteration 27, Batch 83/100: Batch Loss = 0.2955\n",
            "Iteration 27, Batch 84/100: Batch Loss = 0.0497\n",
            "Iteration 27, Batch 85/100: Batch Loss = 0.2940\n",
            "Iteration 27, Batch 86/100: Batch Loss = 0.1622\n",
            "Iteration 27, Batch 87/100: Batch Loss = 0.1731\n",
            "Iteration 27, Batch 88/100: Batch Loss = 0.1391\n",
            "Iteration 27, Batch 89/100: Batch Loss = 0.1289\n",
            "Iteration 27, Batch 90/100: Batch Loss = 0.1610\n",
            "Iteration 27, Batch 91/100: Batch Loss = 0.0675\n",
            "Iteration 27, Batch 92/100: Batch Loss = 0.1026\n",
            "Iteration 27, Batch 93/100: Batch Loss = 0.0550\n",
            "Iteration 27, Batch 94/100: Batch Loss = 0.2632\n",
            "Iteration 27, Batch 95/100: Batch Loss = 0.2334\n",
            "Iteration 27, Batch 96/100: Batch Loss = 0.2051\n",
            "Iteration 27, Batch 97/100: Batch Loss = 0.1297\n",
            "Iteration 27, Batch 98/100: Batch Loss = 0.1866\n",
            "Iteration 27, Batch 99/100: Batch Loss = 0.1785\n",
            "Iteration 27, Batch 100/100: Batch Loss = 0.0594\n",
            "Iteration 27: Train Loss = 0.1311, Val Loss = 0.1678\n",
            "Iteration 28, Batch 1/100: Batch Loss = 0.0983\n",
            "Iteration 28, Batch 2/100: Batch Loss = 0.2171\n",
            "Iteration 28, Batch 3/100: Batch Loss = 0.0318\n",
            "Iteration 28, Batch 4/100: Batch Loss = 0.0562\n",
            "Iteration 28, Batch 5/100: Batch Loss = 0.0500\n",
            "Iteration 28, Batch 6/100: Batch Loss = 0.1555\n",
            "Iteration 28, Batch 7/100: Batch Loss = 0.0579\n",
            "Iteration 28, Batch 8/100: Batch Loss = 0.0675\n",
            "Iteration 28, Batch 9/100: Batch Loss = 0.1731\n",
            "Iteration 28, Batch 10/100: Batch Loss = 0.1269\n",
            "Iteration 28, Batch 11/100: Batch Loss = 0.1242\n",
            "Iteration 28, Batch 12/100: Batch Loss = 0.2163\n",
            "Iteration 28, Batch 13/100: Batch Loss = 0.6416\n",
            "Iteration 28, Batch 14/100: Batch Loss = 0.0767\n",
            "Iteration 28, Batch 15/100: Batch Loss = 0.0693\n",
            "Iteration 28, Batch 16/100: Batch Loss = 0.0878\n",
            "Iteration 28, Batch 17/100: Batch Loss = 0.2362\n",
            "Iteration 28, Batch 18/100: Batch Loss = 0.1014\n",
            "Iteration 28, Batch 19/100: Batch Loss = 0.2904\n",
            "Iteration 28, Batch 20/100: Batch Loss = 0.1597\n",
            "Iteration 28, Batch 21/100: Batch Loss = 0.2250\n",
            "Iteration 28, Batch 22/100: Batch Loss = 0.1189\n",
            "Iteration 28, Batch 23/100: Batch Loss = 0.0776\n",
            "Iteration 28, Batch 24/100: Batch Loss = 0.0899\n",
            "Iteration 28, Batch 25/100: Batch Loss = 0.0802\n",
            "Iteration 28, Batch 26/100: Batch Loss = 0.2511\n",
            "Iteration 28, Batch 27/100: Batch Loss = 0.1458\n",
            "Iteration 28, Batch 28/100: Batch Loss = 0.1219\n",
            "Iteration 28, Batch 29/100: Batch Loss = 0.1121\n",
            "Iteration 28, Batch 30/100: Batch Loss = 0.0673\n",
            "Iteration 28, Batch 31/100: Batch Loss = 0.0931\n",
            "Iteration 28, Batch 32/100: Batch Loss = 0.0559\n",
            "Iteration 28, Batch 33/100: Batch Loss = 0.1019\n",
            "Iteration 28, Batch 34/100: Batch Loss = 0.1534\n",
            "Iteration 28, Batch 35/100: Batch Loss = 0.0792\n",
            "Iteration 28, Batch 36/100: Batch Loss = 0.1079\n",
            "Iteration 28, Batch 37/100: Batch Loss = 0.1946\n",
            "Iteration 28, Batch 38/100: Batch Loss = 0.0947\n",
            "Iteration 28, Batch 39/100: Batch Loss = 0.0870\n",
            "Iteration 28, Batch 40/100: Batch Loss = 0.2294\n",
            "Iteration 28, Batch 41/100: Batch Loss = 0.2032\n",
            "Iteration 28, Batch 42/100: Batch Loss = 0.0332\n",
            "Iteration 28, Batch 43/100: Batch Loss = 0.1851\n",
            "Iteration 28, Batch 44/100: Batch Loss = 0.1114\n",
            "Iteration 28, Batch 45/100: Batch Loss = 0.1818\n",
            "Iteration 28, Batch 46/100: Batch Loss = 0.0676\n",
            "Iteration 28, Batch 47/100: Batch Loss = 0.1627\n",
            "Iteration 28, Batch 48/100: Batch Loss = 0.1463\n",
            "Iteration 28, Batch 49/100: Batch Loss = 0.1488\n",
            "Iteration 28, Batch 50/100: Batch Loss = 0.0609\n",
            "Iteration 28, Batch 51/100: Batch Loss = 0.0911\n",
            "Iteration 28, Batch 52/100: Batch Loss = 0.0642\n",
            "Iteration 28, Batch 53/100: Batch Loss = 0.0351\n",
            "Iteration 28, Batch 54/100: Batch Loss = 0.3541\n",
            "Iteration 28, Batch 55/100: Batch Loss = 0.0987\n",
            "Iteration 28, Batch 56/100: Batch Loss = 0.2633\n",
            "Iteration 28, Batch 57/100: Batch Loss = 0.0971\n",
            "Iteration 28, Batch 58/100: Batch Loss = 0.1159\n",
            "Iteration 28, Batch 59/100: Batch Loss = 0.1002\n",
            "Iteration 28, Batch 60/100: Batch Loss = 0.0722\n",
            "Iteration 28, Batch 61/100: Batch Loss = 0.2196\n",
            "Iteration 28, Batch 62/100: Batch Loss = 0.0885\n",
            "Iteration 28, Batch 63/100: Batch Loss = 0.1905\n",
            "Iteration 28, Batch 64/100: Batch Loss = 0.0774\n",
            "Iteration 28, Batch 65/100: Batch Loss = 0.1430\n",
            "Iteration 28, Batch 66/100: Batch Loss = 0.0453\n",
            "Iteration 28, Batch 67/100: Batch Loss = 0.0676\n",
            "Iteration 28, Batch 68/100: Batch Loss = 0.1373\n",
            "Iteration 28, Batch 69/100: Batch Loss = 0.2090\n",
            "Iteration 28, Batch 70/100: Batch Loss = 0.0477\n",
            "Iteration 28, Batch 71/100: Batch Loss = 0.0759\n",
            "Iteration 28, Batch 72/100: Batch Loss = 0.0713\n",
            "Iteration 28, Batch 73/100: Batch Loss = 0.1392\n",
            "Iteration 28, Batch 74/100: Batch Loss = 0.0535\n",
            "Iteration 28, Batch 75/100: Batch Loss = 0.0723\n",
            "Iteration 28, Batch 76/100: Batch Loss = 0.1740\n",
            "Iteration 28, Batch 77/100: Batch Loss = 0.0422\n",
            "Iteration 28, Batch 78/100: Batch Loss = 0.1006\n",
            "Iteration 28, Batch 79/100: Batch Loss = 0.2591\n",
            "Iteration 28, Batch 80/100: Batch Loss = 0.2314\n",
            "Iteration 28, Batch 81/100: Batch Loss = 0.2010\n",
            "Iteration 28, Batch 82/100: Batch Loss = 0.1003\n",
            "Iteration 28, Batch 83/100: Batch Loss = 0.0214\n",
            "Iteration 28, Batch 84/100: Batch Loss = 0.1312\n",
            "Iteration 28, Batch 85/100: Batch Loss = 0.0793\n",
            "Iteration 28, Batch 86/100: Batch Loss = 0.1227\n",
            "Iteration 28, Batch 87/100: Batch Loss = 0.0805\n",
            "Iteration 28, Batch 88/100: Batch Loss = 0.0556\n",
            "Iteration 28, Batch 89/100: Batch Loss = 0.0456\n",
            "Iteration 28, Batch 90/100: Batch Loss = 0.0903\n",
            "Iteration 28, Batch 91/100: Batch Loss = 0.1859\n",
            "Iteration 28, Batch 92/100: Batch Loss = 0.2909\n",
            "Iteration 28, Batch 93/100: Batch Loss = 0.1442\n",
            "Iteration 28, Batch 94/100: Batch Loss = 0.1278\n",
            "Iteration 28, Batch 95/100: Batch Loss = 0.1600\n",
            "Iteration 28, Batch 96/100: Batch Loss = 0.1128\n",
            "Iteration 28, Batch 97/100: Batch Loss = 0.0662\n",
            "Iteration 28, Batch 98/100: Batch Loss = 0.2231\n",
            "Iteration 28, Batch 99/100: Batch Loss = 0.0344\n",
            "Iteration 28, Batch 100/100: Batch Loss = 0.0127\n",
            "Iteration 28: Train Loss = 0.1285, Val Loss = 0.1606\n",
            "Iteration 29, Batch 1/100: Batch Loss = 0.0404\n",
            "Iteration 29, Batch 2/100: Batch Loss = 0.1031\n",
            "Iteration 29, Batch 3/100: Batch Loss = 0.0618\n",
            "Iteration 29, Batch 4/100: Batch Loss = 0.1533\n",
            "Iteration 29, Batch 5/100: Batch Loss = 0.0563\n",
            "Iteration 29, Batch 6/100: Batch Loss = 0.0950\n",
            "Iteration 29, Batch 7/100: Batch Loss = 0.1082\n",
            "Iteration 29, Batch 8/100: Batch Loss = 0.1724\n",
            "Iteration 29, Batch 9/100: Batch Loss = 0.0329\n",
            "Iteration 29, Batch 10/100: Batch Loss = 0.1105\n",
            "Iteration 29, Batch 11/100: Batch Loss = 0.1338\n",
            "Iteration 29, Batch 12/100: Batch Loss = 0.1002\n",
            "Iteration 29, Batch 13/100: Batch Loss = 0.1302\n",
            "Iteration 29, Batch 14/100: Batch Loss = 0.2128\n",
            "Iteration 29, Batch 15/100: Batch Loss = 0.0878\n",
            "Iteration 29, Batch 16/100: Batch Loss = 0.1417\n",
            "Iteration 29, Batch 17/100: Batch Loss = 0.0855\n",
            "Iteration 29, Batch 18/100: Batch Loss = 0.1652\n",
            "Iteration 29, Batch 19/100: Batch Loss = 0.0984\n",
            "Iteration 29, Batch 20/100: Batch Loss = 0.0534\n",
            "Iteration 29, Batch 21/100: Batch Loss = 0.1801\n",
            "Iteration 29, Batch 22/100: Batch Loss = 0.1200\n",
            "Iteration 29, Batch 23/100: Batch Loss = 0.1186\n",
            "Iteration 29, Batch 24/100: Batch Loss = 0.1780\n",
            "Iteration 29, Batch 25/100: Batch Loss = 0.2325\n",
            "Iteration 29, Batch 26/100: Batch Loss = 0.1837\n",
            "Iteration 29, Batch 27/100: Batch Loss = 0.0605\n",
            "Iteration 29, Batch 28/100: Batch Loss = 0.1571\n",
            "Iteration 29, Batch 29/100: Batch Loss = 0.0306\n",
            "Iteration 29, Batch 30/100: Batch Loss = 0.1366\n",
            "Iteration 29, Batch 31/100: Batch Loss = 0.1643\n",
            "Iteration 29, Batch 32/100: Batch Loss = 0.2364\n",
            "Iteration 29, Batch 33/100: Batch Loss = 0.3349\n",
            "Iteration 29, Batch 34/100: Batch Loss = 0.1534\n",
            "Iteration 29, Batch 35/100: Batch Loss = 0.2067\n",
            "Iteration 29, Batch 36/100: Batch Loss = 0.1446\n",
            "Iteration 29, Batch 37/100: Batch Loss = 0.1234\n",
            "Iteration 29, Batch 38/100: Batch Loss = 0.1052\n",
            "Iteration 29, Batch 39/100: Batch Loss = 0.1166\n",
            "Iteration 29, Batch 40/100: Batch Loss = 0.1085\n",
            "Iteration 29, Batch 41/100: Batch Loss = 0.1821\n",
            "Iteration 29, Batch 42/100: Batch Loss = 0.2861\n",
            "Iteration 29, Batch 43/100: Batch Loss = 0.1141\n",
            "Iteration 29, Batch 44/100: Batch Loss = 0.1698\n",
            "Iteration 29, Batch 45/100: Batch Loss = 0.0835\n",
            "Iteration 29, Batch 46/100: Batch Loss = 0.2325\n",
            "Iteration 29, Batch 47/100: Batch Loss = 0.1552\n",
            "Iteration 29, Batch 48/100: Batch Loss = 0.0696\n",
            "Iteration 29, Batch 49/100: Batch Loss = 0.2831\n",
            "Iteration 29, Batch 50/100: Batch Loss = 0.0457\n",
            "Iteration 29, Batch 51/100: Batch Loss = 0.1798\n",
            "Iteration 29, Batch 52/100: Batch Loss = 0.1096\n",
            "Iteration 29, Batch 53/100: Batch Loss = 0.1872\n",
            "Iteration 29, Batch 54/100: Batch Loss = 0.0893\n",
            "Iteration 29, Batch 55/100: Batch Loss = 0.2394\n",
            "Iteration 29, Batch 56/100: Batch Loss = 0.0670\n",
            "Iteration 29, Batch 57/100: Batch Loss = 0.2849\n",
            "Iteration 29, Batch 58/100: Batch Loss = 0.1380\n",
            "Iteration 29, Batch 59/100: Batch Loss = 0.0206\n",
            "Iteration 29, Batch 60/100: Batch Loss = 0.1011\n",
            "Iteration 29, Batch 61/100: Batch Loss = 0.0858\n",
            "Iteration 29, Batch 62/100: Batch Loss = 0.0757\n",
            "Iteration 29, Batch 63/100: Batch Loss = 0.1092\n",
            "Iteration 29, Batch 64/100: Batch Loss = 0.0271\n",
            "Iteration 29, Batch 65/100: Batch Loss = 0.0535\n",
            "Iteration 29, Batch 66/100: Batch Loss = 0.1095\n",
            "Iteration 29, Batch 67/100: Batch Loss = 0.2078\n",
            "Iteration 29, Batch 68/100: Batch Loss = 0.0785\n",
            "Iteration 29, Batch 69/100: Batch Loss = 0.1241\n",
            "Iteration 29, Batch 70/100: Batch Loss = 0.1371\n",
            "Iteration 29, Batch 71/100: Batch Loss = 0.0973\n",
            "Iteration 29, Batch 72/100: Batch Loss = 0.1101\n",
            "Iteration 29, Batch 73/100: Batch Loss = 0.0665\n",
            "Iteration 29, Batch 74/100: Batch Loss = 0.0371\n",
            "Iteration 29, Batch 75/100: Batch Loss = 0.0528\n",
            "Iteration 29, Batch 76/100: Batch Loss = 0.2604\n",
            "Iteration 29, Batch 77/100: Batch Loss = 0.2957\n",
            "Iteration 29, Batch 78/100: Batch Loss = 0.2725\n",
            "Iteration 29, Batch 79/100: Batch Loss = 0.0980\n",
            "Iteration 29, Batch 80/100: Batch Loss = 0.0790\n",
            "Iteration 29, Batch 81/100: Batch Loss = 0.0562\n",
            "Iteration 29, Batch 82/100: Batch Loss = 0.0674\n",
            "Iteration 29, Batch 83/100: Batch Loss = 0.1455\n",
            "Iteration 29, Batch 84/100: Batch Loss = 0.0810\n",
            "Iteration 29, Batch 85/100: Batch Loss = 0.0949\n",
            "Iteration 29, Batch 86/100: Batch Loss = 0.2838\n",
            "Iteration 29, Batch 87/100: Batch Loss = 0.0742\n",
            "Iteration 29, Batch 88/100: Batch Loss = 0.0874\n",
            "Iteration 29, Batch 89/100: Batch Loss = 0.0387\n",
            "Iteration 29, Batch 90/100: Batch Loss = 0.1479\n",
            "Iteration 29, Batch 91/100: Batch Loss = 0.0527\n",
            "Iteration 29, Batch 92/100: Batch Loss = 0.0853\n",
            "Iteration 29, Batch 93/100: Batch Loss = 0.1115\n",
            "Iteration 29, Batch 94/100: Batch Loss = 0.0409\n",
            "Iteration 29, Batch 95/100: Batch Loss = 0.3578\n",
            "Iteration 29, Batch 96/100: Batch Loss = 0.1161\n",
            "Iteration 29, Batch 97/100: Batch Loss = 0.0147\n",
            "Iteration 29, Batch 98/100: Batch Loss = 0.2539\n",
            "Iteration 29, Batch 99/100: Batch Loss = 0.0197\n",
            "Iteration 29, Batch 100/100: Batch Loss = 0.0654\n",
            "Iteration 29: Train Loss = 0.1285, Val Loss = 0.1606\n",
            "Iteration 30, Batch 1/100: Batch Loss = 0.1035\n",
            "Iteration 30, Batch 2/100: Batch Loss = 0.1202\n",
            "Iteration 30, Batch 3/100: Batch Loss = 0.0724\n",
            "Iteration 30, Batch 4/100: Batch Loss = 0.0320\n",
            "Iteration 30, Batch 5/100: Batch Loss = 0.0551\n",
            "Iteration 30, Batch 6/100: Batch Loss = 0.0531\n",
            "Iteration 30, Batch 7/100: Batch Loss = 0.0453\n",
            "Iteration 30, Batch 8/100: Batch Loss = 0.0433\n",
            "Iteration 30, Batch 9/100: Batch Loss = 0.0516\n",
            "Iteration 30, Batch 10/100: Batch Loss = 0.2341\n",
            "Iteration 30, Batch 11/100: Batch Loss = 0.0858\n",
            "Iteration 30, Batch 12/100: Batch Loss = 0.1606\n",
            "Iteration 30, Batch 13/100: Batch Loss = 0.1902\n",
            "Iteration 30, Batch 14/100: Batch Loss = 0.0669\n",
            "Iteration 30, Batch 15/100: Batch Loss = 0.0536\n",
            "Iteration 30, Batch 16/100: Batch Loss = 0.0582\n",
            "Iteration 30, Batch 17/100: Batch Loss = 0.0673\n",
            "Iteration 30, Batch 18/100: Batch Loss = 0.0576\n",
            "Iteration 30, Batch 19/100: Batch Loss = 0.0751\n",
            "Iteration 30, Batch 20/100: Batch Loss = 0.2817\n",
            "Iteration 30, Batch 21/100: Batch Loss = 0.1853\n",
            "Iteration 30, Batch 22/100: Batch Loss = 0.1187\n",
            "Iteration 30, Batch 23/100: Batch Loss = 0.1956\n",
            "Iteration 30, Batch 24/100: Batch Loss = 0.1961\n",
            "Iteration 30, Batch 25/100: Batch Loss = 0.1788\n",
            "Iteration 30, Batch 26/100: Batch Loss = 0.1580\n",
            "Iteration 30, Batch 27/100: Batch Loss = 0.0925\n",
            "Iteration 30, Batch 28/100: Batch Loss = 0.2935\n",
            "Iteration 30, Batch 29/100: Batch Loss = 0.0491\n",
            "Iteration 30, Batch 30/100: Batch Loss = 0.0423\n",
            "Iteration 30, Batch 31/100: Batch Loss = 0.0469\n",
            "Iteration 30, Batch 32/100: Batch Loss = 0.2349\n",
            "Iteration 30, Batch 33/100: Batch Loss = 0.0423\n",
            "Iteration 30, Batch 34/100: Batch Loss = 0.2258\n",
            "Iteration 30, Batch 35/100: Batch Loss = 0.0696\n",
            "Iteration 30, Batch 36/100: Batch Loss = 0.1798\n",
            "Iteration 30, Batch 37/100: Batch Loss = 0.1555\n",
            "Iteration 30, Batch 38/100: Batch Loss = 0.0705\n",
            "Iteration 30, Batch 39/100: Batch Loss = 0.1454\n",
            "Iteration 30, Batch 40/100: Batch Loss = 0.0779\n",
            "Iteration 30, Batch 41/100: Batch Loss = 0.2893\n",
            "Iteration 30, Batch 42/100: Batch Loss = 0.1549\n",
            "Iteration 30, Batch 43/100: Batch Loss = 0.2104\n",
            "Iteration 30, Batch 44/100: Batch Loss = 0.1177\n",
            "Iteration 30, Batch 45/100: Batch Loss = 0.0462\n",
            "Iteration 30, Batch 46/100: Batch Loss = 0.0634\n",
            "Iteration 30, Batch 47/100: Batch Loss = 0.1399\n",
            "Iteration 30, Batch 48/100: Batch Loss = 0.0382\n",
            "Iteration 30, Batch 49/100: Batch Loss = 0.1700\n",
            "Iteration 30, Batch 50/100: Batch Loss = 0.0819\n",
            "Iteration 30, Batch 51/100: Batch Loss = 0.1456\n",
            "Iteration 30, Batch 52/100: Batch Loss = 0.0218\n",
            "Iteration 30, Batch 53/100: Batch Loss = 0.1162\n",
            "Iteration 30, Batch 54/100: Batch Loss = 0.3370\n",
            "Iteration 30, Batch 55/100: Batch Loss = 0.3510\n",
            "Iteration 30, Batch 56/100: Batch Loss = 0.0401\n",
            "Iteration 30, Batch 57/100: Batch Loss = 0.0410\n",
            "Iteration 30, Batch 58/100: Batch Loss = 0.1001\n",
            "Iteration 30, Batch 59/100: Batch Loss = 0.1604\n",
            "Iteration 30, Batch 60/100: Batch Loss = 0.0689\n",
            "Iteration 30, Batch 61/100: Batch Loss = 0.0290\n",
            "Iteration 30, Batch 62/100: Batch Loss = 0.2735\n",
            "Iteration 30, Batch 63/100: Batch Loss = 0.1493\n",
            "Iteration 30, Batch 64/100: Batch Loss = 0.1195\n",
            "Iteration 30, Batch 65/100: Batch Loss = 0.1687\n",
            "Iteration 30, Batch 66/100: Batch Loss = 0.0975\n",
            "Iteration 30, Batch 67/100: Batch Loss = 0.2085\n",
            "Iteration 30, Batch 68/100: Batch Loss = 0.0827\n",
            "Iteration 30, Batch 69/100: Batch Loss = 0.1984\n",
            "Iteration 30, Batch 70/100: Batch Loss = 0.0766\n",
            "Iteration 30, Batch 71/100: Batch Loss = 0.0753\n",
            "Iteration 30, Batch 72/100: Batch Loss = 0.0735\n",
            "Iteration 30, Batch 73/100: Batch Loss = 0.1642\n",
            "Iteration 30, Batch 74/100: Batch Loss = 0.0549\n",
            "Iteration 30, Batch 75/100: Batch Loss = 0.1453\n",
            "Iteration 30, Batch 76/100: Batch Loss = 0.1665\n",
            "Iteration 30, Batch 77/100: Batch Loss = 0.1230\n",
            "Iteration 30, Batch 78/100: Batch Loss = 0.1365\n",
            "Iteration 30, Batch 79/100: Batch Loss = 0.1741\n",
            "Iteration 30, Batch 80/100: Batch Loss = 0.2048\n",
            "Iteration 30, Batch 81/100: Batch Loss = 0.0755\n",
            "Iteration 30, Batch 82/100: Batch Loss = 0.0317\n",
            "Iteration 30, Batch 83/100: Batch Loss = 0.0724\n",
            "Iteration 30, Batch 84/100: Batch Loss = 0.2834\n",
            "Iteration 30, Batch 85/100: Batch Loss = 0.2183\n",
            "Iteration 30, Batch 86/100: Batch Loss = 0.1946\n",
            "Iteration 30, Batch 87/100: Batch Loss = 0.0854\n",
            "Iteration 30, Batch 88/100: Batch Loss = 0.0681\n",
            "Iteration 30, Batch 89/100: Batch Loss = 0.0490\n",
            "Iteration 30, Batch 90/100: Batch Loss = 0.2398\n",
            "Iteration 30, Batch 91/100: Batch Loss = 0.2867\n",
            "Iteration 30, Batch 92/100: Batch Loss = 0.0458\n",
            "Iteration 30, Batch 93/100: Batch Loss = 0.1471\n",
            "Iteration 30, Batch 94/100: Batch Loss = 0.2325\n",
            "Iteration 30, Batch 95/100: Batch Loss = 0.1387\n",
            "Iteration 30, Batch 96/100: Batch Loss = 0.2716\n",
            "Iteration 30, Batch 97/100: Batch Loss = 0.2577\n",
            "Iteration 30, Batch 98/100: Batch Loss = 0.2667\n",
            "Iteration 30, Batch 99/100: Batch Loss = 0.0958\n",
            "Iteration 30, Batch 100/100: Batch Loss = 0.1334\n",
            "Iteration 30: Train Loss = 0.1328, Val Loss = 0.1596\n",
            "Iteration 31, Batch 1/100: Batch Loss = 0.1755\n",
            "Iteration 31, Batch 2/100: Batch Loss = 0.1610\n",
            "Iteration 31, Batch 3/100: Batch Loss = 0.0093\n",
            "Iteration 31, Batch 4/100: Batch Loss = 0.0741\n",
            "Iteration 31, Batch 5/100: Batch Loss = 0.1274\n",
            "Iteration 31, Batch 6/100: Batch Loss = 0.1260\n",
            "Iteration 31, Batch 7/100: Batch Loss = 0.1537\n",
            "Iteration 31, Batch 8/100: Batch Loss = 0.2068\n",
            "Iteration 31, Batch 9/100: Batch Loss = 0.0544\n",
            "Iteration 31, Batch 10/100: Batch Loss = 0.0589\n",
            "Iteration 31, Batch 11/100: Batch Loss = 0.0465\n",
            "Iteration 31, Batch 12/100: Batch Loss = 0.0776\n",
            "Iteration 31, Batch 13/100: Batch Loss = 0.0862\n",
            "Iteration 31, Batch 14/100: Batch Loss = 0.0252\n",
            "Iteration 31, Batch 15/100: Batch Loss = 0.0855\n",
            "Iteration 31, Batch 16/100: Batch Loss = 0.3136\n",
            "Iteration 31, Batch 17/100: Batch Loss = 0.1280\n",
            "Iteration 31, Batch 18/100: Batch Loss = 0.1417\n",
            "Iteration 31, Batch 19/100: Batch Loss = 0.1569\n",
            "Iteration 31, Batch 20/100: Batch Loss = 0.0304\n",
            "Iteration 31, Batch 21/100: Batch Loss = 0.0642\n",
            "Iteration 31, Batch 22/100: Batch Loss = 0.0539\n",
            "Iteration 31, Batch 23/100: Batch Loss = 0.0772\n",
            "Iteration 31, Batch 24/100: Batch Loss = 0.1832\n",
            "Iteration 31, Batch 25/100: Batch Loss = 0.1028\n",
            "Iteration 31, Batch 26/100: Batch Loss = 0.2319\n",
            "Iteration 31, Batch 27/100: Batch Loss = 0.0887\n",
            "Iteration 31, Batch 28/100: Batch Loss = 0.1321\n",
            "Iteration 31, Batch 29/100: Batch Loss = 0.2463\n",
            "Iteration 31, Batch 30/100: Batch Loss = 0.2091\n",
            "Iteration 31, Batch 31/100: Batch Loss = 0.1413\n",
            "Iteration 31, Batch 32/100: Batch Loss = 0.1693\n",
            "Iteration 31, Batch 33/100: Batch Loss = 0.1878\n",
            "Iteration 31, Batch 34/100: Batch Loss = 0.0351\n",
            "Iteration 31, Batch 35/100: Batch Loss = 0.0589\n",
            "Iteration 31, Batch 36/100: Batch Loss = 0.1786\n",
            "Iteration 31, Batch 37/100: Batch Loss = 0.0500\n",
            "Iteration 31, Batch 38/100: Batch Loss = 0.1384\n",
            "Iteration 31, Batch 39/100: Batch Loss = 0.2021\n",
            "Iteration 31, Batch 40/100: Batch Loss = 0.0588\n",
            "Iteration 31, Batch 41/100: Batch Loss = 0.0732\n",
            "Iteration 31, Batch 42/100: Batch Loss = 0.1033\n",
            "Iteration 31, Batch 43/100: Batch Loss = 0.1215\n",
            "Iteration 31, Batch 44/100: Batch Loss = 0.1173\n",
            "Iteration 31, Batch 45/100: Batch Loss = 0.0218\n",
            "Iteration 31, Batch 46/100: Batch Loss = 0.3827\n",
            "Iteration 31, Batch 47/100: Batch Loss = 0.0481\n",
            "Iteration 31, Batch 48/100: Batch Loss = 0.2224\n",
            "Iteration 31, Batch 49/100: Batch Loss = 0.1028\n",
            "Iteration 31, Batch 50/100: Batch Loss = 0.1575\n",
            "Iteration 31, Batch 51/100: Batch Loss = 0.1036\n",
            "Iteration 31, Batch 52/100: Batch Loss = 0.0874\n",
            "Iteration 31, Batch 53/100: Batch Loss = 0.2180\n",
            "Iteration 31, Batch 54/100: Batch Loss = 0.1098\n",
            "Iteration 31, Batch 55/100: Batch Loss = 0.1514\n",
            "Iteration 31, Batch 56/100: Batch Loss = 0.0417\n",
            "Iteration 31, Batch 57/100: Batch Loss = 0.0659\n",
            "Iteration 31, Batch 58/100: Batch Loss = 0.1664\n",
            "Iteration 31, Batch 59/100: Batch Loss = 0.1117\n",
            "Iteration 31, Batch 60/100: Batch Loss = 0.0442\n",
            "Iteration 31, Batch 61/100: Batch Loss = 0.1678\n",
            "Iteration 31, Batch 62/100: Batch Loss = 0.0973\n",
            "Iteration 31, Batch 63/100: Batch Loss = 0.0542\n",
            "Iteration 31, Batch 64/100: Batch Loss = 0.1231\n",
            "Iteration 31, Batch 65/100: Batch Loss = 0.0502\n",
            "Iteration 31, Batch 66/100: Batch Loss = 0.2863\n",
            "Iteration 31, Batch 67/100: Batch Loss = 0.0981\n",
            "Iteration 31, Batch 68/100: Batch Loss = 0.1316\n",
            "Iteration 31, Batch 69/100: Batch Loss = 0.1557\n",
            "Iteration 31, Batch 70/100: Batch Loss = 0.0653\n",
            "Iteration 31, Batch 71/100: Batch Loss = 0.2248\n",
            "Iteration 31, Batch 72/100: Batch Loss = 0.2452\n",
            "Iteration 31, Batch 73/100: Batch Loss = 0.1344\n",
            "Iteration 31, Batch 74/100: Batch Loss = 0.1905\n",
            "Iteration 31, Batch 75/100: Batch Loss = 0.1067\n",
            "Iteration 31, Batch 76/100: Batch Loss = 0.1610\n",
            "Iteration 31, Batch 77/100: Batch Loss = 0.0761\n",
            "Iteration 31, Batch 78/100: Batch Loss = 0.0728\n",
            "Iteration 31, Batch 79/100: Batch Loss = 0.1664\n",
            "Iteration 31, Batch 80/100: Batch Loss = 0.0345\n",
            "Iteration 31, Batch 81/100: Batch Loss = 0.2675\n",
            "Iteration 31, Batch 82/100: Batch Loss = 0.3727\n",
            "Iteration 31, Batch 83/100: Batch Loss = 0.1864\n",
            "Iteration 31, Batch 84/100: Batch Loss = 0.2610\n",
            "Iteration 31, Batch 85/100: Batch Loss = 0.0416\n",
            "Iteration 31, Batch 86/100: Batch Loss = 0.0935\n",
            "Iteration 31, Batch 87/100: Batch Loss = 0.0446\n",
            "Iteration 31, Batch 88/100: Batch Loss = 0.0402\n",
            "Iteration 31, Batch 89/100: Batch Loss = 0.1598\n",
            "Iteration 31, Batch 90/100: Batch Loss = 0.0455\n",
            "Iteration 31, Batch 91/100: Batch Loss = 0.1173\n",
            "Iteration 31, Batch 92/100: Batch Loss = 0.0351\n",
            "Iteration 31, Batch 93/100: Batch Loss = 0.4044\n",
            "Iteration 31, Batch 94/100: Batch Loss = 0.0597\n",
            "Iteration 31, Batch 95/100: Batch Loss = 0.1921\n",
            "Iteration 31, Batch 96/100: Batch Loss = 0.0853\n",
            "Iteration 31, Batch 97/100: Batch Loss = 0.0840\n",
            "Iteration 31, Batch 98/100: Batch Loss = 0.1713\n",
            "Iteration 31, Batch 99/100: Batch Loss = 0.0598\n",
            "Iteration 31, Batch 100/100: Batch Loss = 0.0324\n",
            "Iteration 31: Train Loss = 0.1270, Val Loss = 0.1643\n",
            "Iteration 32, Batch 1/100: Batch Loss = 0.0404\n",
            "Iteration 32, Batch 2/100: Batch Loss = 0.0766\n",
            "Iteration 32, Batch 3/100: Batch Loss = 0.0832\n",
            "Iteration 32, Batch 4/100: Batch Loss = 0.0437\n",
            "Iteration 32, Batch 5/100: Batch Loss = 0.1139\n",
            "Iteration 32, Batch 6/100: Batch Loss = 0.1691\n",
            "Iteration 32, Batch 7/100: Batch Loss = 0.1651\n",
            "Iteration 32, Batch 8/100: Batch Loss = 0.0859\n",
            "Iteration 32, Batch 9/100: Batch Loss = 0.0819\n",
            "Iteration 32, Batch 10/100: Batch Loss = 0.0830\n",
            "Iteration 32, Batch 11/100: Batch Loss = 0.1609\n",
            "Iteration 32, Batch 12/100: Batch Loss = 0.1974\n",
            "Iteration 32, Batch 13/100: Batch Loss = 0.2082\n",
            "Iteration 32, Batch 14/100: Batch Loss = 0.1197\n",
            "Iteration 32, Batch 15/100: Batch Loss = 0.2002\n",
            "Iteration 32, Batch 16/100: Batch Loss = 0.1427\n",
            "Iteration 32, Batch 17/100: Batch Loss = 0.1701\n",
            "Iteration 32, Batch 18/100: Batch Loss = 0.1242\n",
            "Iteration 32, Batch 19/100: Batch Loss = 0.0384\n",
            "Iteration 32, Batch 20/100: Batch Loss = 0.0610\n",
            "Iteration 32, Batch 21/100: Batch Loss = 0.2134\n",
            "Iteration 32, Batch 22/100: Batch Loss = 0.1375\n",
            "Iteration 32, Batch 23/100: Batch Loss = 0.2698\n",
            "Iteration 32, Batch 24/100: Batch Loss = 0.1112\n",
            "Iteration 32, Batch 25/100: Batch Loss = 0.0705\n",
            "Iteration 32, Batch 26/100: Batch Loss = 0.1085\n",
            "Iteration 32, Batch 27/100: Batch Loss = 0.2105\n",
            "Iteration 32, Batch 28/100: Batch Loss = 0.0450\n",
            "Iteration 32, Batch 29/100: Batch Loss = 0.1608\n",
            "Iteration 32, Batch 30/100: Batch Loss = 0.0749\n",
            "Iteration 32, Batch 31/100: Batch Loss = 0.0594\n",
            "Iteration 32, Batch 32/100: Batch Loss = 0.0757\n",
            "Iteration 32, Batch 33/100: Batch Loss = 0.2294\n",
            "Iteration 32, Batch 34/100: Batch Loss = 0.0763\n",
            "Iteration 32, Batch 35/100: Batch Loss = 0.2592\n",
            "Iteration 32, Batch 36/100: Batch Loss = 0.0758\n",
            "Iteration 32, Batch 37/100: Batch Loss = 0.3560\n",
            "Iteration 32, Batch 38/100: Batch Loss = 0.1213\n",
            "Iteration 32, Batch 39/100: Batch Loss = 0.0408\n",
            "Iteration 32, Batch 40/100: Batch Loss = 0.0922\n",
            "Iteration 32, Batch 41/100: Batch Loss = 0.1622\n",
            "Iteration 32, Batch 42/100: Batch Loss = 0.0300\n",
            "Iteration 32, Batch 43/100: Batch Loss = 0.1438\n",
            "Iteration 32, Batch 44/100: Batch Loss = 0.1974\n",
            "Iteration 32, Batch 45/100: Batch Loss = 0.0969\n",
            "Iteration 32, Batch 46/100: Batch Loss = 0.0350\n",
            "Iteration 32, Batch 47/100: Batch Loss = 0.1758\n",
            "Iteration 32, Batch 48/100: Batch Loss = 0.0743\n",
            "Iteration 32, Batch 49/100: Batch Loss = 0.1163\n",
            "Iteration 32, Batch 50/100: Batch Loss = 0.0518\n",
            "Iteration 32, Batch 51/100: Batch Loss = 0.1529\n",
            "Iteration 32, Batch 52/100: Batch Loss = 0.1486\n",
            "Iteration 32, Batch 53/100: Batch Loss = 0.2264\n",
            "Iteration 32, Batch 54/100: Batch Loss = 0.0868\n",
            "Iteration 32, Batch 55/100: Batch Loss = 0.0722\n",
            "Iteration 32, Batch 56/100: Batch Loss = 0.1572\n",
            "Iteration 32, Batch 57/100: Batch Loss = 0.1740\n",
            "Iteration 32, Batch 58/100: Batch Loss = 0.0913\n",
            "Iteration 32, Batch 59/100: Batch Loss = 0.0856\n",
            "Iteration 32, Batch 60/100: Batch Loss = 0.3179\n",
            "Iteration 32, Batch 61/100: Batch Loss = 0.0484\n",
            "Iteration 32, Batch 62/100: Batch Loss = 0.1702\n",
            "Iteration 32, Batch 63/100: Batch Loss = 0.0894\n",
            "Iteration 32, Batch 64/100: Batch Loss = 0.1025\n",
            "Iteration 32, Batch 65/100: Batch Loss = 0.1642\n",
            "Iteration 32, Batch 66/100: Batch Loss = 0.1091\n",
            "Iteration 32, Batch 67/100: Batch Loss = 0.1106\n",
            "Iteration 32, Batch 68/100: Batch Loss = 0.1719\n",
            "Iteration 32, Batch 69/100: Batch Loss = 0.0630\n",
            "Iteration 32, Batch 70/100: Batch Loss = 0.1464\n",
            "Iteration 32, Batch 71/100: Batch Loss = 0.1872\n",
            "Iteration 32, Batch 72/100: Batch Loss = 0.2955\n",
            "Iteration 32, Batch 73/100: Batch Loss = 0.0637\n",
            "Iteration 32, Batch 74/100: Batch Loss = 0.1461\n",
            "Iteration 32, Batch 75/100: Batch Loss = 0.1346\n",
            "Iteration 32, Batch 76/100: Batch Loss = 0.0523\n",
            "Iteration 32, Batch 77/100: Batch Loss = 0.2708\n",
            "Iteration 32, Batch 78/100: Batch Loss = 0.1789\n",
            "Iteration 32, Batch 79/100: Batch Loss = 0.2792\n",
            "Iteration 32, Batch 80/100: Batch Loss = 0.1399\n",
            "Iteration 32, Batch 81/100: Batch Loss = 0.1598\n",
            "Iteration 32, Batch 82/100: Batch Loss = 0.1075\n",
            "Iteration 32, Batch 83/100: Batch Loss = 0.1498\n",
            "Iteration 32, Batch 84/100: Batch Loss = 0.1805\n",
            "Iteration 32, Batch 85/100: Batch Loss = 0.0694\n",
            "Iteration 32, Batch 86/100: Batch Loss = 0.2245\n",
            "Iteration 32, Batch 87/100: Batch Loss = 0.0896\n",
            "Iteration 32, Batch 88/100: Batch Loss = 0.0399\n",
            "Iteration 32, Batch 89/100: Batch Loss = 0.0923\n",
            "Iteration 32, Batch 90/100: Batch Loss = 0.0582\n",
            "Iteration 32, Batch 91/100: Batch Loss = 0.0727\n",
            "Iteration 32, Batch 92/100: Batch Loss = 0.1656\n",
            "Iteration 32, Batch 93/100: Batch Loss = 0.1192\n",
            "Iteration 32, Batch 94/100: Batch Loss = 0.0805\n",
            "Iteration 32, Batch 95/100: Batch Loss = 0.0270\n",
            "Iteration 32, Batch 96/100: Batch Loss = 0.1083\n",
            "Iteration 32, Batch 97/100: Batch Loss = 0.2743\n",
            "Iteration 32, Batch 98/100: Batch Loss = 0.0302\n",
            "Iteration 32, Batch 99/100: Batch Loss = 0.1509\n",
            "Iteration 32, Batch 100/100: Batch Loss = 0.1084\n",
            "Iteration 32: Train Loss = 0.1298, Val Loss = 0.1631\n",
            "Iteration 33, Batch 1/100: Batch Loss = 0.1137\n",
            "Iteration 33, Batch 2/100: Batch Loss = 0.1297\n",
            "Iteration 33, Batch 3/100: Batch Loss = 0.1821\n",
            "Iteration 33, Batch 4/100: Batch Loss = 0.0598\n",
            "Iteration 33, Batch 5/100: Batch Loss = 0.0235\n",
            "Iteration 33, Batch 6/100: Batch Loss = 0.0705\n",
            "Iteration 33, Batch 7/100: Batch Loss = 0.1319\n",
            "Iteration 33, Batch 8/100: Batch Loss = 0.0485\n",
            "Iteration 33, Batch 9/100: Batch Loss = 0.0242\n",
            "Iteration 33, Batch 10/100: Batch Loss = 0.1685\n",
            "Iteration 33, Batch 11/100: Batch Loss = 0.0627\n",
            "Iteration 33, Batch 12/100: Batch Loss = 0.2244\n",
            "Iteration 33, Batch 13/100: Batch Loss = 0.2071\n",
            "Iteration 33, Batch 14/100: Batch Loss = 0.2586\n",
            "Iteration 33, Batch 15/100: Batch Loss = 0.1283\n",
            "Iteration 33, Batch 16/100: Batch Loss = 0.2542\n",
            "Iteration 33, Batch 17/100: Batch Loss = 0.0867\n",
            "Iteration 33, Batch 18/100: Batch Loss = 0.2041\n",
            "Iteration 33, Batch 19/100: Batch Loss = 0.0601\n",
            "Iteration 33, Batch 20/100: Batch Loss = 0.0470\n",
            "Iteration 33, Batch 21/100: Batch Loss = 0.0586\n",
            "Iteration 33, Batch 22/100: Batch Loss = 0.1561\n",
            "Iteration 33, Batch 23/100: Batch Loss = 0.1548\n",
            "Iteration 33, Batch 24/100: Batch Loss = 0.1533\n",
            "Iteration 33, Batch 25/100: Batch Loss = 0.1457\n",
            "Iteration 33, Batch 26/100: Batch Loss = 0.0881\n",
            "Iteration 33, Batch 27/100: Batch Loss = 0.0472\n",
            "Iteration 33, Batch 28/100: Batch Loss = 0.0354\n",
            "Iteration 33, Batch 29/100: Batch Loss = 0.1163\n",
            "Iteration 33, Batch 30/100: Batch Loss = 0.0304\n",
            "Iteration 33, Batch 31/100: Batch Loss = 0.0390\n",
            "Iteration 33, Batch 32/100: Batch Loss = 0.0554\n",
            "Iteration 33, Batch 33/100: Batch Loss = 0.0305\n",
            "Iteration 33, Batch 34/100: Batch Loss = 0.0593\n",
            "Iteration 33, Batch 35/100: Batch Loss = 0.1535\n",
            "Iteration 33, Batch 36/100: Batch Loss = 0.2253\n",
            "Iteration 33, Batch 37/100: Batch Loss = 0.1877\n",
            "Iteration 33, Batch 38/100: Batch Loss = 0.0958\n",
            "Iteration 33, Batch 39/100: Batch Loss = 0.3144\n",
            "Iteration 33, Batch 40/100: Batch Loss = 0.0717\n",
            "Iteration 33, Batch 41/100: Batch Loss = 0.1698\n",
            "Iteration 33, Batch 42/100: Batch Loss = 0.0272\n",
            "Iteration 33, Batch 43/100: Batch Loss = 0.0356\n",
            "Iteration 33, Batch 44/100: Batch Loss = 0.1089\n",
            "Iteration 33, Batch 45/100: Batch Loss = 0.0602\n",
            "Iteration 33, Batch 46/100: Batch Loss = 0.2477\n",
            "Iteration 33, Batch 47/100: Batch Loss = 0.1790\n",
            "Iteration 33, Batch 48/100: Batch Loss = 0.1662\n",
            "Iteration 33, Batch 49/100: Batch Loss = 0.0834\n",
            "Iteration 33, Batch 50/100: Batch Loss = 0.1186\n",
            "Iteration 33, Batch 51/100: Batch Loss = 0.1832\n",
            "Iteration 33, Batch 52/100: Batch Loss = 0.1823\n",
            "Iteration 33, Batch 53/100: Batch Loss = 0.1347\n",
            "Iteration 33, Batch 54/100: Batch Loss = 0.1044\n",
            "Iteration 33, Batch 55/100: Batch Loss = 0.0782\n",
            "Iteration 33, Batch 56/100: Batch Loss = 0.0894\n",
            "Iteration 33, Batch 57/100: Batch Loss = 0.1451\n",
            "Iteration 33, Batch 58/100: Batch Loss = 0.0368\n",
            "Iteration 33, Batch 59/100: Batch Loss = 0.1839\n",
            "Iteration 33, Batch 60/100: Batch Loss = 0.0712\n",
            "Iteration 33, Batch 61/100: Batch Loss = 0.0466\n",
            "Iteration 33, Batch 62/100: Batch Loss = 0.0841\n",
            "Iteration 33, Batch 63/100: Batch Loss = 0.1688\n",
            "Iteration 33, Batch 64/100: Batch Loss = 0.1718\n",
            "Iteration 33, Batch 65/100: Batch Loss = 0.2637\n",
            "Iteration 33, Batch 66/100: Batch Loss = 0.2747\n",
            "Iteration 33, Batch 67/100: Batch Loss = 0.2327\n",
            "Iteration 33, Batch 68/100: Batch Loss = 0.0897\n",
            "Iteration 33, Batch 69/100: Batch Loss = 0.0615\n",
            "Iteration 33, Batch 70/100: Batch Loss = 0.0961\n",
            "Iteration 33, Batch 71/100: Batch Loss = 0.0408\n",
            "Iteration 33, Batch 72/100: Batch Loss = 0.0762\n",
            "Iteration 33, Batch 73/100: Batch Loss = 0.0576\n",
            "Iteration 33, Batch 74/100: Batch Loss = 0.1472\n",
            "Iteration 33, Batch 75/100: Batch Loss = 0.0847\n",
            "Iteration 33, Batch 76/100: Batch Loss = 0.0740\n",
            "Iteration 33, Batch 77/100: Batch Loss = 0.1786\n",
            "Iteration 33, Batch 78/100: Batch Loss = 0.1499\n",
            "Iteration 33, Batch 79/100: Batch Loss = 0.1635\n",
            "Iteration 33, Batch 80/100: Batch Loss = 0.0781\n",
            "Iteration 33, Batch 81/100: Batch Loss = 0.1024\n",
            "Iteration 33, Batch 82/100: Batch Loss = 0.5123\n",
            "Iteration 33, Batch 83/100: Batch Loss = 0.1135\n",
            "Iteration 33, Batch 84/100: Batch Loss = 0.0229\n",
            "Iteration 33, Batch 85/100: Batch Loss = 0.1370\n",
            "Iteration 33, Batch 86/100: Batch Loss = 0.1425\n",
            "Iteration 33, Batch 87/100: Batch Loss = 0.0750\n",
            "Iteration 33, Batch 88/100: Batch Loss = 0.0423\n",
            "Iteration 33, Batch 89/100: Batch Loss = 0.1464\n",
            "Iteration 33, Batch 90/100: Batch Loss = 0.1241\n",
            "Iteration 33, Batch 91/100: Batch Loss = 0.3134\n",
            "Iteration 33, Batch 92/100: Batch Loss = 0.1663\n",
            "Iteration 33, Batch 93/100: Batch Loss = 0.0876\n",
            "Iteration 33, Batch 94/100: Batch Loss = 0.1695\n",
            "Iteration 33, Batch 95/100: Batch Loss = 0.0937\n",
            "Iteration 33, Batch 96/100: Batch Loss = 0.0682\n",
            "Iteration 33, Batch 97/100: Batch Loss = 0.0641\n",
            "Iteration 33, Batch 98/100: Batch Loss = 0.2046\n",
            "Iteration 33, Batch 99/100: Batch Loss = 0.2728\n",
            "Iteration 33, Batch 100/100: Batch Loss = 0.0714\n",
            "Iteration 33: Train Loss = 0.1258, Val Loss = 0.1562\n",
            "Iteration 34, Batch 1/100: Batch Loss = 0.1231\n",
            "Iteration 34, Batch 2/100: Batch Loss = 0.1063\n",
            "Iteration 34, Batch 3/100: Batch Loss = 0.1032\n",
            "Iteration 34, Batch 4/100: Batch Loss = 0.1232\n",
            "Iteration 34, Batch 5/100: Batch Loss = 0.1665\n",
            "Iteration 34, Batch 6/100: Batch Loss = 0.0600\n",
            "Iteration 34, Batch 7/100: Batch Loss = 0.0771\n",
            "Iteration 34, Batch 8/100: Batch Loss = 0.2638\n",
            "Iteration 34, Batch 9/100: Batch Loss = 0.1533\n",
            "Iteration 34, Batch 10/100: Batch Loss = 0.0793\n",
            "Iteration 34, Batch 11/100: Batch Loss = 0.1135\n",
            "Iteration 34, Batch 12/100: Batch Loss = 0.0310\n",
            "Iteration 34, Batch 13/100: Batch Loss = 0.0767\n",
            "Iteration 34, Batch 14/100: Batch Loss = 0.0461\n",
            "Iteration 34, Batch 15/100: Batch Loss = 0.0716\n",
            "Iteration 34, Batch 16/100: Batch Loss = 0.1014\n",
            "Iteration 34, Batch 17/100: Batch Loss = 0.1653\n",
            "Iteration 34, Batch 18/100: Batch Loss = 0.1260\n",
            "Iteration 34, Batch 19/100: Batch Loss = 0.0894\n",
            "Iteration 34, Batch 20/100: Batch Loss = 0.0826\n",
            "Iteration 34, Batch 21/100: Batch Loss = 0.1959\n",
            "Iteration 34, Batch 22/100: Batch Loss = 0.0743\n",
            "Iteration 34, Batch 23/100: Batch Loss = 0.2826\n",
            "Iteration 34, Batch 24/100: Batch Loss = 0.1298\n",
            "Iteration 34, Batch 25/100: Batch Loss = 0.1149\n",
            "Iteration 34, Batch 26/100: Batch Loss = 0.0711\n",
            "Iteration 34, Batch 27/100: Batch Loss = 0.1270\n",
            "Iteration 34, Batch 28/100: Batch Loss = 0.0521\n",
            "Iteration 34, Batch 29/100: Batch Loss = 0.0989\n",
            "Iteration 34, Batch 30/100: Batch Loss = 0.1482\n",
            "Iteration 34, Batch 31/100: Batch Loss = 0.1807\n",
            "Iteration 34, Batch 32/100: Batch Loss = 0.1962\n",
            "Iteration 34, Batch 33/100: Batch Loss = 0.2327\n",
            "Iteration 34, Batch 34/100: Batch Loss = 0.1357\n",
            "Iteration 34, Batch 35/100: Batch Loss = 0.0857\n",
            "Iteration 34, Batch 36/100: Batch Loss = 0.0749\n",
            "Iteration 34, Batch 37/100: Batch Loss = 0.0772\n",
            "Iteration 34, Batch 38/100: Batch Loss = 0.3063\n",
            "Iteration 34, Batch 39/100: Batch Loss = 0.1891\n",
            "Iteration 34, Batch 40/100: Batch Loss = 0.0447\n",
            "Iteration 34, Batch 41/100: Batch Loss = 0.0600\n",
            "Iteration 34, Batch 42/100: Batch Loss = 0.0226\n",
            "Iteration 34, Batch 43/100: Batch Loss = 0.3037\n",
            "Iteration 34, Batch 44/100: Batch Loss = 0.0781\n",
            "Iteration 34, Batch 45/100: Batch Loss = 0.1606\n",
            "Iteration 34, Batch 46/100: Batch Loss = 0.1058\n",
            "Iteration 34, Batch 47/100: Batch Loss = 0.0726\n",
            "Iteration 34, Batch 48/100: Batch Loss = 0.1077\n",
            "Iteration 34, Batch 49/100: Batch Loss = 0.3674\n",
            "Iteration 34, Batch 50/100: Batch Loss = 0.0833\n",
            "Iteration 34, Batch 51/100: Batch Loss = 0.0627\n",
            "Iteration 34, Batch 52/100: Batch Loss = 0.0984\n",
            "Iteration 34, Batch 53/100: Batch Loss = 0.1371\n",
            "Iteration 34, Batch 54/100: Batch Loss = 0.0761\n",
            "Iteration 34, Batch 55/100: Batch Loss = 0.2089\n",
            "Iteration 34, Batch 56/100: Batch Loss = 0.2574\n",
            "Iteration 34, Batch 57/100: Batch Loss = 0.1529\n",
            "Iteration 34, Batch 58/100: Batch Loss = 0.1992\n",
            "Iteration 34, Batch 59/100: Batch Loss = 0.1517\n",
            "Iteration 34, Batch 60/100: Batch Loss = 0.0902\n",
            "Iteration 34, Batch 61/100: Batch Loss = 0.1970\n",
            "Iteration 34, Batch 62/100: Batch Loss = 0.1371\n",
            "Iteration 34, Batch 63/100: Batch Loss = 0.1835\n",
            "Iteration 34, Batch 64/100: Batch Loss = 0.0627\n",
            "Iteration 34, Batch 65/100: Batch Loss = 0.1523\n",
            "Iteration 34, Batch 66/100: Batch Loss = 0.1294\n",
            "Iteration 34, Batch 67/100: Batch Loss = 0.0497\n",
            "Iteration 34, Batch 68/100: Batch Loss = 0.0517\n",
            "Iteration 34, Batch 69/100: Batch Loss = 0.0476\n",
            "Iteration 34, Batch 70/100: Batch Loss = 0.0640\n",
            "Iteration 34, Batch 71/100: Batch Loss = 0.3682\n",
            "Iteration 34, Batch 72/100: Batch Loss = 0.0306\n",
            "Iteration 34, Batch 73/100: Batch Loss = 0.1368\n",
            "Iteration 34, Batch 74/100: Batch Loss = 0.1603\n",
            "Iteration 34, Batch 75/100: Batch Loss = 0.1600\n",
            "Iteration 34, Batch 76/100: Batch Loss = 0.1341\n",
            "Iteration 34, Batch 77/100: Batch Loss = 0.1667\n",
            "Iteration 34, Batch 78/100: Batch Loss = 0.3542\n",
            "Iteration 34, Batch 79/100: Batch Loss = 0.1107\n",
            "Iteration 34, Batch 80/100: Batch Loss = 0.2112\n",
            "Iteration 34, Batch 81/100: Batch Loss = 0.0753\n",
            "Iteration 34, Batch 82/100: Batch Loss = 0.0695\n",
            "Iteration 34, Batch 83/100: Batch Loss = 0.1748\n",
            "Iteration 34, Batch 84/100: Batch Loss = 0.2277\n",
            "Iteration 34, Batch 85/100: Batch Loss = 0.0506\n",
            "Iteration 34, Batch 86/100: Batch Loss = 0.0254\n",
            "Iteration 34, Batch 87/100: Batch Loss = 0.0604\n",
            "Iteration 34, Batch 88/100: Batch Loss = 0.1154\n",
            "Iteration 34, Batch 89/100: Batch Loss = 0.0425\n",
            "Iteration 34, Batch 90/100: Batch Loss = 0.0336\n",
            "Iteration 34, Batch 91/100: Batch Loss = 0.1134\n",
            "Iteration 34, Batch 92/100: Batch Loss = 0.0810\n",
            "Iteration 34, Batch 93/100: Batch Loss = 0.2231\n",
            "Iteration 34, Batch 94/100: Batch Loss = 0.0292\n",
            "Iteration 34, Batch 95/100: Batch Loss = 0.3287\n",
            "Iteration 34, Batch 96/100: Batch Loss = 0.3772\n",
            "Iteration 34, Batch 97/100: Batch Loss = 0.3480\n",
            "Iteration 34, Batch 98/100: Batch Loss = 0.2456\n",
            "Iteration 34, Batch 99/100: Batch Loss = 0.0848\n",
            "Iteration 34, Batch 100/100: Batch Loss = 0.1421\n",
            "Iteration 34: Train Loss = 0.1353, Val Loss = 0.1630\n",
            "Iteration 35, Batch 1/100: Batch Loss = 0.2100\n",
            "Iteration 35, Batch 2/100: Batch Loss = 0.0629\n",
            "Iteration 35, Batch 3/100: Batch Loss = 0.1885\n",
            "Iteration 35, Batch 4/100: Batch Loss = 0.1890\n",
            "Iteration 35, Batch 5/100: Batch Loss = 0.0748\n",
            "Iteration 35, Batch 6/100: Batch Loss = 0.2108\n",
            "Iteration 35, Batch 7/100: Batch Loss = 0.1219\n",
            "Iteration 35, Batch 8/100: Batch Loss = 0.0550\n",
            "Iteration 35, Batch 9/100: Batch Loss = 0.1325\n",
            "Iteration 35, Batch 10/100: Batch Loss = 0.1923\n",
            "Iteration 35, Batch 11/100: Batch Loss = 0.1225\n",
            "Iteration 35, Batch 12/100: Batch Loss = 0.1416\n",
            "Iteration 35, Batch 13/100: Batch Loss = 0.0771\n",
            "Iteration 35, Batch 14/100: Batch Loss = 0.0604\n",
            "Iteration 35, Batch 15/100: Batch Loss = 0.1915\n",
            "Iteration 35, Batch 16/100: Batch Loss = 0.0631\n",
            "Iteration 35, Batch 17/100: Batch Loss = 0.0524\n",
            "Iteration 35, Batch 18/100: Batch Loss = 0.2544\n",
            "Iteration 35, Batch 19/100: Batch Loss = 0.0367\n",
            "Iteration 35, Batch 20/100: Batch Loss = 0.1282\n",
            "Iteration 35, Batch 21/100: Batch Loss = 0.0745\n",
            "Iteration 35, Batch 22/100: Batch Loss = 0.5155\n",
            "Iteration 35, Batch 23/100: Batch Loss = 0.0826\n",
            "Iteration 35, Batch 24/100: Batch Loss = 0.1482\n",
            "Iteration 35, Batch 25/100: Batch Loss = 0.0905\n",
            "Iteration 35, Batch 26/100: Batch Loss = 0.2131\n",
            "Iteration 35, Batch 27/100: Batch Loss = 0.1886\n",
            "Iteration 35, Batch 28/100: Batch Loss = 0.1152\n",
            "Iteration 35, Batch 29/100: Batch Loss = 0.1759\n",
            "Iteration 35, Batch 30/100: Batch Loss = 0.1608\n",
            "Iteration 35, Batch 31/100: Batch Loss = 0.1367\n",
            "Iteration 35, Batch 32/100: Batch Loss = 0.0862\n",
            "Iteration 35, Batch 33/100: Batch Loss = 0.0695\n",
            "Iteration 35, Batch 34/100: Batch Loss = 0.0701\n",
            "Iteration 35, Batch 35/100: Batch Loss = 0.1591\n",
            "Iteration 35, Batch 36/100: Batch Loss = 0.0538\n",
            "Iteration 35, Batch 37/100: Batch Loss = 0.0136\n",
            "Iteration 35, Batch 38/100: Batch Loss = 0.2224\n",
            "Iteration 35, Batch 39/100: Batch Loss = 0.0710\n",
            "Iteration 35, Batch 40/100: Batch Loss = 0.2232\n",
            "Iteration 35, Batch 41/100: Batch Loss = 0.0200\n",
            "Iteration 35, Batch 42/100: Batch Loss = 0.2211\n",
            "Iteration 35, Batch 43/100: Batch Loss = 0.0307\n",
            "Iteration 35, Batch 44/100: Batch Loss = 0.0746\n",
            "Iteration 35, Batch 45/100: Batch Loss = 0.3479\n",
            "Iteration 35, Batch 46/100: Batch Loss = 0.1593\n",
            "Iteration 35, Batch 47/100: Batch Loss = 0.2541\n",
            "Iteration 35, Batch 48/100: Batch Loss = 0.3041\n",
            "Iteration 35, Batch 49/100: Batch Loss = 0.0209\n",
            "Iteration 35, Batch 50/100: Batch Loss = 0.0384\n",
            "Iteration 35, Batch 51/100: Batch Loss = 0.1395\n",
            "Iteration 35, Batch 52/100: Batch Loss = 0.0312\n",
            "Iteration 35, Batch 53/100: Batch Loss = 0.0467\n",
            "Iteration 35, Batch 54/100: Batch Loss = 0.0647\n",
            "Iteration 35, Batch 55/100: Batch Loss = 0.0322\n",
            "Iteration 35, Batch 56/100: Batch Loss = 0.2573\n",
            "Iteration 35, Batch 57/100: Batch Loss = 0.1193\n",
            "Iteration 35, Batch 58/100: Batch Loss = 0.2280\n",
            "Iteration 35, Batch 59/100: Batch Loss = 0.0268\n",
            "Iteration 35, Batch 60/100: Batch Loss = 0.0779\n",
            "Iteration 35, Batch 61/100: Batch Loss = 0.0439\n",
            "Iteration 35, Batch 62/100: Batch Loss = 0.1175\n",
            "Iteration 35, Batch 63/100: Batch Loss = 0.1602\n",
            "Iteration 35, Batch 64/100: Batch Loss = 0.1071\n",
            "Iteration 35, Batch 65/100: Batch Loss = 0.1413\n",
            "Iteration 35, Batch 66/100: Batch Loss = 0.0755\n",
            "Iteration 35, Batch 67/100: Batch Loss = 0.0556\n",
            "Iteration 35, Batch 68/100: Batch Loss = 0.2996\n",
            "Iteration 35, Batch 69/100: Batch Loss = 0.0415\n",
            "Iteration 35, Batch 70/100: Batch Loss = 0.0897\n",
            "Iteration 35, Batch 71/100: Batch Loss = 0.3252\n",
            "Iteration 35, Batch 72/100: Batch Loss = 0.1961\n",
            "Iteration 35, Batch 73/100: Batch Loss = 0.0658\n",
            "Iteration 35, Batch 74/100: Batch Loss = 0.0999\n",
            "Iteration 35, Batch 75/100: Batch Loss = 0.1686\n",
            "Iteration 35, Batch 76/100: Batch Loss = 0.1275\n",
            "Iteration 35, Batch 77/100: Batch Loss = 0.1645\n",
            "Iteration 35, Batch 78/100: Batch Loss = 0.2226\n",
            "Iteration 35, Batch 79/100: Batch Loss = 0.0523\n",
            "Iteration 35, Batch 80/100: Batch Loss = 0.1515\n",
            "Iteration 35, Batch 81/100: Batch Loss = 0.2028\n",
            "Iteration 35, Batch 82/100: Batch Loss = 0.1135\n",
            "Iteration 35, Batch 83/100: Batch Loss = 0.0751\n",
            "Iteration 35, Batch 84/100: Batch Loss = 0.0432\n",
            "Iteration 35, Batch 85/100: Batch Loss = 0.0982\n",
            "Iteration 35, Batch 86/100: Batch Loss = 0.1359\n",
            "Iteration 35, Batch 87/100: Batch Loss = 0.0890\n",
            "Iteration 35, Batch 88/100: Batch Loss = 0.0727\n",
            "Iteration 35, Batch 89/100: Batch Loss = 0.3103\n",
            "Iteration 35, Batch 90/100: Batch Loss = 0.0984\n",
            "Iteration 35, Batch 91/100: Batch Loss = 0.0764\n",
            "Iteration 35, Batch 92/100: Batch Loss = 0.0748\n",
            "Iteration 35, Batch 93/100: Batch Loss = 0.2028\n",
            "Iteration 35, Batch 94/100: Batch Loss = 0.0780\n",
            "Iteration 35, Batch 95/100: Batch Loss = 0.0389\n",
            "Iteration 35, Batch 96/100: Batch Loss = 0.1018\n",
            "Iteration 35, Batch 97/100: Batch Loss = 0.1546\n",
            "Iteration 35, Batch 98/100: Batch Loss = 0.3859\n",
            "Iteration 35, Batch 99/100: Batch Loss = 0.1187\n",
            "Iteration 35, Batch 100/100: Batch Loss = 0.1296\n",
            "Iteration 35: Train Loss = 0.1324, Val Loss = 0.1530\n",
            "Iteration 36, Batch 1/100: Batch Loss = 0.1478\n",
            "Iteration 36, Batch 2/100: Batch Loss = 0.1060\n",
            "Iteration 36, Batch 3/100: Batch Loss = 0.0915\n",
            "Iteration 36, Batch 4/100: Batch Loss = 0.0734\n",
            "Iteration 36, Batch 5/100: Batch Loss = 0.1741\n",
            "Iteration 36, Batch 6/100: Batch Loss = 0.2393\n",
            "Iteration 36, Batch 7/100: Batch Loss = 0.1608\n",
            "Iteration 36, Batch 8/100: Batch Loss = 0.1820\n",
            "Iteration 36, Batch 9/100: Batch Loss = 0.0250\n",
            "Iteration 36, Batch 10/100: Batch Loss = 0.1893\n",
            "Iteration 36, Batch 11/100: Batch Loss = 0.1021\n",
            "Iteration 36, Batch 12/100: Batch Loss = 0.1596\n",
            "Iteration 36, Batch 13/100: Batch Loss = 0.0243\n",
            "Iteration 36, Batch 14/100: Batch Loss = 0.1396\n",
            "Iteration 36, Batch 15/100: Batch Loss = 0.1437\n",
            "Iteration 36, Batch 16/100: Batch Loss = 0.0302\n",
            "Iteration 36, Batch 17/100: Batch Loss = 0.0485\n",
            "Iteration 36, Batch 18/100: Batch Loss = 0.2030\n",
            "Iteration 36, Batch 19/100: Batch Loss = 0.1207\n",
            "Iteration 36, Batch 20/100: Batch Loss = 0.0735\n",
            "Iteration 36, Batch 21/100: Batch Loss = 0.1323\n",
            "Iteration 36, Batch 22/100: Batch Loss = 0.0684\n",
            "Iteration 36, Batch 23/100: Batch Loss = 0.1621\n",
            "Iteration 36, Batch 24/100: Batch Loss = 0.0757\n",
            "Iteration 36, Batch 25/100: Batch Loss = 0.2016\n",
            "Iteration 36, Batch 26/100: Batch Loss = 0.0731\n",
            "Iteration 36, Batch 27/100: Batch Loss = 0.0390\n",
            "Iteration 36, Batch 28/100: Batch Loss = 0.1648\n",
            "Iteration 36, Batch 29/100: Batch Loss = 0.2227\n",
            "Iteration 36, Batch 30/100: Batch Loss = 0.2097\n",
            "Iteration 36, Batch 31/100: Batch Loss = 0.0798\n",
            "Iteration 36, Batch 32/100: Batch Loss = 0.0449\n",
            "Iteration 36, Batch 33/100: Batch Loss = 0.0274\n",
            "Iteration 36, Batch 34/100: Batch Loss = 0.0552\n",
            "Iteration 36, Batch 35/100: Batch Loss = 0.1008\n",
            "Iteration 36, Batch 36/100: Batch Loss = 0.0303\n",
            "Iteration 36, Batch 37/100: Batch Loss = 0.0544\n",
            "Iteration 36, Batch 38/100: Batch Loss = 0.0983\n",
            "Iteration 36, Batch 39/100: Batch Loss = 0.2085\n",
            "Iteration 36, Batch 40/100: Batch Loss = 0.3131\n",
            "Iteration 36, Batch 41/100: Batch Loss = 0.0595\n",
            "Iteration 36, Batch 42/100: Batch Loss = 0.0715\n",
            "Iteration 36, Batch 43/100: Batch Loss = 0.0831\n",
            "Iteration 36, Batch 44/100: Batch Loss = 0.2202\n",
            "Iteration 36, Batch 45/100: Batch Loss = 0.0849\n",
            "Iteration 36, Batch 46/100: Batch Loss = 0.1607\n",
            "Iteration 36, Batch 47/100: Batch Loss = 0.1076\n",
            "Iteration 36, Batch 48/100: Batch Loss = 0.0981\n",
            "Iteration 36, Batch 49/100: Batch Loss = 0.1992\n",
            "Iteration 36, Batch 50/100: Batch Loss = 0.2340\n",
            "Iteration 36, Batch 51/100: Batch Loss = 0.0620\n",
            "Iteration 36, Batch 52/100: Batch Loss = 0.0890\n",
            "Iteration 36, Batch 53/100: Batch Loss = 0.0464\n",
            "Iteration 36, Batch 54/100: Batch Loss = 0.2393\n",
            "Iteration 36, Batch 55/100: Batch Loss = 0.0873\n",
            "Iteration 36, Batch 56/100: Batch Loss = 0.3153\n",
            "Iteration 36, Batch 57/100: Batch Loss = 0.1259\n",
            "Iteration 36, Batch 58/100: Batch Loss = 0.0521\n",
            "Iteration 36, Batch 59/100: Batch Loss = 0.0382\n",
            "Iteration 36, Batch 60/100: Batch Loss = 0.1057\n",
            "Iteration 36, Batch 61/100: Batch Loss = 0.0461\n",
            "Iteration 36, Batch 62/100: Batch Loss = 0.1367\n",
            "Iteration 36, Batch 63/100: Batch Loss = 0.1203\n",
            "Iteration 36, Batch 64/100: Batch Loss = 0.1559\n",
            "Iteration 36, Batch 65/100: Batch Loss = 0.0857\n",
            "Iteration 36, Batch 66/100: Batch Loss = 0.1231\n",
            "Iteration 36, Batch 67/100: Batch Loss = 0.0530\n",
            "Iteration 36, Batch 68/100: Batch Loss = 0.1543\n",
            "Iteration 36, Batch 69/100: Batch Loss = 0.1055\n",
            "Iteration 36, Batch 70/100: Batch Loss = 0.1066\n",
            "Iteration 36, Batch 71/100: Batch Loss = 0.0287\n",
            "Iteration 36, Batch 72/100: Batch Loss = 0.3212\n",
            "Iteration 36, Batch 73/100: Batch Loss = 0.1892\n",
            "Iteration 36, Batch 74/100: Batch Loss = 0.0662\n",
            "Iteration 36, Batch 75/100: Batch Loss = 0.1024\n",
            "Iteration 36, Batch 76/100: Batch Loss = 0.1157\n",
            "Iteration 36, Batch 77/100: Batch Loss = 0.0981\n",
            "Iteration 36, Batch 78/100: Batch Loss = 0.2549\n",
            "Iteration 36, Batch 79/100: Batch Loss = 0.1012\n",
            "Iteration 36, Batch 80/100: Batch Loss = 0.0784\n",
            "Iteration 36, Batch 81/100: Batch Loss = 0.1727\n",
            "Iteration 36, Batch 82/100: Batch Loss = 0.0657\n",
            "Iteration 36, Batch 83/100: Batch Loss = 0.2440\n",
            "Iteration 36, Batch 84/100: Batch Loss = 0.0778\n",
            "Iteration 36, Batch 85/100: Batch Loss = 0.0512\n",
            "Iteration 36, Batch 86/100: Batch Loss = 0.1397\n",
            "Iteration 36, Batch 87/100: Batch Loss = 0.0870\n",
            "Iteration 36, Batch 88/100: Batch Loss = 0.1029\n",
            "Iteration 36, Batch 89/100: Batch Loss = 0.2077\n",
            "Iteration 36, Batch 90/100: Batch Loss = 0.1133\n",
            "Iteration 36, Batch 91/100: Batch Loss = 0.1831\n",
            "Iteration 36, Batch 92/100: Batch Loss = 0.0552\n",
            "Iteration 36, Batch 93/100: Batch Loss = 0.2466\n",
            "Iteration 36, Batch 94/100: Batch Loss = 0.1439\n",
            "Iteration 36, Batch 95/100: Batch Loss = 0.1224\n",
            "Iteration 36, Batch 96/100: Batch Loss = 0.1928\n",
            "Iteration 36, Batch 97/100: Batch Loss = 0.1269\n",
            "Iteration 36, Batch 98/100: Batch Loss = 0.0919\n",
            "Iteration 36, Batch 99/100: Batch Loss = 0.1179\n",
            "Iteration 36, Batch 100/100: Batch Loss = 0.0175\n",
            "Iteration 36: Train Loss = 0.1228, Val Loss = 0.1495\n",
            "Iteration 37, Batch 1/100: Batch Loss = 0.0865\n",
            "Iteration 37, Batch 2/100: Batch Loss = 0.0330\n",
            "Iteration 37, Batch 3/100: Batch Loss = 0.1697\n",
            "Iteration 37, Batch 4/100: Batch Loss = 0.1130\n",
            "Iteration 37, Batch 5/100: Batch Loss = 0.1472\n",
            "Iteration 37, Batch 6/100: Batch Loss = 0.0416\n",
            "Iteration 37, Batch 7/100: Batch Loss = 0.2378\n",
            "Iteration 37, Batch 8/100: Batch Loss = 0.0351\n",
            "Iteration 37, Batch 9/100: Batch Loss = 0.1263\n",
            "Iteration 37, Batch 10/100: Batch Loss = 0.0358\n",
            "Iteration 37, Batch 11/100: Batch Loss = 0.2013\n",
            "Iteration 37, Batch 12/100: Batch Loss = 0.0810\n",
            "Iteration 37, Batch 13/100: Batch Loss = 0.1906\n",
            "Iteration 37, Batch 14/100: Batch Loss = 0.1800\n",
            "Iteration 37, Batch 15/100: Batch Loss = 0.0744\n",
            "Iteration 37, Batch 16/100: Batch Loss = 0.1268\n",
            "Iteration 37, Batch 17/100: Batch Loss = 0.0828\n",
            "Iteration 37, Batch 18/100: Batch Loss = 0.1921\n",
            "Iteration 37, Batch 19/100: Batch Loss = 0.1230\n",
            "Iteration 37, Batch 20/100: Batch Loss = 0.3300\n",
            "Iteration 37, Batch 21/100: Batch Loss = 0.1622\n",
            "Iteration 37, Batch 22/100: Batch Loss = 0.1615\n",
            "Iteration 37, Batch 23/100: Batch Loss = 0.2701\n",
            "Iteration 37, Batch 24/100: Batch Loss = 0.0833\n",
            "Iteration 37, Batch 25/100: Batch Loss = 0.0696\n",
            "Iteration 37, Batch 26/100: Batch Loss = 0.1148\n",
            "Iteration 37, Batch 27/100: Batch Loss = 0.1082\n",
            "Iteration 37, Batch 28/100: Batch Loss = 0.0419\n",
            "Iteration 37, Batch 29/100: Batch Loss = 0.0810\n",
            "Iteration 37, Batch 30/100: Batch Loss = 0.1142\n",
            "Iteration 37, Batch 31/100: Batch Loss = 0.0967\n",
            "Iteration 37, Batch 32/100: Batch Loss = 0.0728\n",
            "Iteration 37, Batch 33/100: Batch Loss = 0.1394\n",
            "Iteration 37, Batch 34/100: Batch Loss = 0.1255\n",
            "Iteration 37, Batch 35/100: Batch Loss = 0.1448\n",
            "Iteration 37, Batch 36/100: Batch Loss = 0.2355\n",
            "Iteration 37, Batch 37/100: Batch Loss = 0.1431\n",
            "Iteration 37, Batch 38/100: Batch Loss = 0.0567\n",
            "Iteration 37, Batch 39/100: Batch Loss = 0.0668\n",
            "Iteration 37, Batch 40/100: Batch Loss = 0.1411\n",
            "Iteration 37, Batch 41/100: Batch Loss = 0.1089\n",
            "Iteration 37, Batch 42/100: Batch Loss = 0.4715\n",
            "Iteration 37, Batch 43/100: Batch Loss = 0.0778\n",
            "Iteration 37, Batch 44/100: Batch Loss = 0.1017\n",
            "Iteration 37, Batch 45/100: Batch Loss = 0.1168\n",
            "Iteration 37, Batch 46/100: Batch Loss = 0.1228\n",
            "Iteration 37, Batch 47/100: Batch Loss = 0.0969\n",
            "Iteration 37, Batch 48/100: Batch Loss = 0.1637\n",
            "Iteration 37, Batch 49/100: Batch Loss = 0.0378\n",
            "Iteration 37, Batch 50/100: Batch Loss = 0.1008\n",
            "Iteration 37, Batch 51/100: Batch Loss = 0.0686\n",
            "Iteration 37, Batch 52/100: Batch Loss = 0.0387\n",
            "Iteration 37, Batch 53/100: Batch Loss = 0.0496\n",
            "Iteration 37, Batch 54/100: Batch Loss = 0.0699\n",
            "Iteration 37, Batch 55/100: Batch Loss = 0.1139\n",
            "Iteration 37, Batch 56/100: Batch Loss = 0.1395\n",
            "Iteration 37, Batch 57/100: Batch Loss = 0.2538\n",
            "Iteration 37, Batch 58/100: Batch Loss = 0.0737\n",
            "Iteration 37, Batch 59/100: Batch Loss = 0.1290\n",
            "Iteration 37, Batch 60/100: Batch Loss = 0.0300\n",
            "Iteration 37, Batch 61/100: Batch Loss = 0.0580\n",
            "Iteration 37, Batch 62/100: Batch Loss = 0.1084\n",
            "Iteration 37, Batch 63/100: Batch Loss = 0.0434\n",
            "Iteration 37, Batch 64/100: Batch Loss = 0.0998\n",
            "Iteration 37, Batch 65/100: Batch Loss = 0.0858\n",
            "Iteration 37, Batch 66/100: Batch Loss = 0.0446\n",
            "Iteration 37, Batch 67/100: Batch Loss = 0.0975\n",
            "Iteration 37, Batch 68/100: Batch Loss = 0.2506\n",
            "Iteration 37, Batch 69/100: Batch Loss = 0.3192\n",
            "Iteration 37, Batch 70/100: Batch Loss = 0.0396\n",
            "Iteration 37, Batch 71/100: Batch Loss = 0.0858\n",
            "Iteration 37, Batch 72/100: Batch Loss = 0.0844\n",
            "Iteration 37, Batch 73/100: Batch Loss = 0.3308\n",
            "Iteration 37, Batch 74/100: Batch Loss = 0.0594\n",
            "Iteration 37, Batch 75/100: Batch Loss = 0.0653\n",
            "Iteration 37, Batch 76/100: Batch Loss = 0.0335\n",
            "Iteration 37, Batch 77/100: Batch Loss = 0.1176\n",
            "Iteration 37, Batch 78/100: Batch Loss = 0.0667\n",
            "Iteration 37, Batch 79/100: Batch Loss = 0.1023\n",
            "Iteration 37, Batch 80/100: Batch Loss = 0.0657\n",
            "Iteration 37, Batch 81/100: Batch Loss = 0.0657\n",
            "Iteration 37, Batch 82/100: Batch Loss = 0.2115\n",
            "Iteration 37, Batch 83/100: Batch Loss = 0.0352\n",
            "Iteration 37, Batch 84/100: Batch Loss = 0.3005\n",
            "Iteration 37, Batch 85/100: Batch Loss = 0.1912\n",
            "Iteration 37, Batch 86/100: Batch Loss = 0.2597\n",
            "Iteration 37, Batch 87/100: Batch Loss = 0.0462\n",
            "Iteration 37, Batch 88/100: Batch Loss = 0.1035\n",
            "Iteration 37, Batch 89/100: Batch Loss = 0.3184\n",
            "Iteration 37, Batch 90/100: Batch Loss = 0.0845\n",
            "Iteration 37, Batch 91/100: Batch Loss = 0.0664\n",
            "Iteration 37, Batch 92/100: Batch Loss = 0.4546\n",
            "Iteration 37, Batch 93/100: Batch Loss = 0.0470\n",
            "Iteration 37, Batch 94/100: Batch Loss = 0.0546\n",
            "Iteration 37, Batch 95/100: Batch Loss = 0.0913\n",
            "Iteration 37, Batch 96/100: Batch Loss = 0.0874\n",
            "Iteration 37, Batch 97/100: Batch Loss = 0.2301\n",
            "Iteration 37, Batch 98/100: Batch Loss = 0.1321\n",
            "Iteration 37, Batch 99/100: Batch Loss = 0.0330\n",
            "Iteration 37, Batch 100/100: Batch Loss = 0.2322\n",
            "Iteration 37: Train Loss = 0.1261, Val Loss = 0.1559\n",
            "Iteration 38, Batch 1/100: Batch Loss = 0.0822\n",
            "Iteration 38, Batch 2/100: Batch Loss = 0.0305\n",
            "Iteration 38, Batch 3/100: Batch Loss = 0.1901\n",
            "Iteration 38, Batch 4/100: Batch Loss = 0.0351\n",
            "Iteration 38, Batch 5/100: Batch Loss = 0.1079\n",
            "Iteration 38, Batch 6/100: Batch Loss = 0.2312\n",
            "Iteration 38, Batch 7/100: Batch Loss = 0.1679\n",
            "Iteration 38, Batch 8/100: Batch Loss = 0.0946\n",
            "Iteration 38, Batch 9/100: Batch Loss = 0.1517\n",
            "Iteration 38, Batch 10/100: Batch Loss = 0.1508\n",
            "Iteration 38, Batch 11/100: Batch Loss = 0.0972\n",
            "Iteration 38, Batch 12/100: Batch Loss = 0.0657\n",
            "Iteration 38, Batch 13/100: Batch Loss = 0.1490\n",
            "Iteration 38, Batch 14/100: Batch Loss = 0.0751\n",
            "Iteration 38, Batch 15/100: Batch Loss = 0.2254\n",
            "Iteration 38, Batch 16/100: Batch Loss = 0.0839\n",
            "Iteration 38, Batch 17/100: Batch Loss = 0.1671\n",
            "Iteration 38, Batch 18/100: Batch Loss = 0.1378\n",
            "Iteration 38, Batch 19/100: Batch Loss = 0.0377\n",
            "Iteration 38, Batch 20/100: Batch Loss = 0.0525\n",
            "Iteration 38, Batch 21/100: Batch Loss = 0.0309\n",
            "Iteration 38, Batch 22/100: Batch Loss = 0.1416\n",
            "Iteration 38, Batch 23/100: Batch Loss = 0.2072\n",
            "Iteration 38, Batch 24/100: Batch Loss = 0.0196\n",
            "Iteration 38, Batch 25/100: Batch Loss = 0.0295\n",
            "Iteration 38, Batch 26/100: Batch Loss = 0.1019\n",
            "Iteration 38, Batch 27/100: Batch Loss = 0.1446\n",
            "Iteration 38, Batch 28/100: Batch Loss = 0.0603\n",
            "Iteration 38, Batch 29/100: Batch Loss = 0.1527\n",
            "Iteration 38, Batch 30/100: Batch Loss = 0.1369\n",
            "Iteration 38, Batch 31/100: Batch Loss = 0.1404\n",
            "Iteration 38, Batch 32/100: Batch Loss = 0.0750\n",
            "Iteration 38, Batch 33/100: Batch Loss = 0.1103\n",
            "Iteration 38, Batch 34/100: Batch Loss = 0.0813\n",
            "Iteration 38, Batch 35/100: Batch Loss = 0.1257\n",
            "Iteration 38, Batch 36/100: Batch Loss = 0.1168\n",
            "Iteration 38, Batch 37/100: Batch Loss = 0.0948\n",
            "Iteration 38, Batch 38/100: Batch Loss = 0.0248\n",
            "Iteration 38, Batch 39/100: Batch Loss = 0.1816\n",
            "Iteration 38, Batch 40/100: Batch Loss = 0.1879\n",
            "Iteration 38, Batch 41/100: Batch Loss = 0.0397\n",
            "Iteration 38, Batch 42/100: Batch Loss = 0.1295\n",
            "Iteration 38, Batch 43/100: Batch Loss = 0.0995\n",
            "Iteration 38, Batch 44/100: Batch Loss = 0.1148\n",
            "Iteration 38, Batch 45/100: Batch Loss = 0.2273\n",
            "Iteration 38, Batch 46/100: Batch Loss = 0.0823\n",
            "Iteration 38, Batch 47/100: Batch Loss = 0.0566\n",
            "Iteration 38, Batch 48/100: Batch Loss = 0.1548\n",
            "Iteration 38, Batch 49/100: Batch Loss = 0.0700\n",
            "Iteration 38, Batch 50/100: Batch Loss = 0.0663\n",
            "Iteration 38, Batch 51/100: Batch Loss = 0.0865\n",
            "Iteration 38, Batch 52/100: Batch Loss = 0.1769\n",
            "Iteration 38, Batch 53/100: Batch Loss = 0.1769\n",
            "Iteration 38, Batch 54/100: Batch Loss = 0.0847\n",
            "Iteration 38, Batch 55/100: Batch Loss = 0.3022\n",
            "Iteration 38, Batch 56/100: Batch Loss = 0.1301\n",
            "Iteration 38, Batch 57/100: Batch Loss = 0.0667\n",
            "Iteration 38, Batch 58/100: Batch Loss = 0.0700\n",
            "Iteration 38, Batch 59/100: Batch Loss = 0.0503\n",
            "Iteration 38, Batch 60/100: Batch Loss = 0.1078\n",
            "Iteration 38, Batch 61/100: Batch Loss = 0.1857\n",
            "Iteration 38, Batch 62/100: Batch Loss = 0.0768\n",
            "Iteration 38, Batch 63/100: Batch Loss = 0.1046\n",
            "Iteration 38, Batch 64/100: Batch Loss = 0.1663\n",
            "Iteration 38, Batch 65/100: Batch Loss = 0.2921\n",
            "Iteration 38, Batch 66/100: Batch Loss = 0.0913\n",
            "Iteration 38, Batch 67/100: Batch Loss = 0.0642\n",
            "Iteration 38, Batch 68/100: Batch Loss = 0.2717\n",
            "Iteration 38, Batch 69/100: Batch Loss = 0.2176\n",
            "Iteration 38, Batch 70/100: Batch Loss = 0.2453\n",
            "Iteration 38, Batch 71/100: Batch Loss = 0.1037\n",
            "Iteration 38, Batch 72/100: Batch Loss = 0.1244\n",
            "Iteration 38, Batch 73/100: Batch Loss = 0.1068\n",
            "Iteration 38, Batch 74/100: Batch Loss = 0.0844\n",
            "Iteration 38, Batch 75/100: Batch Loss = 0.0471\n",
            "Iteration 38, Batch 76/100: Batch Loss = 0.2066\n",
            "Iteration 38, Batch 77/100: Batch Loss = 0.2013\n",
            "Iteration 38, Batch 78/100: Batch Loss = 0.2558\n",
            "Iteration 38, Batch 79/100: Batch Loss = 0.1659\n",
            "Iteration 38, Batch 80/100: Batch Loss = 0.0781\n",
            "Iteration 38, Batch 81/100: Batch Loss = 0.1942\n",
            "Iteration 38, Batch 82/100: Batch Loss = 0.2649\n",
            "Iteration 38, Batch 83/100: Batch Loss = 0.1262\n",
            "Iteration 38, Batch 84/100: Batch Loss = 0.1473\n",
            "Iteration 38, Batch 85/100: Batch Loss = 0.1724\n",
            "Iteration 38, Batch 86/100: Batch Loss = 0.0906\n",
            "Iteration 38, Batch 87/100: Batch Loss = 0.0414\n",
            "Iteration 38, Batch 88/100: Batch Loss = 0.0424\n",
            "Iteration 38, Batch 89/100: Batch Loss = 0.2155\n",
            "Iteration 38, Batch 90/100: Batch Loss = 0.0470\n",
            "Iteration 38, Batch 91/100: Batch Loss = 0.0553\n",
            "Iteration 38, Batch 92/100: Batch Loss = 0.0939\n",
            "Iteration 38, Batch 93/100: Batch Loss = 0.1257\n",
            "Iteration 38, Batch 94/100: Batch Loss = 0.2595\n",
            "Iteration 38, Batch 95/100: Batch Loss = 0.0752\n",
            "Iteration 38, Batch 96/100: Batch Loss = 0.1096\n",
            "Iteration 38, Batch 97/100: Batch Loss = 0.1608\n",
            "Iteration 38, Batch 98/100: Batch Loss = 0.1729\n",
            "Iteration 38, Batch 99/100: Batch Loss = 0.0483\n",
            "Iteration 38, Batch 100/100: Batch Loss = 0.2677\n",
            "Iteration 38: Train Loss = 0.1259, Val Loss = 0.1670\n",
            "Iteration 39, Batch 1/100: Batch Loss = 0.2051\n",
            "Iteration 39, Batch 2/100: Batch Loss = 0.1255\n",
            "Iteration 39, Batch 3/100: Batch Loss = 0.1838\n",
            "Iteration 39, Batch 4/100: Batch Loss = 0.0967\n",
            "Iteration 39, Batch 5/100: Batch Loss = 0.0212\n",
            "Iteration 39, Batch 6/100: Batch Loss = 0.1189\n",
            "Iteration 39, Batch 7/100: Batch Loss = 0.0482\n",
            "Iteration 39, Batch 8/100: Batch Loss = 0.0847\n",
            "Iteration 39, Batch 9/100: Batch Loss = 0.2266\n",
            "Iteration 39, Batch 10/100: Batch Loss = 0.0361\n",
            "Iteration 39, Batch 11/100: Batch Loss = 0.0487\n",
            "Iteration 39, Batch 12/100: Batch Loss = 0.1334\n",
            "Iteration 39, Batch 13/100: Batch Loss = 0.3076\n",
            "Iteration 39, Batch 14/100: Batch Loss = 0.0851\n",
            "Iteration 39, Batch 15/100: Batch Loss = 0.1508\n",
            "Iteration 39, Batch 16/100: Batch Loss = 0.1241\n",
            "Iteration 39, Batch 17/100: Batch Loss = 0.1074\n",
            "Iteration 39, Batch 18/100: Batch Loss = 0.1490\n",
            "Iteration 39, Batch 19/100: Batch Loss = 0.0590\n",
            "Iteration 39, Batch 20/100: Batch Loss = 0.1882\n",
            "Iteration 39, Batch 21/100: Batch Loss = 0.1238\n",
            "Iteration 39, Batch 22/100: Batch Loss = 0.0624\n",
            "Iteration 39, Batch 23/100: Batch Loss = 0.1958\n",
            "Iteration 39, Batch 24/100: Batch Loss = 0.0781\n",
            "Iteration 39, Batch 25/100: Batch Loss = 0.0350\n",
            "Iteration 39, Batch 26/100: Batch Loss = 0.2527\n",
            "Iteration 39, Batch 27/100: Batch Loss = 0.1952\n",
            "Iteration 39, Batch 28/100: Batch Loss = 0.0544\n",
            "Iteration 39, Batch 29/100: Batch Loss = 0.1553\n",
            "Iteration 39, Batch 30/100: Batch Loss = 0.0702\n",
            "Iteration 39, Batch 31/100: Batch Loss = 0.3194\n",
            "Iteration 39, Batch 32/100: Batch Loss = 0.0410\n",
            "Iteration 39, Batch 33/100: Batch Loss = 0.0371\n",
            "Iteration 39, Batch 34/100: Batch Loss = 0.1715\n",
            "Iteration 39, Batch 35/100: Batch Loss = 0.2138\n",
            "Iteration 39, Batch 36/100: Batch Loss = 0.2267\n",
            "Iteration 39, Batch 37/100: Batch Loss = 0.1346\n",
            "Iteration 39, Batch 38/100: Batch Loss = 0.0761\n",
            "Iteration 39, Batch 39/100: Batch Loss = 0.0981\n",
            "Iteration 39, Batch 40/100: Batch Loss = 0.1147\n",
            "Iteration 39, Batch 41/100: Batch Loss = 0.0397\n",
            "Iteration 39, Batch 42/100: Batch Loss = 0.2425\n",
            "Iteration 39, Batch 43/100: Batch Loss = 0.1736\n",
            "Iteration 39, Batch 44/100: Batch Loss = 0.1109\n",
            "Iteration 39, Batch 45/100: Batch Loss = 0.0449\n",
            "Iteration 39, Batch 46/100: Batch Loss = 0.1837\n",
            "Iteration 39, Batch 47/100: Batch Loss = 0.1071\n",
            "Iteration 39, Batch 48/100: Batch Loss = 0.1095\n",
            "Iteration 39, Batch 49/100: Batch Loss = 0.2298\n",
            "Iteration 39, Batch 50/100: Batch Loss = 0.1973\n",
            "Iteration 39, Batch 51/100: Batch Loss = 0.2309\n",
            "Iteration 39, Batch 52/100: Batch Loss = 0.1479\n",
            "Iteration 39, Batch 53/100: Batch Loss = 0.1418\n",
            "Iteration 39, Batch 54/100: Batch Loss = 0.1114\n",
            "Iteration 39, Batch 55/100: Batch Loss = 0.0979\n",
            "Iteration 39, Batch 56/100: Batch Loss = 0.0671\n",
            "Iteration 39, Batch 57/100: Batch Loss = 0.0983\n",
            "Iteration 39, Batch 58/100: Batch Loss = 0.1003\n",
            "Iteration 39, Batch 59/100: Batch Loss = 0.1178\n",
            "Iteration 39, Batch 60/100: Batch Loss = 0.0471\n",
            "Iteration 39, Batch 61/100: Batch Loss = 0.2458\n",
            "Iteration 39, Batch 62/100: Batch Loss = 0.0644\n",
            "Iteration 39, Batch 63/100: Batch Loss = 0.0653\n",
            "Iteration 39, Batch 64/100: Batch Loss = 0.1145\n",
            "Iteration 39, Batch 65/100: Batch Loss = 0.0654\n",
            "Iteration 39, Batch 66/100: Batch Loss = 0.1115\n",
            "Iteration 39, Batch 67/100: Batch Loss = 0.1843\n",
            "Iteration 39, Batch 68/100: Batch Loss = 0.2659\n",
            "Iteration 39, Batch 69/100: Batch Loss = 0.0188\n",
            "Iteration 39, Batch 70/100: Batch Loss = 0.0584\n",
            "Iteration 39, Batch 71/100: Batch Loss = 0.0633\n",
            "Iteration 39, Batch 72/100: Batch Loss = 0.1157\n",
            "Iteration 39, Batch 73/100: Batch Loss = 0.0661\n",
            "Iteration 39, Batch 74/100: Batch Loss = 0.2084\n",
            "Iteration 39, Batch 75/100: Batch Loss = 0.0645\n",
            "Iteration 39, Batch 76/100: Batch Loss = 0.2852\n",
            "Iteration 39, Batch 77/100: Batch Loss = 0.0592\n",
            "Iteration 39, Batch 78/100: Batch Loss = 0.0719\n",
            "Iteration 39, Batch 79/100: Batch Loss = 0.2473\n",
            "Iteration 39, Batch 80/100: Batch Loss = 0.1640\n",
            "Iteration 39, Batch 81/100: Batch Loss = 0.1322\n",
            "Iteration 39, Batch 82/100: Batch Loss = 0.0334\n",
            "Iteration 39, Batch 83/100: Batch Loss = 0.0988\n",
            "Iteration 39, Batch 84/100: Batch Loss = 0.2307\n",
            "Iteration 39, Batch 85/100: Batch Loss = 0.0655\n",
            "Iteration 39, Batch 86/100: Batch Loss = 0.0948\n",
            "Iteration 39, Batch 87/100: Batch Loss = 0.1446\n",
            "Iteration 39, Batch 88/100: Batch Loss = 0.1395\n",
            "Iteration 39, Batch 89/100: Batch Loss = 0.1729\n",
            "Iteration 39, Batch 90/100: Batch Loss = 0.0691\n",
            "Iteration 39, Batch 91/100: Batch Loss = 0.2190\n",
            "Iteration 39, Batch 92/100: Batch Loss = 0.1272\n",
            "Iteration 39, Batch 93/100: Batch Loss = 0.2073\n",
            "Iteration 39, Batch 94/100: Batch Loss = 0.0905\n",
            "Iteration 39, Batch 95/100: Batch Loss = 0.1332\n",
            "Iteration 39, Batch 96/100: Batch Loss = 0.0339\n",
            "Iteration 39, Batch 97/100: Batch Loss = 0.1054\n",
            "Iteration 39, Batch 98/100: Batch Loss = 0.1399\n",
            "Iteration 39, Batch 99/100: Batch Loss = 0.1417\n",
            "Iteration 39, Batch 100/100: Batch Loss = 0.1226\n",
            "Iteration 39: Train Loss = 0.1280, Val Loss = 0.1542\n",
            "Iteration 40, Batch 1/100: Batch Loss = 0.0541\n",
            "Iteration 40, Batch 2/100: Batch Loss = 0.0308\n",
            "Iteration 40, Batch 3/100: Batch Loss = 0.0356\n",
            "Iteration 40, Batch 4/100: Batch Loss = 0.1758\n",
            "Iteration 40, Batch 5/100: Batch Loss = 0.0847\n",
            "Iteration 40, Batch 6/100: Batch Loss = 0.0740\n",
            "Iteration 40, Batch 7/100: Batch Loss = 0.1591\n",
            "Iteration 40, Batch 8/100: Batch Loss = 0.0492\n",
            "Iteration 40, Batch 9/100: Batch Loss = 0.1114\n",
            "Iteration 40, Batch 10/100: Batch Loss = 0.0494\n",
            "Iteration 40, Batch 11/100: Batch Loss = 0.0601\n",
            "Iteration 40, Batch 12/100: Batch Loss = 0.3058\n",
            "Iteration 40, Batch 13/100: Batch Loss = 0.0414\n",
            "Iteration 40, Batch 14/100: Batch Loss = 0.2012\n",
            "Iteration 40, Batch 15/100: Batch Loss = 0.0462\n",
            "Iteration 40, Batch 16/100: Batch Loss = 0.0625\n",
            "Iteration 40, Batch 17/100: Batch Loss = 0.0595\n",
            "Iteration 40, Batch 18/100: Batch Loss = 0.0521\n",
            "Iteration 40, Batch 19/100: Batch Loss = 0.0914\n",
            "Iteration 40, Batch 20/100: Batch Loss = 0.1647\n",
            "Iteration 40, Batch 21/100: Batch Loss = 0.0530\n",
            "Iteration 40, Batch 22/100: Batch Loss = 0.2327\n",
            "Iteration 40, Batch 23/100: Batch Loss = 0.0317\n",
            "Iteration 40, Batch 24/100: Batch Loss = 0.0406\n",
            "Iteration 40, Batch 25/100: Batch Loss = 0.0992\n",
            "Iteration 40, Batch 26/100: Batch Loss = 0.2061\n",
            "Iteration 40, Batch 27/100: Batch Loss = 0.1246\n",
            "Iteration 40, Batch 28/100: Batch Loss = 0.0995\n",
            "Iteration 40, Batch 29/100: Batch Loss = 0.0861\n",
            "Iteration 40, Batch 30/100: Batch Loss = 0.0405\n",
            "Iteration 40, Batch 31/100: Batch Loss = 0.1101\n",
            "Iteration 40, Batch 32/100: Batch Loss = 0.1753\n",
            "Iteration 40, Batch 33/100: Batch Loss = 0.0512\n",
            "Iteration 40, Batch 34/100: Batch Loss = 0.0510\n",
            "Iteration 40, Batch 35/100: Batch Loss = 0.0570\n",
            "Iteration 40, Batch 36/100: Batch Loss = 0.2263\n",
            "Iteration 40, Batch 37/100: Batch Loss = 0.1286\n",
            "Iteration 40, Batch 38/100: Batch Loss = 0.1588\n",
            "Iteration 40, Batch 39/100: Batch Loss = 0.1728\n",
            "Iteration 40, Batch 40/100: Batch Loss = 0.0856\n",
            "Iteration 40, Batch 41/100: Batch Loss = 0.0906\n",
            "Iteration 40, Batch 42/100: Batch Loss = 0.0685\n",
            "Iteration 40, Batch 43/100: Batch Loss = 0.0634\n",
            "Iteration 40, Batch 44/100: Batch Loss = 0.0826\n",
            "Iteration 40, Batch 45/100: Batch Loss = 0.2246\n",
            "Iteration 40, Batch 46/100: Batch Loss = 0.0818\n",
            "Iteration 40, Batch 47/100: Batch Loss = 0.1618\n",
            "Iteration 40, Batch 48/100: Batch Loss = 0.1277\n",
            "Iteration 40, Batch 49/100: Batch Loss = 0.1347\n",
            "Iteration 40, Batch 50/100: Batch Loss = 0.0400\n",
            "Iteration 40, Batch 51/100: Batch Loss = 0.1740\n",
            "Iteration 40, Batch 52/100: Batch Loss = 0.1933\n",
            "Iteration 40, Batch 53/100: Batch Loss = 0.0964\n",
            "Iteration 40, Batch 54/100: Batch Loss = 0.4018\n",
            "Iteration 40, Batch 55/100: Batch Loss = 0.3039\n",
            "Iteration 40, Batch 56/100: Batch Loss = 0.1769\n",
            "Iteration 40, Batch 57/100: Batch Loss = 0.0127\n",
            "Iteration 40, Batch 58/100: Batch Loss = 0.1966\n",
            "Iteration 40, Batch 59/100: Batch Loss = 0.2643\n",
            "Iteration 40, Batch 60/100: Batch Loss = 0.2452\n",
            "Iteration 40, Batch 61/100: Batch Loss = 0.1044\n",
            "Iteration 40, Batch 62/100: Batch Loss = 0.1822\n",
            "Iteration 40, Batch 63/100: Batch Loss = 0.0852\n",
            "Iteration 40, Batch 64/100: Batch Loss = 0.0747\n",
            "Iteration 40, Batch 65/100: Batch Loss = 0.0947\n",
            "Iteration 40, Batch 66/100: Batch Loss = 0.1226\n",
            "Iteration 40, Batch 67/100: Batch Loss = 0.1684\n",
            "Iteration 40, Batch 68/100: Batch Loss = 0.2380\n",
            "Iteration 40, Batch 69/100: Batch Loss = 0.0572\n",
            "Iteration 40, Batch 70/100: Batch Loss = 0.1788\n",
            "Iteration 40, Batch 71/100: Batch Loss = 0.0773\n",
            "Iteration 40, Batch 72/100: Batch Loss = 0.0944\n",
            "Iteration 40, Batch 73/100: Batch Loss = 0.0894\n",
            "Iteration 40, Batch 74/100: Batch Loss = 0.1475\n",
            "Iteration 40, Batch 75/100: Batch Loss = 0.1594\n",
            "Iteration 40, Batch 76/100: Batch Loss = 0.2711\n",
            "Iteration 40, Batch 77/100: Batch Loss = 0.1325\n",
            "Iteration 40, Batch 78/100: Batch Loss = 0.3098\n",
            "Iteration 40, Batch 79/100: Batch Loss = 0.2542\n",
            "Iteration 40, Batch 80/100: Batch Loss = 0.1824\n",
            "Iteration 40, Batch 81/100: Batch Loss = 0.1461\n",
            "Iteration 40, Batch 82/100: Batch Loss = 0.1087\n",
            "Iteration 40, Batch 83/100: Batch Loss = 0.0434\n",
            "Iteration 40, Batch 84/100: Batch Loss = 0.0969\n",
            "Iteration 40, Batch 85/100: Batch Loss = 0.0781\n",
            "Iteration 40, Batch 86/100: Batch Loss = 0.1848\n",
            "Iteration 40, Batch 87/100: Batch Loss = 0.1472\n",
            "Iteration 40, Batch 88/100: Batch Loss = 0.1746\n",
            "Iteration 40, Batch 89/100: Batch Loss = 0.1095\n",
            "Iteration 40, Batch 90/100: Batch Loss = 0.0700\n",
            "Iteration 40, Batch 91/100: Batch Loss = 0.1440\n",
            "Iteration 40, Batch 92/100: Batch Loss = 0.0574\n",
            "Iteration 40, Batch 93/100: Batch Loss = 0.2248\n",
            "Iteration 40, Batch 94/100: Batch Loss = 0.0350\n",
            "Iteration 40, Batch 95/100: Batch Loss = 0.0456\n",
            "Iteration 40, Batch 96/100: Batch Loss = 0.0638\n",
            "Iteration 40, Batch 97/100: Batch Loss = 0.1166\n",
            "Iteration 40, Batch 98/100: Batch Loss = 0.0541\n",
            "Iteration 40, Batch 99/100: Batch Loss = 0.0358\n",
            "Iteration 40, Batch 100/100: Batch Loss = 0.1562\n",
            "Iteration 40: Train Loss = 0.1240, Val Loss = 0.1715\n",
            "Iteration 41, Batch 1/100: Batch Loss = 0.0457\n",
            "Iteration 41, Batch 2/100: Batch Loss = 0.0619\n",
            "Iteration 41, Batch 3/100: Batch Loss = 0.0517\n",
            "Iteration 41, Batch 4/100: Batch Loss = 0.2233\n",
            "Iteration 41, Batch 5/100: Batch Loss = 0.1036\n",
            "Iteration 41, Batch 6/100: Batch Loss = 0.2276\n",
            "Iteration 41, Batch 7/100: Batch Loss = 0.2150\n",
            "Iteration 41, Batch 8/100: Batch Loss = 0.0742\n",
            "Iteration 41, Batch 9/100: Batch Loss = 0.1483\n",
            "Iteration 41, Batch 10/100: Batch Loss = 0.1929\n",
            "Iteration 41, Batch 11/100: Batch Loss = 0.0371\n",
            "Iteration 41, Batch 12/100: Batch Loss = 0.1962\n",
            "Iteration 41, Batch 13/100: Batch Loss = 0.0762\n",
            "Iteration 41, Batch 14/100: Batch Loss = 0.1685\n",
            "Iteration 41, Batch 15/100: Batch Loss = 0.0979\n",
            "Iteration 41, Batch 16/100: Batch Loss = 0.0926\n",
            "Iteration 41, Batch 17/100: Batch Loss = 0.1240\n",
            "Iteration 41, Batch 18/100: Batch Loss = 0.2477\n",
            "Iteration 41, Batch 19/100: Batch Loss = 0.0538\n",
            "Iteration 41, Batch 20/100: Batch Loss = 0.0619\n",
            "Iteration 41, Batch 21/100: Batch Loss = 0.2139\n",
            "Iteration 41, Batch 22/100: Batch Loss = 0.1817\n",
            "Iteration 41, Batch 23/100: Batch Loss = 0.0331\n",
            "Iteration 41, Batch 24/100: Batch Loss = 0.1220\n",
            "Iteration 41, Batch 25/100: Batch Loss = 0.0767\n",
            "Iteration 41, Batch 26/100: Batch Loss = 0.0702\n",
            "Iteration 41, Batch 27/100: Batch Loss = 0.1254\n",
            "Iteration 41, Batch 28/100: Batch Loss = 0.3591\n",
            "Iteration 41, Batch 29/100: Batch Loss = 0.3174\n",
            "Iteration 41, Batch 30/100: Batch Loss = 0.1215\n",
            "Iteration 41, Batch 31/100: Batch Loss = 0.0472\n",
            "Iteration 41, Batch 32/100: Batch Loss = 0.0170\n",
            "Iteration 41, Batch 33/100: Batch Loss = 0.0573\n",
            "Iteration 41, Batch 34/100: Batch Loss = 0.0326\n",
            "Iteration 41, Batch 35/100: Batch Loss = 0.2049\n",
            "Iteration 41, Batch 36/100: Batch Loss = 0.1790\n",
            "Iteration 41, Batch 37/100: Batch Loss = 0.0284\n",
            "Iteration 41, Batch 38/100: Batch Loss = 0.0748\n",
            "Iteration 41, Batch 39/100: Batch Loss = 0.0321\n",
            "Iteration 41, Batch 40/100: Batch Loss = 0.1839\n",
            "Iteration 41, Batch 41/100: Batch Loss = 0.1302\n",
            "Iteration 41, Batch 42/100: Batch Loss = 0.0660\n",
            "Iteration 41, Batch 43/100: Batch Loss = 0.1823\n",
            "Iteration 41, Batch 44/100: Batch Loss = 0.0778\n",
            "Iteration 41, Batch 45/100: Batch Loss = 0.0704\n",
            "Iteration 41, Batch 46/100: Batch Loss = 0.2165\n",
            "Iteration 41, Batch 47/100: Batch Loss = 0.0692\n",
            "Iteration 41, Batch 48/100: Batch Loss = 0.4356\n",
            "Iteration 41, Batch 49/100: Batch Loss = 0.3522\n",
            "Iteration 41, Batch 50/100: Batch Loss = 0.1266\n",
            "Iteration 41, Batch 51/100: Batch Loss = 0.2075\n",
            "Iteration 41, Batch 52/100: Batch Loss = 0.1021\n",
            "Iteration 41, Batch 53/100: Batch Loss = 0.1489\n",
            "Iteration 41, Batch 54/100: Batch Loss = 0.1554\n",
            "Iteration 41, Batch 55/100: Batch Loss = 0.0427\n",
            "Iteration 41, Batch 56/100: Batch Loss = 0.0718\n",
            "Iteration 41, Batch 57/100: Batch Loss = 0.1057\n",
            "Iteration 41, Batch 58/100: Batch Loss = 0.1622\n",
            "Iteration 41, Batch 59/100: Batch Loss = 0.0283\n",
            "Iteration 41, Batch 60/100: Batch Loss = 0.0789\n",
            "Iteration 41, Batch 61/100: Batch Loss = 0.0921\n",
            "Iteration 41, Batch 62/100: Batch Loss = 0.0428\n",
            "Iteration 41, Batch 63/100: Batch Loss = 0.1031\n",
            "Iteration 41, Batch 64/100: Batch Loss = 0.0358\n",
            "Iteration 41, Batch 65/100: Batch Loss = 0.0286\n",
            "Iteration 41, Batch 66/100: Batch Loss = 0.2049\n",
            "Iteration 41, Batch 67/100: Batch Loss = 0.0465\n",
            "Iteration 41, Batch 68/100: Batch Loss = 0.0479\n",
            "Iteration 41, Batch 69/100: Batch Loss = 0.0446\n",
            "Iteration 41, Batch 70/100: Batch Loss = 0.2131\n",
            "Iteration 41, Batch 71/100: Batch Loss = 0.0476\n",
            "Iteration 41, Batch 72/100: Batch Loss = 0.1258\n",
            "Iteration 41, Batch 73/100: Batch Loss = 0.0514\n",
            "Iteration 41, Batch 74/100: Batch Loss = 0.1765\n",
            "Iteration 41, Batch 75/100: Batch Loss = 0.1751\n",
            "Iteration 41, Batch 76/100: Batch Loss = 0.1441\n",
            "Iteration 41, Batch 77/100: Batch Loss = 0.1543\n",
            "Iteration 41, Batch 78/100: Batch Loss = 0.1683\n",
            "Iteration 41, Batch 79/100: Batch Loss = 0.1308\n",
            "Iteration 41, Batch 80/100: Batch Loss = 0.0614\n",
            "Iteration 41, Batch 81/100: Batch Loss = 0.0671\n",
            "Iteration 41, Batch 82/100: Batch Loss = 0.1994\n",
            "Iteration 41, Batch 83/100: Batch Loss = 0.1092\n",
            "Iteration 41, Batch 84/100: Batch Loss = 0.0465\n",
            "Iteration 41, Batch 85/100: Batch Loss = 0.1961\n",
            "Iteration 41, Batch 86/100: Batch Loss = 0.0779\n",
            "Iteration 41, Batch 87/100: Batch Loss = 0.1515\n",
            "Iteration 41, Batch 88/100: Batch Loss = 0.1153\n",
            "Iteration 41, Batch 89/100: Batch Loss = 0.0524\n",
            "Iteration 41, Batch 90/100: Batch Loss = 0.1119\n",
            "Iteration 41, Batch 91/100: Batch Loss = 0.0472\n",
            "Iteration 41, Batch 92/100: Batch Loss = 0.1372\n",
            "Iteration 41, Batch 93/100: Batch Loss = 0.0437\n",
            "Iteration 41, Batch 94/100: Batch Loss = 0.2217\n",
            "Iteration 41, Batch 95/100: Batch Loss = 0.1538\n",
            "Iteration 41, Batch 96/100: Batch Loss = 0.0507\n",
            "Iteration 41, Batch 97/100: Batch Loss = 0.1121\n",
            "Iteration 41, Batch 98/100: Batch Loss = 0.0926\n",
            "Iteration 41, Batch 99/100: Batch Loss = 0.2346\n",
            "Iteration 41, Batch 100/100: Batch Loss = 0.1184\n",
            "Iteration 41: Train Loss = 0.1224, Val Loss = 0.1644\n",
            "Iteration 42, Batch 1/100: Batch Loss = 0.0533\n",
            "Iteration 42, Batch 2/100: Batch Loss = 0.0878\n",
            "Iteration 42, Batch 3/100: Batch Loss = 0.1142\n",
            "Iteration 42, Batch 4/100: Batch Loss = 0.1473\n",
            "Iteration 42, Batch 5/100: Batch Loss = 0.1400\n",
            "Iteration 42, Batch 6/100: Batch Loss = 0.0959\n",
            "Iteration 42, Batch 7/100: Batch Loss = 0.0462\n",
            "Iteration 42, Batch 8/100: Batch Loss = 0.1364\n",
            "Iteration 42, Batch 9/100: Batch Loss = 0.2442\n",
            "Iteration 42, Batch 10/100: Batch Loss = 0.0702\n",
            "Iteration 42, Batch 11/100: Batch Loss = 0.1478\n",
            "Iteration 42, Batch 12/100: Batch Loss = 0.0498\n",
            "Iteration 42, Batch 13/100: Batch Loss = 0.1557\n",
            "Iteration 42, Batch 14/100: Batch Loss = 0.1574\n",
            "Iteration 42, Batch 15/100: Batch Loss = 0.1847\n",
            "Iteration 42, Batch 16/100: Batch Loss = 0.1529\n",
            "Iteration 42, Batch 17/100: Batch Loss = 0.1850\n",
            "Iteration 42, Batch 18/100: Batch Loss = 0.1249\n",
            "Iteration 42, Batch 19/100: Batch Loss = 0.0658\n",
            "Iteration 42, Batch 20/100: Batch Loss = 0.1730\n",
            "Iteration 42, Batch 21/100: Batch Loss = 0.1541\n",
            "Iteration 42, Batch 22/100: Batch Loss = 0.0459\n",
            "Iteration 42, Batch 23/100: Batch Loss = 0.1140\n",
            "Iteration 42, Batch 24/100: Batch Loss = 0.0736\n",
            "Iteration 42, Batch 25/100: Batch Loss = 0.0663\n",
            "Iteration 42, Batch 26/100: Batch Loss = 0.2107\n",
            "Iteration 42, Batch 27/100: Batch Loss = 0.0584\n",
            "Iteration 42, Batch 28/100: Batch Loss = 0.3311\n",
            "Iteration 42, Batch 29/100: Batch Loss = 0.0654\n",
            "Iteration 42, Batch 30/100: Batch Loss = 0.1525\n",
            "Iteration 42, Batch 31/100: Batch Loss = 0.2348\n",
            "Iteration 42, Batch 32/100: Batch Loss = 0.0771\n",
            "Iteration 42, Batch 33/100: Batch Loss = 0.2313\n",
            "Iteration 42, Batch 34/100: Batch Loss = 0.0667\n",
            "Iteration 42, Batch 35/100: Batch Loss = 0.0198\n",
            "Iteration 42, Batch 36/100: Batch Loss = 0.0506\n",
            "Iteration 42, Batch 37/100: Batch Loss = 0.0394\n",
            "Iteration 42, Batch 38/100: Batch Loss = 0.0785\n",
            "Iteration 42, Batch 39/100: Batch Loss = 0.1932\n",
            "Iteration 42, Batch 40/100: Batch Loss = 0.0585\n",
            "Iteration 42, Batch 41/100: Batch Loss = 0.2379\n",
            "Iteration 42, Batch 42/100: Batch Loss = 0.0341\n",
            "Iteration 42, Batch 43/100: Batch Loss = 0.2235\n",
            "Iteration 42, Batch 44/100: Batch Loss = 0.1934\n",
            "Iteration 42, Batch 45/100: Batch Loss = 0.1346\n",
            "Iteration 42, Batch 46/100: Batch Loss = 0.0961\n",
            "Iteration 42, Batch 47/100: Batch Loss = 0.1959\n",
            "Iteration 42, Batch 48/100: Batch Loss = 0.0372\n",
            "Iteration 42, Batch 49/100: Batch Loss = 0.0901\n",
            "Iteration 42, Batch 50/100: Batch Loss = 0.0316\n",
            "Iteration 42, Batch 51/100: Batch Loss = 0.0783\n",
            "Iteration 42, Batch 52/100: Batch Loss = 0.0943\n",
            "Iteration 42, Batch 53/100: Batch Loss = 0.0588\n",
            "Iteration 42, Batch 54/100: Batch Loss = 0.1897\n",
            "Iteration 42, Batch 55/100: Batch Loss = 0.0177\n",
            "Iteration 42, Batch 56/100: Batch Loss = 0.1465\n",
            "Iteration 42, Batch 57/100: Batch Loss = 0.0231\n",
            "Iteration 42, Batch 58/100: Batch Loss = 0.0643\n",
            "Iteration 42, Batch 59/100: Batch Loss = 0.2115\n",
            "Iteration 42, Batch 60/100: Batch Loss = 0.0449\n",
            "Iteration 42, Batch 61/100: Batch Loss = 0.0616\n",
            "Iteration 42, Batch 62/100: Batch Loss = 0.0146\n",
            "Iteration 42, Batch 63/100: Batch Loss = 0.2974\n",
            "Iteration 42, Batch 64/100: Batch Loss = 0.1271\n",
            "Iteration 42, Batch 65/100: Batch Loss = 0.0677\n",
            "Iteration 42, Batch 66/100: Batch Loss = 0.0315\n",
            "Iteration 42, Batch 67/100: Batch Loss = 0.0470\n",
            "Iteration 42, Batch 68/100: Batch Loss = 0.1676\n",
            "Iteration 42, Batch 69/100: Batch Loss = 0.0925\n",
            "Iteration 42, Batch 70/100: Batch Loss = 0.2588\n",
            "Iteration 42, Batch 71/100: Batch Loss = 0.0284\n",
            "Iteration 42, Batch 72/100: Batch Loss = 0.1213\n",
            "Iteration 42, Batch 73/100: Batch Loss = 0.1017\n",
            "Iteration 42, Batch 74/100: Batch Loss = 0.1199\n",
            "Iteration 42, Batch 75/100: Batch Loss = 0.1697\n",
            "Iteration 42, Batch 76/100: Batch Loss = 0.2415\n",
            "Iteration 42, Batch 77/100: Batch Loss = 0.0926\n",
            "Iteration 42, Batch 78/100: Batch Loss = 0.1040\n",
            "Iteration 42, Batch 79/100: Batch Loss = 0.0462\n",
            "Iteration 42, Batch 80/100: Batch Loss = 0.2415\n",
            "Iteration 42, Batch 81/100: Batch Loss = 0.1952\n",
            "Iteration 42, Batch 82/100: Batch Loss = 0.1451\n",
            "Iteration 42, Batch 83/100: Batch Loss = 0.1340\n",
            "Iteration 42, Batch 84/100: Batch Loss = 0.2552\n",
            "Iteration 42, Batch 85/100: Batch Loss = 0.0941\n",
            "Iteration 42, Batch 86/100: Batch Loss = 0.1084\n",
            "Iteration 42, Batch 87/100: Batch Loss = 0.2129\n",
            "Iteration 42, Batch 88/100: Batch Loss = 0.1150\n",
            "Iteration 42, Batch 89/100: Batch Loss = 0.2287\n",
            "Iteration 42, Batch 90/100: Batch Loss = 0.2700\n",
            "Iteration 42, Batch 91/100: Batch Loss = 0.2587\n",
            "Iteration 42, Batch 92/100: Batch Loss = 0.1477\n",
            "Iteration 42, Batch 93/100: Batch Loss = 0.1022\n",
            "Iteration 42, Batch 94/100: Batch Loss = 0.0910\n",
            "Iteration 42, Batch 95/100: Batch Loss = 0.0332\n",
            "Iteration 42, Batch 96/100: Batch Loss = 0.1393\n",
            "Iteration 42, Batch 97/100: Batch Loss = 0.0893\n",
            "Iteration 42, Batch 98/100: Batch Loss = 0.0565\n",
            "Iteration 42, Batch 99/100: Batch Loss = 0.1937\n",
            "Iteration 42, Batch 100/100: Batch Loss = 0.0213\n",
            "Iteration 42: Train Loss = 0.1238, Val Loss = 0.1589\n",
            "Iteration 43, Batch 1/100: Batch Loss = 0.0500\n",
            "Iteration 43, Batch 2/100: Batch Loss = 0.0342\n",
            "Iteration 43, Batch 3/100: Batch Loss = 0.1441\n",
            "Iteration 43, Batch 4/100: Batch Loss = 0.1114\n",
            "Iteration 43, Batch 5/100: Batch Loss = 0.0383\n",
            "Iteration 43, Batch 6/100: Batch Loss = 0.0972\n",
            "Iteration 43, Batch 7/100: Batch Loss = 0.2161\n",
            "Iteration 43, Batch 8/100: Batch Loss = 0.1045\n",
            "Iteration 43, Batch 9/100: Batch Loss = 0.0760\n",
            "Iteration 43, Batch 10/100: Batch Loss = 0.1992\n",
            "Iteration 43, Batch 11/100: Batch Loss = 0.1286\n",
            "Iteration 43, Batch 12/100: Batch Loss = 0.0982\n",
            "Iteration 43, Batch 13/100: Batch Loss = 0.1498\n",
            "Iteration 43, Batch 14/100: Batch Loss = 0.0287\n",
            "Iteration 43, Batch 15/100: Batch Loss = 0.0929\n",
            "Iteration 43, Batch 16/100: Batch Loss = 0.1658\n",
            "Iteration 43, Batch 17/100: Batch Loss = 0.2523\n",
            "Iteration 43, Batch 18/100: Batch Loss = 0.3711\n",
            "Iteration 43, Batch 19/100: Batch Loss = 0.0875\n",
            "Iteration 43, Batch 20/100: Batch Loss = 0.1797\n",
            "Iteration 43, Batch 21/100: Batch Loss = 0.0839\n",
            "Iteration 43, Batch 22/100: Batch Loss = 0.1587\n",
            "Iteration 43, Batch 23/100: Batch Loss = 0.0421\n",
            "Iteration 43, Batch 24/100: Batch Loss = 0.0538\n",
            "Iteration 43, Batch 25/100: Batch Loss = 0.1053\n",
            "Iteration 43, Batch 26/100: Batch Loss = 0.0740\n",
            "Iteration 43, Batch 27/100: Batch Loss = 0.1534\n",
            "Iteration 43, Batch 28/100: Batch Loss = 0.2325\n",
            "Iteration 43, Batch 29/100: Batch Loss = 0.1953\n",
            "Iteration 43, Batch 30/100: Batch Loss = 0.0496\n",
            "Iteration 43, Batch 31/100: Batch Loss = 0.1705\n",
            "Iteration 43, Batch 32/100: Batch Loss = 0.1659\n",
            "Iteration 43, Batch 33/100: Batch Loss = 0.1923\n",
            "Iteration 43, Batch 34/100: Batch Loss = 0.1075\n",
            "Iteration 43, Batch 35/100: Batch Loss = 0.1112\n",
            "Iteration 43, Batch 36/100: Batch Loss = 0.2557\n",
            "Iteration 43, Batch 37/100: Batch Loss = 0.1123\n",
            "Iteration 43, Batch 38/100: Batch Loss = 0.0878\n",
            "Iteration 43, Batch 39/100: Batch Loss = 0.1088\n",
            "Iteration 43, Batch 40/100: Batch Loss = 0.1039\n",
            "Iteration 43, Batch 41/100: Batch Loss = 0.0791\n",
            "Iteration 43, Batch 42/100: Batch Loss = 0.0526\n",
            "Iteration 43, Batch 43/100: Batch Loss = 0.0466\n",
            "Iteration 43, Batch 44/100: Batch Loss = 0.1705\n",
            "Iteration 43, Batch 45/100: Batch Loss = 0.1098\n",
            "Iteration 43, Batch 46/100: Batch Loss = 0.0838\n",
            "Iteration 43, Batch 47/100: Batch Loss = 0.2417\n",
            "Iteration 43, Batch 48/100: Batch Loss = 0.2638\n",
            "Iteration 43, Batch 49/100: Batch Loss = 0.0548\n",
            "Iteration 43, Batch 50/100: Batch Loss = 0.0712\n",
            "Iteration 43, Batch 51/100: Batch Loss = 0.2471\n",
            "Iteration 43, Batch 52/100: Batch Loss = 0.0428\n",
            "Iteration 43, Batch 53/100: Batch Loss = 0.1126\n",
            "Iteration 43, Batch 54/100: Batch Loss = 0.0860\n",
            "Iteration 43, Batch 55/100: Batch Loss = 0.0784\n",
            "Iteration 43, Batch 56/100: Batch Loss = 0.0940\n",
            "Iteration 43, Batch 57/100: Batch Loss = 0.1899\n",
            "Iteration 43, Batch 58/100: Batch Loss = 0.0765\n",
            "Iteration 43, Batch 59/100: Batch Loss = 0.0706\n",
            "Iteration 43, Batch 60/100: Batch Loss = 0.1766\n",
            "Iteration 43, Batch 61/100: Batch Loss = 0.3039\n",
            "Iteration 43, Batch 62/100: Batch Loss = 0.0411\n",
            "Iteration 43, Batch 63/100: Batch Loss = 0.0542\n",
            "Iteration 43, Batch 64/100: Batch Loss = 0.1566\n",
            "Iteration 43, Batch 65/100: Batch Loss = 0.1413\n",
            "Iteration 43, Batch 66/100: Batch Loss = 0.0191\n",
            "Iteration 43, Batch 67/100: Batch Loss = 0.1505\n",
            "Iteration 43, Batch 68/100: Batch Loss = 0.0617\n",
            "Iteration 43, Batch 69/100: Batch Loss = 0.0436\n",
            "Iteration 43, Batch 70/100: Batch Loss = 0.0690\n",
            "Iteration 43, Batch 71/100: Batch Loss = 0.2029\n",
            "Iteration 43, Batch 72/100: Batch Loss = 0.1111\n",
            "Iteration 43, Batch 73/100: Batch Loss = 0.2059\n",
            "Iteration 43, Batch 74/100: Batch Loss = 0.4625\n",
            "Iteration 43, Batch 75/100: Batch Loss = 0.0343\n",
            "Iteration 43, Batch 76/100: Batch Loss = 0.0683\n",
            "Iteration 43, Batch 77/100: Batch Loss = 0.1412\n",
            "Iteration 43, Batch 78/100: Batch Loss = 0.0326\n",
            "Iteration 43, Batch 79/100: Batch Loss = 0.0939\n",
            "Iteration 43, Batch 80/100: Batch Loss = 0.0760\n",
            "Iteration 43, Batch 81/100: Batch Loss = 0.2224\n",
            "Iteration 43, Batch 82/100: Batch Loss = 0.1479\n",
            "Iteration 43, Batch 83/100: Batch Loss = 0.1506\n",
            "Iteration 43, Batch 84/100: Batch Loss = 0.1220\n",
            "Iteration 43, Batch 85/100: Batch Loss = 0.0379\n",
            "Iteration 43, Batch 86/100: Batch Loss = 0.1824\n",
            "Iteration 43, Batch 87/100: Batch Loss = 0.1997\n",
            "Iteration 43, Batch 88/100: Batch Loss = 0.0953\n",
            "Iteration 43, Batch 89/100: Batch Loss = 0.1329\n",
            "Iteration 43, Batch 90/100: Batch Loss = 0.1097\n",
            "Iteration 43, Batch 91/100: Batch Loss = 0.0801\n",
            "Iteration 43, Batch 92/100: Batch Loss = 0.2694\n",
            "Iteration 43, Batch 93/100: Batch Loss = 0.0887\n",
            "Iteration 43, Batch 94/100: Batch Loss = 0.1145\n",
            "Iteration 43, Batch 95/100: Batch Loss = 0.0470\n",
            "Iteration 43, Batch 96/100: Batch Loss = 0.0295\n",
            "Iteration 43, Batch 97/100: Batch Loss = 0.0599\n",
            "Iteration 43, Batch 98/100: Batch Loss = 0.0957\n",
            "Iteration 43, Batch 99/100: Batch Loss = 0.0783\n",
            "Iteration 43, Batch 100/100: Batch Loss = 0.0509\n",
            "Iteration 43: Train Loss = 0.1232, Val Loss = 0.1510\n",
            "Iteration 44, Batch 1/100: Batch Loss = 0.0847\n",
            "Iteration 44, Batch 2/100: Batch Loss = 0.0405\n",
            "Iteration 44, Batch 3/100: Batch Loss = 0.1463\n",
            "Iteration 44, Batch 4/100: Batch Loss = 0.1041\n",
            "Iteration 44, Batch 5/100: Batch Loss = 0.1575\n",
            "Iteration 44, Batch 6/100: Batch Loss = 0.1555\n",
            "Iteration 44, Batch 7/100: Batch Loss = 0.0657\n",
            "Iteration 44, Batch 8/100: Batch Loss = 0.3093\n",
            "Iteration 44, Batch 9/100: Batch Loss = 0.1629\n",
            "Iteration 44, Batch 10/100: Batch Loss = 0.1405\n",
            "Iteration 44, Batch 11/100: Batch Loss = 0.0509\n",
            "Iteration 44, Batch 12/100: Batch Loss = 0.0524\n",
            "Iteration 44, Batch 13/100: Batch Loss = 0.0787\n",
            "Iteration 44, Batch 14/100: Batch Loss = 0.0324\n",
            "Iteration 44, Batch 15/100: Batch Loss = 0.0754\n",
            "Iteration 44, Batch 16/100: Batch Loss = 0.0314\n",
            "Iteration 44, Batch 17/100: Batch Loss = 0.1334\n",
            "Iteration 44, Batch 18/100: Batch Loss = 0.1846\n",
            "Iteration 44, Batch 19/100: Batch Loss = 0.0375\n",
            "Iteration 44, Batch 20/100: Batch Loss = 0.0508\n",
            "Iteration 44, Batch 21/100: Batch Loss = 0.0387\n",
            "Iteration 44, Batch 22/100: Batch Loss = 0.2210\n",
            "Iteration 44, Batch 23/100: Batch Loss = 0.0317\n",
            "Iteration 44, Batch 24/100: Batch Loss = 0.1884\n",
            "Iteration 44, Batch 25/100: Batch Loss = 0.0414\n",
            "Iteration 44, Batch 26/100: Batch Loss = 0.1464\n",
            "Iteration 44, Batch 27/100: Batch Loss = 0.1114\n",
            "Iteration 44, Batch 28/100: Batch Loss = 0.0343\n",
            "Iteration 44, Batch 29/100: Batch Loss = 0.0364\n",
            "Iteration 44, Batch 30/100: Batch Loss = 0.2219\n",
            "Iteration 44, Batch 31/100: Batch Loss = 0.0508\n",
            "Iteration 44, Batch 32/100: Batch Loss = 0.1554\n",
            "Iteration 44, Batch 33/100: Batch Loss = 0.0488\n",
            "Iteration 44, Batch 34/100: Batch Loss = 0.0378\n",
            "Iteration 44, Batch 35/100: Batch Loss = 0.4595\n",
            "Iteration 44, Batch 36/100: Batch Loss = 0.0406\n",
            "Iteration 44, Batch 37/100: Batch Loss = 0.0511\n",
            "Iteration 44, Batch 38/100: Batch Loss = 0.0380\n",
            "Iteration 44, Batch 39/100: Batch Loss = 0.0883\n",
            "Iteration 44, Batch 40/100: Batch Loss = 0.0397\n",
            "Iteration 44, Batch 41/100: Batch Loss = 0.3117\n",
            "Iteration 44, Batch 42/100: Batch Loss = 0.0186\n",
            "Iteration 44, Batch 43/100: Batch Loss = 0.2496\n",
            "Iteration 44, Batch 44/100: Batch Loss = 0.1091\n",
            "Iteration 44, Batch 45/100: Batch Loss = 0.0714\n",
            "Iteration 44, Batch 46/100: Batch Loss = 0.0732\n",
            "Iteration 44, Batch 47/100: Batch Loss = 0.1072\n",
            "Iteration 44, Batch 48/100: Batch Loss = 0.2158\n",
            "Iteration 44, Batch 49/100: Batch Loss = 0.0372\n",
            "Iteration 44, Batch 50/100: Batch Loss = 0.1014\n",
            "Iteration 44, Batch 51/100: Batch Loss = 0.4068\n",
            "Iteration 44, Batch 52/100: Batch Loss = 0.3274\n",
            "Iteration 44, Batch 53/100: Batch Loss = 0.0834\n",
            "Iteration 44, Batch 54/100: Batch Loss = 0.1280\n",
            "Iteration 44, Batch 55/100: Batch Loss = 0.1839\n",
            "Iteration 44, Batch 56/100: Batch Loss = 0.1640\n",
            "Iteration 44, Batch 57/100: Batch Loss = 0.1047\n",
            "Iteration 44, Batch 58/100: Batch Loss = 0.2101\n",
            "Iteration 44, Batch 59/100: Batch Loss = 0.1802\n",
            "Iteration 44, Batch 60/100: Batch Loss = 0.1365\n",
            "Iteration 44, Batch 61/100: Batch Loss = 0.0714\n",
            "Iteration 44, Batch 62/100: Batch Loss = 0.0703\n",
            "Iteration 44, Batch 63/100: Batch Loss = 0.1108\n",
            "Iteration 44, Batch 64/100: Batch Loss = 0.0960\n",
            "Iteration 44, Batch 65/100: Batch Loss = 0.0611\n",
            "Iteration 44, Batch 66/100: Batch Loss = 0.0639\n",
            "Iteration 44, Batch 67/100: Batch Loss = 0.0694\n",
            "Iteration 44, Batch 68/100: Batch Loss = 0.0450\n",
            "Iteration 44, Batch 69/100: Batch Loss = 0.1002\n",
            "Iteration 44, Batch 70/100: Batch Loss = 0.1007\n",
            "Iteration 44, Batch 71/100: Batch Loss = 0.3769\n",
            "Iteration 44, Batch 72/100: Batch Loss = 0.1761\n",
            "Iteration 44, Batch 73/100: Batch Loss = 0.0211\n",
            "Iteration 44, Batch 74/100: Batch Loss = 0.2675\n",
            "Iteration 44, Batch 75/100: Batch Loss = 0.0840\n",
            "Iteration 44, Batch 76/100: Batch Loss = 0.0136\n",
            "Iteration 44, Batch 77/100: Batch Loss = 0.2452\n",
            "Iteration 44, Batch 78/100: Batch Loss = 0.0782\n",
            "Iteration 44, Batch 79/100: Batch Loss = 0.0424\n",
            "Iteration 44, Batch 80/100: Batch Loss = 0.2122\n",
            "Iteration 44, Batch 81/100: Batch Loss = 0.1738\n",
            "Iteration 44, Batch 82/100: Batch Loss = 0.1881\n",
            "Iteration 44, Batch 83/100: Batch Loss = 0.0750\n",
            "Iteration 44, Batch 84/100: Batch Loss = 0.2430\n",
            "Iteration 44, Batch 85/100: Batch Loss = 0.1110\n",
            "Iteration 44, Batch 86/100: Batch Loss = 0.1753\n",
            "Iteration 44, Batch 87/100: Batch Loss = 0.0941\n",
            "Iteration 44, Batch 88/100: Batch Loss = 0.1096\n",
            "Iteration 44, Batch 89/100: Batch Loss = 0.1273\n",
            "Iteration 44, Batch 90/100: Batch Loss = 0.2204\n",
            "Iteration 44, Batch 91/100: Batch Loss = 0.0477\n",
            "Iteration 44, Batch 92/100: Batch Loss = 0.1230\n",
            "Iteration 44, Batch 93/100: Batch Loss = 0.1499\n",
            "Iteration 44, Batch 94/100: Batch Loss = 0.0489\n",
            "Iteration 44, Batch 95/100: Batch Loss = 0.4432\n",
            "Iteration 44, Batch 96/100: Batch Loss = 0.1689\n",
            "Iteration 44, Batch 97/100: Batch Loss = 0.0377\n",
            "Iteration 44, Batch 98/100: Batch Loss = 0.1127\n",
            "Iteration 44, Batch 99/100: Batch Loss = 0.0691\n",
            "Iteration 44, Batch 100/100: Batch Loss = 0.0396\n",
            "Iteration 44: Train Loss = 0.1246, Val Loss = 0.1522\n",
            "Iteration 45, Batch 1/100: Batch Loss = 0.3237\n",
            "Iteration 45, Batch 2/100: Batch Loss = 0.1083\n",
            "Iteration 45, Batch 3/100: Batch Loss = 0.0530\n",
            "Iteration 45, Batch 4/100: Batch Loss = 0.0422\n",
            "Iteration 45, Batch 5/100: Batch Loss = 0.0563\n",
            "Iteration 45, Batch 6/100: Batch Loss = 0.0678\n",
            "Iteration 45, Batch 7/100: Batch Loss = 0.2242\n",
            "Iteration 45, Batch 8/100: Batch Loss = 0.0594\n",
            "Iteration 45, Batch 9/100: Batch Loss = 0.0958\n",
            "Iteration 45, Batch 10/100: Batch Loss = 0.2144\n",
            "Iteration 45, Batch 11/100: Batch Loss = 0.1164\n",
            "Iteration 45, Batch 12/100: Batch Loss = 0.1662\n",
            "Iteration 45, Batch 13/100: Batch Loss = 0.0906\n",
            "Iteration 45, Batch 14/100: Batch Loss = 0.0681\n",
            "Iteration 45, Batch 15/100: Batch Loss = 0.0770\n",
            "Iteration 45, Batch 16/100: Batch Loss = 0.1407\n",
            "Iteration 45, Batch 17/100: Batch Loss = 0.0656\n",
            "Iteration 45, Batch 18/100: Batch Loss = 0.1162\n",
            "Iteration 45, Batch 19/100: Batch Loss = 0.1002\n",
            "Iteration 45, Batch 20/100: Batch Loss = 0.1034\n",
            "Iteration 45, Batch 21/100: Batch Loss = 0.0559\n",
            "Iteration 45, Batch 22/100: Batch Loss = 0.0915\n",
            "Iteration 45, Batch 23/100: Batch Loss = 0.3234\n",
            "Iteration 45, Batch 24/100: Batch Loss = 0.1543\n",
            "Iteration 45, Batch 25/100: Batch Loss = 0.1308\n",
            "Iteration 45, Batch 26/100: Batch Loss = 0.1848\n",
            "Iteration 45, Batch 27/100: Batch Loss = 0.0904\n",
            "Iteration 45, Batch 28/100: Batch Loss = 0.1139\n",
            "Iteration 45, Batch 29/100: Batch Loss = 0.1800\n",
            "Iteration 45, Batch 30/100: Batch Loss = 0.1082\n",
            "Iteration 45, Batch 31/100: Batch Loss = 0.0762\n",
            "Iteration 45, Batch 32/100: Batch Loss = 0.1592\n",
            "Iteration 45, Batch 33/100: Batch Loss = 0.0328\n",
            "Iteration 45, Batch 34/100: Batch Loss = 0.3005\n",
            "Iteration 45, Batch 35/100: Batch Loss = 0.1453\n",
            "Iteration 45, Batch 36/100: Batch Loss = 0.0903\n",
            "Iteration 45, Batch 37/100: Batch Loss = 0.0911\n",
            "Iteration 45, Batch 38/100: Batch Loss = 0.0790\n",
            "Iteration 45, Batch 39/100: Batch Loss = 0.4090\n",
            "Iteration 45, Batch 40/100: Batch Loss = 0.1390\n",
            "Iteration 45, Batch 41/100: Batch Loss = 0.0943\n",
            "Iteration 45, Batch 42/100: Batch Loss = 0.1152\n",
            "Iteration 45, Batch 43/100: Batch Loss = 0.2535\n",
            "Iteration 45, Batch 44/100: Batch Loss = 0.1405\n",
            "Iteration 45, Batch 45/100: Batch Loss = 0.0459\n",
            "Iteration 45, Batch 46/100: Batch Loss = 0.0463\n",
            "Iteration 45, Batch 47/100: Batch Loss = 0.0443\n",
            "Iteration 45, Batch 48/100: Batch Loss = 0.0792\n",
            "Iteration 45, Batch 49/100: Batch Loss = 0.2237\n",
            "Iteration 45, Batch 50/100: Batch Loss = 0.4130\n",
            "Iteration 45, Batch 51/100: Batch Loss = 0.0352\n",
            "Iteration 45, Batch 52/100: Batch Loss = 0.0980\n",
            "Iteration 45, Batch 53/100: Batch Loss = 0.1778\n",
            "Iteration 45, Batch 54/100: Batch Loss = 0.0680\n",
            "Iteration 45, Batch 55/100: Batch Loss = 0.0241\n",
            "Iteration 45, Batch 56/100: Batch Loss = 0.1429\n",
            "Iteration 45, Batch 57/100: Batch Loss = 0.3763\n",
            "Iteration 45, Batch 58/100: Batch Loss = 0.0229\n",
            "Iteration 45, Batch 59/100: Batch Loss = 0.2648\n",
            "Iteration 45, Batch 60/100: Batch Loss = 0.0859\n",
            "Iteration 45, Batch 61/100: Batch Loss = 0.1125\n",
            "Iteration 45, Batch 62/100: Batch Loss = 0.1186\n",
            "Iteration 45, Batch 63/100: Batch Loss = 0.0851\n",
            "Iteration 45, Batch 64/100: Batch Loss = 0.2372\n",
            "Iteration 45, Batch 65/100: Batch Loss = 0.0866\n",
            "Iteration 45, Batch 66/100: Batch Loss = 0.0834\n",
            "Iteration 45, Batch 67/100: Batch Loss = 0.2672\n",
            "Iteration 45, Batch 68/100: Batch Loss = 0.1932\n",
            "Iteration 45, Batch 69/100: Batch Loss = 0.1509\n",
            "Iteration 45, Batch 70/100: Batch Loss = 0.1718\n",
            "Iteration 45, Batch 71/100: Batch Loss = 0.1405\n",
            "Iteration 45, Batch 72/100: Batch Loss = 0.0715\n",
            "Iteration 45, Batch 73/100: Batch Loss = 0.1494\n",
            "Iteration 45, Batch 74/100: Batch Loss = 0.0343\n",
            "Iteration 45, Batch 75/100: Batch Loss = 0.2170\n",
            "Iteration 45, Batch 76/100: Batch Loss = 0.1236\n",
            "Iteration 45, Batch 77/100: Batch Loss = 0.1294\n",
            "Iteration 45, Batch 78/100: Batch Loss = 0.0550\n",
            "Iteration 45, Batch 79/100: Batch Loss = 0.0533\n",
            "Iteration 45, Batch 80/100: Batch Loss = 0.1579\n",
            "Iteration 45, Batch 81/100: Batch Loss = 0.2159\n",
            "Iteration 45, Batch 82/100: Batch Loss = 0.0634\n",
            "Iteration 45, Batch 83/100: Batch Loss = 0.0512\n",
            "Iteration 45, Batch 84/100: Batch Loss = 0.0921\n",
            "Iteration 45, Batch 85/100: Batch Loss = 0.0880\n",
            "Iteration 45, Batch 86/100: Batch Loss = 0.0730\n",
            "Iteration 45, Batch 87/100: Batch Loss = 0.0567\n",
            "Iteration 45, Batch 88/100: Batch Loss = 0.0706\n",
            "Iteration 45, Batch 89/100: Batch Loss = 0.0778\n",
            "Iteration 45, Batch 90/100: Batch Loss = 0.0449\n",
            "Iteration 45, Batch 91/100: Batch Loss = 0.0628\n",
            "Iteration 45, Batch 92/100: Batch Loss = 0.1429\n",
            "Iteration 45, Batch 93/100: Batch Loss = 0.0240\n",
            "Iteration 45, Batch 94/100: Batch Loss = 0.0379\n",
            "Iteration 45, Batch 95/100: Batch Loss = 0.0902\n",
            "Iteration 45, Batch 96/100: Batch Loss = 0.2130\n",
            "Iteration 45, Batch 97/100: Batch Loss = 0.3146\n",
            "Iteration 45, Batch 98/100: Batch Loss = 0.1807\n",
            "Iteration 45, Batch 99/100: Batch Loss = 0.1829\n",
            "Iteration 45, Batch 100/100: Batch Loss = 0.1105\n",
            "Iteration 45: Train Loss = 0.1283, Val Loss = 0.1526\n",
            "Iteration 46, Batch 1/100: Batch Loss = 0.0639\n",
            "Iteration 46, Batch 2/100: Batch Loss = 0.0542\n",
            "Iteration 46, Batch 3/100: Batch Loss = 0.0717\n",
            "Iteration 46, Batch 4/100: Batch Loss = 0.2116\n",
            "Iteration 46, Batch 5/100: Batch Loss = 0.1377\n",
            "Iteration 46, Batch 6/100: Batch Loss = 0.0587\n",
            "Iteration 46, Batch 7/100: Batch Loss = 0.1297\n",
            "Iteration 46, Batch 8/100: Batch Loss = 0.1162\n",
            "Iteration 46, Batch 9/100: Batch Loss = 0.0239\n",
            "Iteration 46, Batch 10/100: Batch Loss = 0.2075\n",
            "Iteration 46, Batch 11/100: Batch Loss = 0.1854\n",
            "Iteration 46, Batch 12/100: Batch Loss = 0.1332\n",
            "Iteration 46, Batch 13/100: Batch Loss = 0.0416\n",
            "Iteration 46, Batch 14/100: Batch Loss = 0.1724\n",
            "Iteration 46, Batch 15/100: Batch Loss = 0.0423\n",
            "Iteration 46, Batch 16/100: Batch Loss = 0.0295\n",
            "Iteration 46, Batch 17/100: Batch Loss = 0.2005\n",
            "Iteration 46, Batch 18/100: Batch Loss = 0.0570\n",
            "Iteration 46, Batch 19/100: Batch Loss = 0.1237\n",
            "Iteration 46, Batch 20/100: Batch Loss = 0.2834\n",
            "Iteration 46, Batch 21/100: Batch Loss = 0.0825\n",
            "Iteration 46, Batch 22/100: Batch Loss = 0.0701\n",
            "Iteration 46, Batch 23/100: Batch Loss = 0.1801\n",
            "Iteration 46, Batch 24/100: Batch Loss = 0.1472\n",
            "Iteration 46, Batch 25/100: Batch Loss = 0.0411\n",
            "Iteration 46, Batch 26/100: Batch Loss = 0.0674\n",
            "Iteration 46, Batch 27/100: Batch Loss = 0.0369\n",
            "Iteration 46, Batch 28/100: Batch Loss = 0.1109\n",
            "Iteration 46, Batch 29/100: Batch Loss = 0.0773\n",
            "Iteration 46, Batch 30/100: Batch Loss = 0.3530\n",
            "Iteration 46, Batch 31/100: Batch Loss = 0.0225\n",
            "Iteration 46, Batch 32/100: Batch Loss = 0.0328\n",
            "Iteration 46, Batch 33/100: Batch Loss = 0.0317\n",
            "Iteration 46, Batch 34/100: Batch Loss = 0.0953\n",
            "Iteration 46, Batch 35/100: Batch Loss = 0.0786\n",
            "Iteration 46, Batch 36/100: Batch Loss = 0.1229\n",
            "Iteration 46, Batch 37/100: Batch Loss = 0.0530\n",
            "Iteration 46, Batch 38/100: Batch Loss = 0.4243\n",
            "Iteration 46, Batch 39/100: Batch Loss = 0.0230\n",
            "Iteration 46, Batch 40/100: Batch Loss = 0.1242\n",
            "Iteration 46, Batch 41/100: Batch Loss = 0.1148\n",
            "Iteration 46, Batch 42/100: Batch Loss = 0.1035\n",
            "Iteration 46, Batch 43/100: Batch Loss = 0.1403\n",
            "Iteration 46, Batch 44/100: Batch Loss = 0.1290\n",
            "Iteration 46, Batch 45/100: Batch Loss = 0.1525\n",
            "Iteration 46, Batch 46/100: Batch Loss = 0.1257\n",
            "Iteration 46, Batch 47/100: Batch Loss = 0.2889\n",
            "Iteration 46, Batch 48/100: Batch Loss = 0.2474\n",
            "Iteration 46, Batch 49/100: Batch Loss = 0.0385\n",
            "Iteration 46, Batch 50/100: Batch Loss = 0.1040\n",
            "Iteration 46, Batch 51/100: Batch Loss = 0.1926\n",
            "Iteration 46, Batch 52/100: Batch Loss = 0.1334\n",
            "Iteration 46, Batch 53/100: Batch Loss = 0.1568\n",
            "Iteration 46, Batch 54/100: Batch Loss = 0.0687\n",
            "Iteration 46, Batch 55/100: Batch Loss = 0.1246\n",
            "Iteration 46, Batch 56/100: Batch Loss = 0.0672\n",
            "Iteration 46, Batch 57/100: Batch Loss = 0.1956\n",
            "Iteration 46, Batch 58/100: Batch Loss = 0.1057\n",
            "Iteration 46, Batch 59/100: Batch Loss = 0.2114\n",
            "Iteration 46, Batch 60/100: Batch Loss = 0.1320\n",
            "Iteration 46, Batch 61/100: Batch Loss = 0.0327\n",
            "Iteration 46, Batch 62/100: Batch Loss = 0.1455\n",
            "Iteration 46, Batch 63/100: Batch Loss = 0.1969\n",
            "Iteration 46, Batch 64/100: Batch Loss = 0.2304\n",
            "Iteration 46, Batch 65/100: Batch Loss = 0.2273\n",
            "Iteration 46, Batch 66/100: Batch Loss = 0.1005\n",
            "Iteration 46, Batch 67/100: Batch Loss = 0.1326\n",
            "Iteration 46, Batch 68/100: Batch Loss = 0.2151\n",
            "Iteration 46, Batch 69/100: Batch Loss = 0.1768\n",
            "Iteration 46, Batch 70/100: Batch Loss = 0.1292\n",
            "Iteration 46, Batch 71/100: Batch Loss = 0.0435\n",
            "Iteration 46, Batch 72/100: Batch Loss = 0.0383\n",
            "Iteration 46, Batch 73/100: Batch Loss = 0.1500\n",
            "Iteration 46, Batch 74/100: Batch Loss = 0.1589\n",
            "Iteration 46, Batch 75/100: Batch Loss = 0.0899\n",
            "Iteration 46, Batch 76/100: Batch Loss = 0.0567\n",
            "Iteration 46, Batch 77/100: Batch Loss = 0.0740\n",
            "Iteration 46, Batch 78/100: Batch Loss = 0.1135\n",
            "Iteration 46, Batch 79/100: Batch Loss = 0.2390\n",
            "Iteration 46, Batch 80/100: Batch Loss = 0.1228\n",
            "Iteration 46, Batch 81/100: Batch Loss = 0.1730\n",
            "Iteration 46, Batch 82/100: Batch Loss = 0.1060\n",
            "Iteration 46, Batch 83/100: Batch Loss = 0.0897\n",
            "Iteration 46, Batch 84/100: Batch Loss = 0.0835\n",
            "Iteration 46, Batch 85/100: Batch Loss = 0.2213\n",
            "Iteration 46, Batch 86/100: Batch Loss = 0.0365\n",
            "Iteration 46, Batch 87/100: Batch Loss = 0.0446\n",
            "Iteration 46, Batch 88/100: Batch Loss = 0.0975\n",
            "Iteration 46, Batch 89/100: Batch Loss = 0.0361\n",
            "Iteration 46, Batch 90/100: Batch Loss = 0.0313\n",
            "Iteration 46, Batch 91/100: Batch Loss = 0.0633\n",
            "Iteration 46, Batch 92/100: Batch Loss = 0.1115\n",
            "Iteration 46, Batch 93/100: Batch Loss = 0.2357\n",
            "Iteration 46, Batch 94/100: Batch Loss = 0.0626\n",
            "Iteration 46, Batch 95/100: Batch Loss = 0.0585\n",
            "Iteration 46, Batch 96/100: Batch Loss = 0.1452\n",
            "Iteration 46, Batch 97/100: Batch Loss = 0.0248\n",
            "Iteration 46, Batch 98/100: Batch Loss = 0.0496\n",
            "Iteration 46, Batch 99/100: Batch Loss = 0.4584\n",
            "Iteration 46, Batch 100/100: Batch Loss = 0.0881\n",
            "Iteration 46: Train Loss = 0.1224, Val Loss = 0.1600\n",
            "Iteration 47, Batch 1/100: Batch Loss = 0.1363\n",
            "Iteration 47, Batch 2/100: Batch Loss = 0.1295\n",
            "Iteration 47, Batch 3/100: Batch Loss = 0.0541\n",
            "Iteration 47, Batch 4/100: Batch Loss = 0.1091\n",
            "Iteration 47, Batch 5/100: Batch Loss = 0.0520\n",
            "Iteration 47, Batch 6/100: Batch Loss = 0.1688\n",
            "Iteration 47, Batch 7/100: Batch Loss = 0.2120\n",
            "Iteration 47, Batch 8/100: Batch Loss = 0.0277\n",
            "Iteration 47, Batch 9/100: Batch Loss = 0.1296\n",
            "Iteration 47, Batch 10/100: Batch Loss = 0.5523\n",
            "Iteration 47, Batch 11/100: Batch Loss = 0.1147\n",
            "Iteration 47, Batch 12/100: Batch Loss = 0.2305\n",
            "Iteration 47, Batch 13/100: Batch Loss = 0.1159\n",
            "Iteration 47, Batch 14/100: Batch Loss = 0.1588\n",
            "Iteration 47, Batch 15/100: Batch Loss = 0.2943\n",
            "Iteration 47, Batch 16/100: Batch Loss = 0.0563\n",
            "Iteration 47, Batch 17/100: Batch Loss = 0.0688\n",
            "Iteration 47, Batch 18/100: Batch Loss = 0.2983\n",
            "Iteration 47, Batch 19/100: Batch Loss = 0.0857\n",
            "Iteration 47, Batch 20/100: Batch Loss = 0.0433\n",
            "Iteration 47, Batch 21/100: Batch Loss = 0.3389\n",
            "Iteration 47, Batch 22/100: Batch Loss = 0.0329\n",
            "Iteration 47, Batch 23/100: Batch Loss = 0.0825\n",
            "Iteration 47, Batch 24/100: Batch Loss = 0.1555\n",
            "Iteration 47, Batch 25/100: Batch Loss = 0.1033\n",
            "Iteration 47, Batch 26/100: Batch Loss = 0.0553\n",
            "Iteration 47, Batch 27/100: Batch Loss = 0.0859\n",
            "Iteration 47, Batch 28/100: Batch Loss = 0.0684\n",
            "Iteration 47, Batch 29/100: Batch Loss = 0.0904\n",
            "Iteration 47, Batch 30/100: Batch Loss = 0.1101\n",
            "Iteration 47, Batch 31/100: Batch Loss = 0.0945\n",
            "Iteration 47, Batch 32/100: Batch Loss = 0.1607\n",
            "Iteration 47, Batch 33/100: Batch Loss = 0.0774\n",
            "Iteration 47, Batch 34/100: Batch Loss = 0.0662\n",
            "Iteration 47, Batch 35/100: Batch Loss = 0.1490\n",
            "Iteration 47, Batch 36/100: Batch Loss = 0.1021\n",
            "Iteration 47, Batch 37/100: Batch Loss = 0.2221\n",
            "Iteration 47, Batch 38/100: Batch Loss = 0.0489\n",
            "Iteration 47, Batch 39/100: Batch Loss = 0.0621\n",
            "Iteration 47, Batch 40/100: Batch Loss = 0.1217\n",
            "Iteration 47, Batch 41/100: Batch Loss = 0.0714\n",
            "Iteration 47, Batch 42/100: Batch Loss = 0.0729\n",
            "Iteration 47, Batch 43/100: Batch Loss = 0.0535\n",
            "Iteration 47, Batch 44/100: Batch Loss = 0.0308\n",
            "Iteration 47, Batch 45/100: Batch Loss = 0.0620\n",
            "Iteration 47, Batch 46/100: Batch Loss = 0.1522\n",
            "Iteration 47, Batch 47/100: Batch Loss = 0.0160\n",
            "Iteration 47, Batch 48/100: Batch Loss = 0.0671\n",
            "Iteration 47, Batch 49/100: Batch Loss = 0.1805\n",
            "Iteration 47, Batch 50/100: Batch Loss = 0.2574\n",
            "Iteration 47, Batch 51/100: Batch Loss = 0.0206\n",
            "Iteration 47, Batch 52/100: Batch Loss = 0.1392\n",
            "Iteration 47, Batch 53/100: Batch Loss = 0.3708\n",
            "Iteration 47, Batch 54/100: Batch Loss = 0.1529\n",
            "Iteration 47, Batch 55/100: Batch Loss = 0.0696\n",
            "Iteration 47, Batch 56/100: Batch Loss = 0.1709\n",
            "Iteration 47, Batch 57/100: Batch Loss = 0.0850\n",
            "Iteration 47, Batch 58/100: Batch Loss = 0.1611\n",
            "Iteration 47, Batch 59/100: Batch Loss = 0.1948\n",
            "Iteration 47, Batch 60/100: Batch Loss = 0.2903\n",
            "Iteration 47, Batch 61/100: Batch Loss = 0.0922\n",
            "Iteration 47, Batch 62/100: Batch Loss = 0.1440\n",
            "Iteration 47, Batch 63/100: Batch Loss = 0.1391\n",
            "Iteration 47, Batch 64/100: Batch Loss = 0.1001\n",
            "Iteration 47, Batch 65/100: Batch Loss = 0.1842\n",
            "Iteration 47, Batch 66/100: Batch Loss = 0.0952\n",
            "Iteration 47, Batch 67/100: Batch Loss = 0.0827\n",
            "Iteration 47, Batch 68/100: Batch Loss = 0.1178\n",
            "Iteration 47, Batch 69/100: Batch Loss = 0.0766\n",
            "Iteration 47, Batch 70/100: Batch Loss = 0.0664\n",
            "Iteration 47, Batch 71/100: Batch Loss = 0.1087\n",
            "Iteration 47, Batch 72/100: Batch Loss = 0.0272\n",
            "Iteration 47, Batch 73/100: Batch Loss = 0.1109\n",
            "Iteration 47, Batch 74/100: Batch Loss = 0.0661\n",
            "Iteration 47, Batch 75/100: Batch Loss = 0.1263\n",
            "Iteration 47, Batch 76/100: Batch Loss = 0.0351\n",
            "Iteration 47, Batch 77/100: Batch Loss = 0.0694\n",
            "Iteration 47, Batch 78/100: Batch Loss = 0.1165\n",
            "Iteration 47, Batch 79/100: Batch Loss = 0.0783\n",
            "Iteration 47, Batch 80/100: Batch Loss = 0.0708\n",
            "Iteration 47, Batch 81/100: Batch Loss = 0.2051\n",
            "Iteration 47, Batch 82/100: Batch Loss = 0.3352\n",
            "Iteration 47, Batch 83/100: Batch Loss = 0.1757\n",
            "Iteration 47, Batch 84/100: Batch Loss = 0.0594\n",
            "Iteration 47, Batch 85/100: Batch Loss = 0.1321\n",
            "Iteration 47, Batch 86/100: Batch Loss = 0.1167\n",
            "Iteration 47, Batch 87/100: Batch Loss = 0.1422\n",
            "Iteration 47, Batch 88/100: Batch Loss = 0.0615\n",
            "Iteration 47, Batch 89/100: Batch Loss = 0.1421\n",
            "Iteration 47, Batch 90/100: Batch Loss = 0.1615\n",
            "Iteration 47, Batch 91/100: Batch Loss = 0.1460\n",
            "Iteration 47, Batch 92/100: Batch Loss = 0.1028\n",
            "Iteration 47, Batch 93/100: Batch Loss = 0.1086\n",
            "Iteration 47, Batch 94/100: Batch Loss = 0.0613\n",
            "Iteration 47, Batch 95/100: Batch Loss = 0.1263\n",
            "Iteration 47, Batch 96/100: Batch Loss = 0.0999\n",
            "Iteration 47, Batch 97/100: Batch Loss = 0.2137\n",
            "Iteration 47, Batch 98/100: Batch Loss = 0.3090\n",
            "Iteration 47, Batch 99/100: Batch Loss = 0.0742\n",
            "Iteration 47, Batch 100/100: Batch Loss = 0.0629\n",
            "Iteration 47: Train Loss = 0.1263, Val Loss = 0.1490\n",
            "Iteration 48, Batch 1/100: Batch Loss = 0.0903\n",
            "Iteration 48, Batch 2/100: Batch Loss = 0.1899\n",
            "Iteration 48, Batch 3/100: Batch Loss = 0.0564\n",
            "Iteration 48, Batch 4/100: Batch Loss = 0.2446\n",
            "Iteration 48, Batch 5/100: Batch Loss = 0.1560\n",
            "Iteration 48, Batch 6/100: Batch Loss = 0.0945\n",
            "Iteration 48, Batch 7/100: Batch Loss = 0.0499\n",
            "Iteration 48, Batch 8/100: Batch Loss = 0.0476\n",
            "Iteration 48, Batch 9/100: Batch Loss = 0.1295\n",
            "Iteration 48, Batch 10/100: Batch Loss = 0.0220\n",
            "Iteration 48, Batch 11/100: Batch Loss = 0.0913\n",
            "Iteration 48, Batch 12/100: Batch Loss = 0.1176\n",
            "Iteration 48, Batch 13/100: Batch Loss = 0.2112\n",
            "Iteration 48, Batch 14/100: Batch Loss = 0.0824\n",
            "Iteration 48, Batch 15/100: Batch Loss = 0.1694\n",
            "Iteration 48, Batch 16/100: Batch Loss = 0.0059\n",
            "Iteration 48, Batch 17/100: Batch Loss = 0.0409\n",
            "Iteration 48, Batch 18/100: Batch Loss = 0.0444\n",
            "Iteration 48, Batch 19/100: Batch Loss = 0.1508\n",
            "Iteration 48, Batch 20/100: Batch Loss = 0.3367\n",
            "Iteration 48, Batch 21/100: Batch Loss = 0.0670\n",
            "Iteration 48, Batch 22/100: Batch Loss = 0.0540\n",
            "Iteration 48, Batch 23/100: Batch Loss = 0.1315\n",
            "Iteration 48, Batch 24/100: Batch Loss = 0.0870\n",
            "Iteration 48, Batch 25/100: Batch Loss = 0.0692\n",
            "Iteration 48, Batch 26/100: Batch Loss = 0.1234\n",
            "Iteration 48, Batch 27/100: Batch Loss = 0.0517\n",
            "Iteration 48, Batch 28/100: Batch Loss = 0.2161\n",
            "Iteration 48, Batch 29/100: Batch Loss = 0.0353\n",
            "Iteration 48, Batch 30/100: Batch Loss = 0.4854\n",
            "Iteration 48, Batch 31/100: Batch Loss = 0.0945\n",
            "Iteration 48, Batch 32/100: Batch Loss = 0.1826\n",
            "Iteration 48, Batch 33/100: Batch Loss = 0.0988\n",
            "Iteration 48, Batch 34/100: Batch Loss = 0.0184\n",
            "Iteration 48, Batch 35/100: Batch Loss = 0.1866\n",
            "Iteration 48, Batch 36/100: Batch Loss = 0.0151\n",
            "Iteration 48, Batch 37/100: Batch Loss = 0.1557\n",
            "Iteration 48, Batch 38/100: Batch Loss = 0.1320\n",
            "Iteration 48, Batch 39/100: Batch Loss = 0.0181\n",
            "Iteration 48, Batch 40/100: Batch Loss = 0.0642\n",
            "Iteration 48, Batch 41/100: Batch Loss = 0.1420\n",
            "Iteration 48, Batch 42/100: Batch Loss = 0.1566\n",
            "Iteration 48, Batch 43/100: Batch Loss = 0.0827\n",
            "Iteration 48, Batch 44/100: Batch Loss = 0.1295\n",
            "Iteration 48, Batch 45/100: Batch Loss = 0.1147\n",
            "Iteration 48, Batch 46/100: Batch Loss = 0.0609\n",
            "Iteration 48, Batch 47/100: Batch Loss = 0.0301\n",
            "Iteration 48, Batch 48/100: Batch Loss = 0.0431\n",
            "Iteration 48, Batch 49/100: Batch Loss = 0.1101\n",
            "Iteration 48, Batch 50/100: Batch Loss = 0.0373\n",
            "Iteration 48, Batch 51/100: Batch Loss = 0.0426\n",
            "Iteration 48, Batch 52/100: Batch Loss = 0.2550\n",
            "Iteration 48, Batch 53/100: Batch Loss = 0.1225\n",
            "Iteration 48, Batch 54/100: Batch Loss = 0.0723\n",
            "Iteration 48, Batch 55/100: Batch Loss = 0.0437\n",
            "Iteration 48, Batch 56/100: Batch Loss = 0.0437\n",
            "Iteration 48, Batch 57/100: Batch Loss = 0.0395\n",
            "Iteration 48, Batch 58/100: Batch Loss = 0.1282\n",
            "Iteration 48, Batch 59/100: Batch Loss = 0.2304\n",
            "Iteration 48, Batch 60/100: Batch Loss = 0.2612\n",
            "Iteration 48, Batch 61/100: Batch Loss = 0.2939\n",
            "Iteration 48, Batch 62/100: Batch Loss = 0.3306\n",
            "Iteration 48, Batch 63/100: Batch Loss = 0.0374\n",
            "Iteration 48, Batch 64/100: Batch Loss = 0.0998\n",
            "Iteration 48, Batch 65/100: Batch Loss = 0.2880\n",
            "Iteration 48, Batch 66/100: Batch Loss = 0.0426\n",
            "Iteration 48, Batch 67/100: Batch Loss = 0.2652\n",
            "Iteration 48, Batch 68/100: Batch Loss = 0.1448\n",
            "Iteration 48, Batch 69/100: Batch Loss = 0.0784\n",
            "Iteration 48, Batch 70/100: Batch Loss = 0.1623\n",
            "Iteration 48, Batch 71/100: Batch Loss = 0.0292\n",
            "Iteration 48, Batch 72/100: Batch Loss = 0.2001\n",
            "Iteration 48, Batch 73/100: Batch Loss = 0.3034\n",
            "Iteration 48, Batch 74/100: Batch Loss = 0.0809\n",
            "Iteration 48, Batch 75/100: Batch Loss = 0.0944\n",
            "Iteration 48, Batch 76/100: Batch Loss = 0.1507\n",
            "Iteration 48, Batch 77/100: Batch Loss = 0.0426\n",
            "Iteration 48, Batch 78/100: Batch Loss = 0.0757\n",
            "Iteration 48, Batch 79/100: Batch Loss = 0.1155\n",
            "Iteration 48, Batch 80/100: Batch Loss = 0.2389\n",
            "Iteration 48, Batch 81/100: Batch Loss = 0.1260\n",
            "Iteration 48, Batch 82/100: Batch Loss = 0.2109\n",
            "Iteration 48, Batch 83/100: Batch Loss = 0.2195\n",
            "Iteration 48, Batch 84/100: Batch Loss = 0.1061\n",
            "Iteration 48, Batch 85/100: Batch Loss = 0.1472\n",
            "Iteration 48, Batch 86/100: Batch Loss = 0.2869\n",
            "Iteration 48, Batch 87/100: Batch Loss = 0.4130\n",
            "Iteration 48, Batch 88/100: Batch Loss = 0.0540\n",
            "Iteration 48, Batch 89/100: Batch Loss = 0.1163\n",
            "Iteration 48, Batch 90/100: Batch Loss = 0.0849\n",
            "Iteration 48, Batch 91/100: Batch Loss = 0.0466\n",
            "Iteration 48, Batch 92/100: Batch Loss = 0.0841\n",
            "Iteration 48, Batch 93/100: Batch Loss = 0.0365\n",
            "Iteration 48, Batch 94/100: Batch Loss = 0.0680\n",
            "Iteration 48, Batch 95/100: Batch Loss = 0.0483\n",
            "Iteration 48, Batch 96/100: Batch Loss = 0.1604\n",
            "Iteration 48, Batch 97/100: Batch Loss = 0.2045\n",
            "Iteration 48, Batch 98/100: Batch Loss = 0.1151\n",
            "Iteration 48, Batch 99/100: Batch Loss = 0.1684\n",
            "Iteration 48, Batch 100/100: Batch Loss = 0.0930\n",
            "Iteration 48: Train Loss = 0.1263, Val Loss = 0.1609\n",
            "Iteration 49, Batch 1/100: Batch Loss = 0.1877\n",
            "Iteration 49, Batch 2/100: Batch Loss = 0.1300\n",
            "Iteration 49, Batch 3/100: Batch Loss = 0.1194\n",
            "Iteration 49, Batch 4/100: Batch Loss = 0.1525\n",
            "Iteration 49, Batch 5/100: Batch Loss = 0.0991\n",
            "Iteration 49, Batch 6/100: Batch Loss = 0.1024\n",
            "Iteration 49, Batch 7/100: Batch Loss = 0.0332\n",
            "Iteration 49, Batch 8/100: Batch Loss = 0.1266\n",
            "Iteration 49, Batch 9/100: Batch Loss = 0.1119\n",
            "Iteration 49, Batch 10/100: Batch Loss = 0.0507\n",
            "Iteration 49, Batch 11/100: Batch Loss = 0.1109\n",
            "Iteration 49, Batch 12/100: Batch Loss = 0.0385\n",
            "Iteration 49, Batch 13/100: Batch Loss = 0.0564\n",
            "Iteration 49, Batch 14/100: Batch Loss = 0.0930\n",
            "Iteration 49, Batch 15/100: Batch Loss = 0.0738\n",
            "Iteration 49, Batch 16/100: Batch Loss = 0.0635\n",
            "Iteration 49, Batch 17/100: Batch Loss = 0.0516\n",
            "Iteration 49, Batch 18/100: Batch Loss = 0.1748\n",
            "Iteration 49, Batch 19/100: Batch Loss = 0.1638\n",
            "Iteration 49, Batch 20/100: Batch Loss = 0.1471\n",
            "Iteration 49, Batch 21/100: Batch Loss = 0.3334\n",
            "Iteration 49, Batch 22/100: Batch Loss = 0.1410\n",
            "Iteration 49, Batch 23/100: Batch Loss = 0.0507\n",
            "Iteration 49, Batch 24/100: Batch Loss = 0.2108\n",
            "Iteration 49, Batch 25/100: Batch Loss = 0.0196\n",
            "Iteration 49, Batch 26/100: Batch Loss = 0.1790\n",
            "Iteration 49, Batch 27/100: Batch Loss = 0.1366\n",
            "Iteration 49, Batch 28/100: Batch Loss = 0.0590\n",
            "Iteration 49, Batch 29/100: Batch Loss = 0.0324\n",
            "Iteration 49, Batch 30/100: Batch Loss = 0.0303\n",
            "Iteration 49, Batch 31/100: Batch Loss = 0.0678\n",
            "Iteration 49, Batch 32/100: Batch Loss = 0.1151\n",
            "Iteration 49, Batch 33/100: Batch Loss = 0.1147\n",
            "Iteration 49, Batch 34/100: Batch Loss = 0.4157\n",
            "Iteration 49, Batch 35/100: Batch Loss = 0.0986\n",
            "Iteration 49, Batch 36/100: Batch Loss = 0.2980\n",
            "Iteration 49, Batch 37/100: Batch Loss = 0.0284\n",
            "Iteration 49, Batch 38/100: Batch Loss = 0.2939\n",
            "Iteration 49, Batch 39/100: Batch Loss = 0.1408\n",
            "Iteration 49, Batch 40/100: Batch Loss = 0.0439\n",
            "Iteration 49, Batch 41/100: Batch Loss = 0.0674\n",
            "Iteration 49, Batch 42/100: Batch Loss = 0.0627\n",
            "Iteration 49, Batch 43/100: Batch Loss = 0.0973\n",
            "Iteration 49, Batch 44/100: Batch Loss = 0.1460\n",
            "Iteration 49, Batch 45/100: Batch Loss = 0.0969\n",
            "Iteration 49, Batch 46/100: Batch Loss = 0.1916\n",
            "Iteration 49, Batch 47/100: Batch Loss = 0.1267\n",
            "Iteration 49, Batch 48/100: Batch Loss = 0.0816\n",
            "Iteration 49, Batch 49/100: Batch Loss = 0.0410\n",
            "Iteration 49, Batch 50/100: Batch Loss = 0.1323\n",
            "Iteration 49, Batch 51/100: Batch Loss = 0.1321\n",
            "Iteration 49, Batch 52/100: Batch Loss = 0.1036\n",
            "Iteration 49, Batch 53/100: Batch Loss = 0.0253\n",
            "Iteration 49, Batch 54/100: Batch Loss = 0.0340\n",
            "Iteration 49, Batch 55/100: Batch Loss = 0.1955\n",
            "Iteration 49, Batch 56/100: Batch Loss = 0.0451\n",
            "Iteration 49, Batch 57/100: Batch Loss = 0.2806\n",
            "Iteration 49, Batch 58/100: Batch Loss = 0.1188\n",
            "Iteration 49, Batch 59/100: Batch Loss = 0.1721\n",
            "Iteration 49, Batch 60/100: Batch Loss = 0.0886\n",
            "Iteration 49, Batch 61/100: Batch Loss = 0.1557\n",
            "Iteration 49, Batch 62/100: Batch Loss = 0.1975\n",
            "Iteration 49, Batch 63/100: Batch Loss = 0.1148\n",
            "Iteration 49, Batch 64/100: Batch Loss = 0.2749\n",
            "Iteration 49, Batch 65/100: Batch Loss = 0.1437\n",
            "Iteration 49, Batch 66/100: Batch Loss = 0.2443\n",
            "Iteration 49, Batch 67/100: Batch Loss = 0.1088\n",
            "Iteration 49, Batch 68/100: Batch Loss = 0.2970\n",
            "Iteration 49, Batch 69/100: Batch Loss = 0.0396\n",
            "Iteration 49, Batch 70/100: Batch Loss = 0.0701\n",
            "Iteration 49, Batch 71/100: Batch Loss = 0.1572\n",
            "Iteration 49, Batch 72/100: Batch Loss = 0.2372\n",
            "Iteration 49, Batch 73/100: Batch Loss = 0.1169\n",
            "Iteration 49, Batch 74/100: Batch Loss = 0.0666\n",
            "Iteration 49, Batch 75/100: Batch Loss = 0.0473\n",
            "Iteration 49, Batch 76/100: Batch Loss = 0.4171\n",
            "Iteration 49, Batch 77/100: Batch Loss = 0.0915\n",
            "Iteration 49, Batch 78/100: Batch Loss = 0.1112\n",
            "Iteration 49, Batch 79/100: Batch Loss = 0.0493\n",
            "Iteration 49, Batch 80/100: Batch Loss = 0.1983\n",
            "Iteration 49, Batch 81/100: Batch Loss = 0.1052\n",
            "Iteration 49, Batch 82/100: Batch Loss = 0.1301\n",
            "Iteration 49, Batch 83/100: Batch Loss = 0.1217\n",
            "Iteration 49, Batch 84/100: Batch Loss = 0.1328\n",
            "Iteration 49, Batch 85/100: Batch Loss = 0.1354\n",
            "Iteration 49, Batch 86/100: Batch Loss = 0.0513\n",
            "Iteration 49, Batch 87/100: Batch Loss = 0.1521\n",
            "Iteration 49, Batch 88/100: Batch Loss = 0.2115\n",
            "Iteration 49, Batch 89/100: Batch Loss = 0.1596\n",
            "Iteration 49, Batch 90/100: Batch Loss = 0.1196\n",
            "Iteration 49, Batch 91/100: Batch Loss = 0.0718\n",
            "Iteration 49, Batch 92/100: Batch Loss = 0.0971\n",
            "Iteration 49, Batch 93/100: Batch Loss = 0.2508\n",
            "Iteration 49, Batch 94/100: Batch Loss = 0.2328\n",
            "Iteration 49, Batch 95/100: Batch Loss = 0.0357\n",
            "Iteration 49, Batch 96/100: Batch Loss = 0.0702\n",
            "Iteration 49, Batch 97/100: Batch Loss = 0.0505\n",
            "Iteration 49, Batch 98/100: Batch Loss = 0.0321\n",
            "Iteration 49, Batch 99/100: Batch Loss = 0.0704\n",
            "Iteration 49, Batch 100/100: Batch Loss = 0.0628\n",
            "Iteration 49: Train Loss = 0.1257, Val Loss = 0.1610\n",
            "Test Accuracy: 95.00%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 95.00%\n",
            "Sample 0:\n",
            "  Features: [ 0.23460178  0.36875884 -0.66104846 -0.64409844 -0.79400121 -0.51824907\n",
            "  0.11646977 -0.72303939 -1.1813074 ]\n",
            "  True Label: 0\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 1:\n",
            "  Features: [-1.26743862  0.46932479 -0.50321423 -0.69150571 -0.13393883 -0.74244565\n",
            " -1.02049155  1.16992062  0.31553988]\n",
            "  True Label: 1\n",
            "  Predicted Label: 1\n",
            "\n",
            "Sample 2:\n",
            "  Features: [ 1.23596204  1.10414735  0.73922446  1.219372    2.1259358   1.05112697\n",
            "  1.82191174 -1.30762998  0.90126273]\n",
            "  True Label: 3\n",
            "  Predicted Label: 3\n",
            "\n",
            "Sample 3:\n",
            "  Features: [ 0.23460178  1.17957181 -0.53963751 -0.42164893 -0.38006378  0.42337655\n",
            " -0.1769396  -0.05493586  0.28299972]\n",
            "  True Label: 3\n",
            "  Predicted Label: 3\n",
            "\n",
            "Sample 4:\n",
            "  Features: [ 0.48494184  1.79553826  0.03099394  0.23840615  0.97362349  0.90166258\n",
            "  0.42821722 -0.91790292  0.51728886]\n",
            "  True Label: 1\n",
            "  Predicted Label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#q2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/updated_pollution_dataset.csv\")\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_test, val = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "train, test = train_test_split(train_test, test_size=0.2, random_state=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train.iloc[:, :-1])\n",
        "test_scaled = scaler.transform(test.iloc[:, :-1])\n",
        "val_scaled = scaler.transform(val.iloc[:, :-1])\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder()\n",
        "train_labels = encoder.fit_transform(train.iloc[:, -1].values.reshape(-1, 1))\n",
        "val_labels = encoder.transform(val.iloc[:, -1].values.reshape(-1, 1))\n",
        "test_labels = encoder.transform(test.iloc[:, -1].values.reshape(-1, 1))\n",
        "\n",
        "# Transpose data for neural network\n",
        "train_X = train_scaled.T\n",
        "train_Y = train_labels.toarray().T\n",
        "val_X = val_scaled.T\n",
        "val_Y = val_labels.toarray().T\n",
        "test_X = test_scaled.T\n",
        "test_Y = test_labels.toarray().T\n",
        "\n",
        "# Print shapes\n",
        "print(\"Train X shape:\", train_X.shape)\n",
        "print(\"Train Y shape:\", train_Y.shape)\n",
        "print(\"Validation X shape:\", val_X.shape)\n",
        "print(\"Validation Y shape:\", val_Y.shape)\n",
        "print(\"Test X shape:\", test_X.shape)\n",
        "print(\"Test Y shape:\", test_Y.shape)\n",
        "\n",
        "# Initialize parameters for multiple hidden layers\n",
        "def initialize_parameters(nx, nh, ny):\n",
        "    tf.random.set_seed(1)\n",
        "    W = []\n",
        "    b = []\n",
        "\n",
        "    for i in range(len(nh)):\n",
        "        if i == 0:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nx), minval=-0.01, maxval=0.01)))\n",
        "        else:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nh[i - 1]), minval=-0.01, maxval=0.01)))\n",
        "        b.append(tf.Variable(tf.zeros(shape=(nh[i], 1))))\n",
        "\n",
        "    W2 = tf.Variable(tf.random.uniform(shape=(ny, nh[-1]), minval=-0.01, maxval=0.01))\n",
        "    b2 = tf.Variable(tf.zeros(shape=(ny, 1)))\n",
        "\n",
        "    parameters = {\"W\": W,\n",
        "                  \"b\": b,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters\n",
        "\n",
        "# Log-softmax for numerical stability\n",
        "def log_softmax(x):\n",
        "    x_max = tf.reduce_max(x, axis=0, keepdims=True)\n",
        "    return x - x_max - tf.math.log(tf.reduce_sum(tf.exp(x - x_max), axis=0, keepdims=True))\n",
        "\n",
        "# Forward pass with multiple hidden layers\n",
        "def forward_pass(parameters, X):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "    Z = []\n",
        "    A = [X]\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        Z.append(tf.matmul(parameters[\"W\"][i], A[-1]) + parameters[\"b\"][i])\n",
        "        A.append(tf.nn.relu(Z[-1]))\n",
        "\n",
        "    Yhat = tf.matmul(parameters[\"W2\"], A[-1]) + parameters[\"b2\"]\n",
        "    return tf.nn.softmax(Yhat, axis=0)\n",
        "\n",
        "# Compute loss using log-softmax\n",
        "def compute_loss(Y, Yhat):\n",
        "    Yhat = tf.clip_by_value(Yhat, 1e-10, 1.0) #debugged using ai\n",
        "    individual_losses = -tf.reduce_sum(Y * tf.math.log(Yhat), axis=0)\n",
        "    total_loss = tf.reduce_mean(individual_losses)\n",
        "    return total_loss\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters)\n",
        "    return gradients\n",
        "\n",
        "# Update parameters using Adam\n",
        "def update_parameters_amo(parameters, gradients, learning_rate, m, v, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        m[\"W\"][i] = beta1 * m[\"W\"][i] + (1 - beta1) * gradients[\"W\"][i]\n",
        "        m[\"b\"][i] = beta1 * m[\"b\"][i] + (1 - beta1) * gradients[\"b\"][i]\n",
        "        v[\"W\"][i] = beta2 * v[\"W\"][i] + (1 - beta2) * (gradients[\"W\"][i] ** 2)\n",
        "        v[\"b\"][i] = beta2 * v[\"b\"][i] + (1 - beta2) * (gradients[\"b\"][i] ** 2)\n",
        "\n",
        "    m[\"W2\"] = beta1 * m[\"W2\"] + (1 - beta1) * gradients[\"W2\"]\n",
        "    m[\"b2\"] = beta1 * m[\"b2\"] + (1 - beta1) * gradients[\"b2\"]\n",
        "    v[\"W2\"] = beta2 * v[\"W2\"] + (1 - beta2) * (gradients[\"W2\"] ** 2)\n",
        "    v[\"b2\"] = beta2 * v[\"b2\"] + (1 - beta2) * (gradients[\"b2\"] ** 2)\n",
        "\n",
        "    # Update parameters\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        parameters[\"W\"][i].assign_sub(learning_rate * m[\"W\"][i] / (tf.sqrt(v[\"W\"][i]) + epsilon))\n",
        "        parameters[\"b\"][i].assign_sub(learning_rate * m[\"b\"][i] / (tf.sqrt(v[\"b\"][i]) + epsilon))\n",
        "\n",
        "    parameters[\"W2\"].assign_sub(learning_rate * m[\"W2\"] / (tf.sqrt(v[\"W2\"]) + epsilon))\n",
        "    parameters[\"b2\"].assign_sub(learning_rate * m[\"b2\"] / (tf.sqrt(v[\"b2\"]) + epsilon))\n",
        "\n",
        "    return parameters, m, v\n",
        "\n",
        "# Create and train the model\n",
        "def create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations, learning_rate, batch_size):\n",
        "    # Safety checks\n",
        "    assert train_X.shape[0] == val_X.shape[0], \"train_X and val_X must have the same number of features\"\n",
        "    assert train_X.shape[1] == train_Y.shape[1], \"train_X and train_Y must have the same number of examples\"\n",
        "    assert val_X.shape[1] == val_Y.shape[1], \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "    # Get the number of features and classes\n",
        "    nx = train_X.shape[0]\n",
        "    ny = train_Y.shape[0]  # Number of classes\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(nx, nh, ny)\n",
        "\n",
        "    # Initialize moments\n",
        "    m = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "    v = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iterations):\n",
        "        permuted_indices = np.random.permutation(train_X.shape[1])\n",
        "        train_X_shuffled = train_X[:, permuted_indices]\n",
        "        train_Y_shuffled = train_Y[:, permuted_indices]\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = train_X_shuffled.shape[1] // batch_size\n",
        "\n",
        "        for j in range(num_batches):\n",
        "            start_idx = j * batch_size\n",
        "            end_idx = (j + 1) * batch_size\n",
        "            batch_X = train_X_shuffled[:, start_idx:end_idx]\n",
        "            batch_Y = train_Y_shuffled[:, start_idx:end_idx]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass on batch\n",
        "                batch_Yhat = forward_pass(parameters, batch_X)\n",
        "                batch_loss = compute_loss(batch_Y, batch_Yhat)\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            gradients = backward_pass(parameters, batch_loss, tape)\n",
        "            parameters, m, v = update_parameters_amo(parameters, gradients, learning_rate, m, v)\n",
        "\n",
        "            print(f\"Iteration {i}, Batch {j + 1}/{num_batches}: Batch Loss = {batch_loss:.4f}\")\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        train_loss = epoch_loss.numpy()\n",
        "\n",
        "        # Forward pass on validation data\n",
        "        val_Yhat = forward_pass(parameters, val_X)\n",
        "        val_loss = compute_loss(val_Y, val_Yhat)\n",
        "\n",
        "        # Print losses\n",
        "        print(f\"Iteration {i}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "        # Append losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    # Return parameters and history\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return parameters, history\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(parameters, X, Y):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predictions = tf.argmax(Yhat, axis=0)\n",
        "    actual = tf.argmax(Y, axis=0)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, actual), tf.float32))\n",
        "    return accuracy.numpy()\n",
        "\n",
        "# Train the model\n",
        "nx = train_X.shape[0]\n",
        "nh = [10, 8, 5, 4, 2]\n",
        "ny = train_Y.shape[0]\n",
        "\n",
        "parameters, history = create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations=50, learning_rate=0.01, batch_size=32)\n",
        "\n",
        "# Test accuracy\n",
        "test_accuracy = evaluate_model(parameters, test_X, test_Y)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot losses\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predict and print sample results\n",
        "def predict(parameters, X):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predicted_labels = tf.argmax(Yhat, axis=0)\n",
        "    return predicted_labels\n",
        "\n",
        "predicted_labels = predict(parameters, test_X)\n",
        "true_labels = tf.argmax(test_Y, axis=0)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true_labels), tf.float32))\n",
        "print(f\"Test Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "\n",
        "num_examples = 5\n",
        "for i in range(num_examples):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"  Features: {val_X[:, i]}\")\n",
        "    print(f\"  True Label: {true_labels[i].numpy()}\")\n",
        "    print(f\"  Predicted Label: {predicted_labels[i].numpy()}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZGlrIKTgI8P"
      },
      "source": [
        "#3\n",
        "3- Add Weight decay with Adam (5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qZ4LVV3Got3r",
        "outputId": "0aad84dd-cbc1-44d8-e6f4-f5835b8ab0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train X shape: (9, 3200)\n",
            "Train Y shape: (4, 3200)\n",
            "Validation X shape: (9, 1000)\n",
            "Validation Y shape: (4, 1000)\n",
            "Test X shape: (9, 800)\n",
            "Test Y shape: (4, 800)\n",
            "Iteration 0: Train Loss = 0.7903, Val Loss = 0.6157\n",
            "Iteration 1: Train Loss = 0.5536, Val Loss = 0.4381\n",
            "Iteration 2: Train Loss = 0.3761, Val Loss = 0.3280\n",
            "Iteration 3: Train Loss = 0.2912, Val Loss = 0.2848\n",
            "Iteration 4: Train Loss = 0.2523, Val Loss = 0.2550\n",
            "Iteration 5: Train Loss = 0.2276, Val Loss = 0.2353\n",
            "Iteration 6: Train Loss = 0.2083, Val Loss = 0.2056\n",
            "Iteration 7: Train Loss = 0.1858, Val Loss = 0.2251\n",
            "Iteration 8: Train Loss = 0.1850, Val Loss = 0.1984\n",
            "Iteration 9: Train Loss = 0.1725, Val Loss = 0.2003\n",
            "Iteration 10: Train Loss = 0.1695, Val Loss = 0.2053\n",
            "Iteration 11: Train Loss = 0.1634, Val Loss = 0.2175\n",
            "Iteration 12: Train Loss = 0.1640, Val Loss = 0.1804\n",
            "Iteration 13: Train Loss = 0.1576, Val Loss = 0.1825\n",
            "Iteration 14: Train Loss = 0.1561, Val Loss = 0.1742\n",
            "Iteration 15: Train Loss = 0.1612, Val Loss = 0.1708\n",
            "Iteration 16: Train Loss = 0.1498, Val Loss = 0.1721\n",
            "Iteration 17: Train Loss = 0.1446, Val Loss = 0.1771\n",
            "Iteration 18: Train Loss = 0.1452, Val Loss = 0.1775\n",
            "Iteration 19: Train Loss = 0.1461, Val Loss = 0.1883\n",
            "Iteration 20: Train Loss = 0.1446, Val Loss = 0.1871\n",
            "Iteration 21: Train Loss = 0.1437, Val Loss = 0.1742\n",
            "Iteration 22: Train Loss = 0.1429, Val Loss = 0.1728\n",
            "Iteration 23: Train Loss = 0.1391, Val Loss = 0.1615\n",
            "Iteration 24: Train Loss = 0.1423, Val Loss = 0.1768\n",
            "Iteration 25: Train Loss = 0.1515, Val Loss = 0.1789\n",
            "Iteration 26: Train Loss = 0.1389, Val Loss = 0.1685\n",
            "Iteration 27: Train Loss = 0.1349, Val Loss = 0.1723\n",
            "Iteration 28: Train Loss = 0.1335, Val Loss = 0.1769\n",
            "Iteration 29: Train Loss = 0.1332, Val Loss = 0.1884\n",
            "Iteration 30: Train Loss = 0.1362, Val Loss = 0.1700\n",
            "Iteration 31: Train Loss = 0.1358, Val Loss = 0.1693\n",
            "Iteration 32: Train Loss = 0.1322, Val Loss = 0.1799\n",
            "Iteration 33: Train Loss = 0.1334, Val Loss = 0.1632\n",
            "Iteration 34: Train Loss = 0.1398, Val Loss = 0.1767\n",
            "Iteration 35: Train Loss = 0.1349, Val Loss = 0.1673\n",
            "Iteration 36: Train Loss = 0.1293, Val Loss = 0.1684\n",
            "Iteration 37: Train Loss = 0.1335, Val Loss = 0.1828\n",
            "Iteration 38: Train Loss = 0.1300, Val Loss = 0.2026\n",
            "Iteration 39: Train Loss = 0.1290, Val Loss = 0.2092\n",
            "Iteration 40: Train Loss = 0.1378, Val Loss = 0.1771\n",
            "Iteration 41: Train Loss = 0.1301, Val Loss = 0.1749\n",
            "Iteration 42: Train Loss = 0.1357, Val Loss = 0.1704\n",
            "Iteration 43: Train Loss = 0.1296, Val Loss = 0.1926\n",
            "Iteration 44: Train Loss = 0.1324, Val Loss = 0.1858\n",
            "Iteration 45: Train Loss = 0.1253, Val Loss = 0.1685\n",
            "Iteration 46: Train Loss = 0.1277, Val Loss = 0.1738\n",
            "Iteration 47: Train Loss = 0.1354, Val Loss = 0.1788\n",
            "Iteration 48: Train Loss = 0.1271, Val Loss = 0.1710\n",
            "Iteration 49: Train Loss = 0.1318, Val Loss = 0.1719\n",
            "Test Accuracy: 93.75%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 93.75%\n",
            "Sample 0:\n",
            "  Features: [ 0.23460178  0.36875884 -0.66104846 -0.64409844 -0.79400121 -0.51824907\n",
            "  0.11646977 -0.72303939 -1.1813074 ]\n",
            "  True Label: 0\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 1:\n",
            "  Features: [-1.26743862  0.46932479 -0.50321423 -0.69150571 -0.13393883 -0.74244565\n",
            " -1.02049155  1.16992062  0.31553988]\n",
            "  True Label: 1\n",
            "  Predicted Label: 1\n",
            "\n",
            "Sample 2:\n",
            "  Features: [ 1.23596204  1.10414735  0.73922446  1.219372    2.1259358   1.05112697\n",
            "  1.82191174 -1.30762998  0.90126273]\n",
            "  True Label: 3\n",
            "  Predicted Label: 3\n",
            "\n",
            "Sample 3:\n",
            "  Features: [ 0.23460178  1.17957181 -0.53963751 -0.42164893 -0.38006378  0.42337655\n",
            " -0.1769396  -0.05493586  0.28299972]\n",
            "  True Label: 3\n",
            "  Predicted Label: 3\n",
            "\n",
            "Sample 4:\n",
            "  Features: [ 0.48494184  1.79553826  0.03099394  0.23840615  0.97362349  0.90166258\n",
            "  0.42821722 -0.91790292  0.51728886]\n",
            "  True Label: 1\n",
            "  Predicted Label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#q3\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/updated_pollution_dataset.csv\")\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_test, val = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "train, test = train_test_split(train_test, test_size=0.2, random_state=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train.iloc[:, :-1])\n",
        "test_scaled = scaler.transform(test.iloc[:, :-1])\n",
        "val_scaled = scaler.transform(val.iloc[:, :-1])\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder()\n",
        "train_labels = encoder.fit_transform(train.iloc[:, -1].values.reshape(-1, 1))\n",
        "val_labels = encoder.transform(val.iloc[:, -1].values.reshape(-1, 1))\n",
        "test_labels = encoder.transform(test.iloc[:, -1].values.reshape(-1, 1))\n",
        "\n",
        "# Transpose data for neural network\n",
        "train_X = train_scaled.T\n",
        "train_Y = train_labels.toarray().T\n",
        "val_X = val_scaled.T\n",
        "val_Y = val_labels.toarray().T\n",
        "test_X = test_scaled.T\n",
        "test_Y = test_labels.toarray().T\n",
        "\n",
        "# Print shapes\n",
        "print(\"Train X shape:\", train_X.shape)\n",
        "print(\"Train Y shape:\", train_Y.shape)\n",
        "print(\"Validation X shape:\", val_X.shape)\n",
        "print(\"Validation Y shape:\", val_Y.shape)\n",
        "print(\"Test X shape:\", test_X.shape)\n",
        "print(\"Test Y shape:\", test_Y.shape)\n",
        "\n",
        "# Initialize parameters for multiple hidden layers\n",
        "def initialize_parameters(nx, nh, ny):\n",
        "    tf.random.set_seed(1)\n",
        "    W = []\n",
        "    b = []\n",
        "\n",
        "    for i in range(len(nh)):\n",
        "        if i == 0:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nx), minval=-0.01, maxval=0.01)))\n",
        "        else:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nh[i - 1]), minval=-0.01, maxval=0.01)))\n",
        "        b.append(tf.Variable(tf.zeros(shape=(nh[i], 1))))\n",
        "\n",
        "    W2 = tf.Variable(tf.random.uniform(shape=(ny, nh[-1]), minval=-0.01, maxval=0.01))\n",
        "    b2 = tf.Variable(tf.zeros(shape=(ny, 1)))\n",
        "\n",
        "    parameters = {\"W\": W,\n",
        "                  \"b\": b,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters\n",
        "\n",
        "# Log-softmax for numerical stability\n",
        "def log_softmax(x):\n",
        "    x_max = tf.reduce_max(x, axis=0, keepdims=True)\n",
        "    return x - x_max - tf.math.log(tf.reduce_sum(tf.exp(x - x_max), axis=0, keepdims=True))\n",
        "\n",
        "# Forward pass with multiple hidden layers\n",
        "def forward_pass(parameters, X):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "    Z = []\n",
        "    A = [X]\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        Z.append(tf.matmul(parameters[\"W\"][i], A[-1]) + parameters[\"b\"][i])\n",
        "        A.append(tf.nn.relu(Z[-1]))\n",
        "\n",
        "    Yhat = tf.matmul(parameters[\"W2\"], A[-1]) + parameters[\"b2\"]\n",
        "    return tf.nn.softmax(Yhat, axis=0)\n",
        "\n",
        "# Compute loss using log-softmax\n",
        "def compute_loss(Y, Yhat):\n",
        "    Yhat = tf.clip_by_value(Yhat, 1e-10, 1.0)  #debugged using ai\n",
        "    individual_losses = -tf.reduce_sum(Y * tf.math.log(Yhat), axis=0)\n",
        "    total_loss = tf.reduce_mean(individual_losses)\n",
        "    return total_loss\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters)\n",
        "    return gradients\n",
        "\n",
        "# Update parameters using AdamW\n",
        "def update_parameters_adamw(parameters, gradients, learning_rate, m, v, weight_decay=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        m[\"W\"][i] = beta1 * m[\"W\"][i] + (1 - beta1) * gradients[\"W\"][i]\n",
        "        m[\"b\"][i] = beta1 * m[\"b\"][i] + (1 - beta1) * gradients[\"b\"][i]\n",
        "        v[\"W\"][i] = beta2 * v[\"W\"][i] + (1 - beta2) * (gradients[\"W\"][i] ** 2)\n",
        "        v[\"b\"][i] = beta2 * v[\"b\"][i] + (1 - beta2) * (gradients[\"b\"][i] ** 2)\n",
        "\n",
        "    m[\"W2\"] = beta1 * m[\"W2\"] + (1 - beta1) * gradients[\"W2\"]\n",
        "    m[\"b2\"] = beta1 * m[\"b2\"] + (1 - beta1) * gradients[\"b2\"]\n",
        "    v[\"W2\"] = beta2 * v[\"W2\"] + (1 - beta2) * (gradients[\"W2\"] ** 2)\n",
        "    v[\"b2\"] = beta2 * v[\"b2\"] + (1 - beta2) * (gradients[\"b2\"] ** 2)\n",
        "\n",
        "    # Update parameters with weight decay\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        parameters[\"W\"][i].assign_sub(learning_rate * (m[\"W\"][i] / (tf.sqrt(v[\"W\"][i]) + epsilon) + weight_decay * parameters[\"W\"][i]))\n",
        "        parameters[\"b\"][i].assign_sub(learning_rate * (m[\"b\"][i] / (tf.sqrt(v[\"b\"][i]) + epsilon) + weight_decay * parameters[\"b\"][i]))\n",
        "\n",
        "    parameters[\"W2\"].assign_sub(learning_rate * (m[\"W2\"] / (tf.sqrt(v[\"W2\"]) + epsilon) + weight_decay * parameters[\"W2\"]))\n",
        "    parameters[\"b2\"].assign_sub(learning_rate * (m[\"b2\"] / (tf.sqrt(v[\"b2\"]) + epsilon) + weight_decay * parameters[\"b2\"]))\n",
        "\n",
        "    return parameters, m, v\n",
        "\n",
        "# Create and train the model\n",
        "def create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations, learning_rate, batch_size):\n",
        "    # Safety checks\n",
        "    assert train_X.shape[0] == val_X.shape[0], \"train_X and val_X must have the same number of features\"\n",
        "    assert train_X.shape[1] == train_Y.shape[1], \"train_X and train_Y must have the same number of examples\"\n",
        "    assert val_X.shape[1] == val_Y.shape[1], \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "    # Get the number of features and classes\n",
        "    nx = train_X.shape[0]\n",
        "    ny = train_Y.shape[0]  # Number of classes\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(nx, nh, ny)\n",
        "\n",
        "    # Initialize moments for AdamW\n",
        "    m = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "    v = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iterations):\n",
        "        permuted_indices = np.random.permutation(train_X.shape[1])\n",
        "        train_X_shuffled = train_X[:, permuted_indices]\n",
        "        train_Y_shuffled = train_Y[:, permuted_indices]\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = train_X_shuffled.shape[1] // batch_size\n",
        "\n",
        "        for j in range(num_batches):\n",
        "            start_idx = j * batch_size\n",
        "            end_idx = (j + 1) * batch_size\n",
        "            batch_X = train_X_shuffled[:, start_idx:end_idx]\n",
        "            batch_Y = train_Y_shuffled[:, start_idx:end_idx]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass on batch\n",
        "                batch_Yhat = forward_pass(parameters, batch_X)\n",
        "                batch_loss = compute_loss(batch_Y, batch_Yhat)\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            gradients = backward_pass(parameters, batch_loss, tape)\n",
        "            parameters, m, v = update_parameters_adamw(parameters, gradients, learning_rate, m, v)\n",
        "\n",
        "            #print(f\"Iteration {i}, Batch {j + 1}/{num_batches}: Batch Loss = {batch_loss:.4f}\")\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        train_loss = epoch_loss.numpy()\n",
        "\n",
        "        # Forward pass on validation data\n",
        "        val_Yhat = forward_pass(parameters, val_X)\n",
        "        val_loss = compute_loss(val_Y, val_Yhat)\n",
        "\n",
        "        # Print losses\n",
        "        print(f\"Iteration {i}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "        # Append losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    # Return parameters and history\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return parameters, history\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(parameters, X, Y):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predictions = tf.argmax(Yhat, axis=0)\n",
        "    actual = tf.argmax(Y, axis=0)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, actual), tf.float32))\n",
        "    return accuracy.numpy()\n",
        "\n",
        "# Train the model\n",
        "nx = train_X.shape[0]\n",
        "nh = [10, 8, 5, 4, 2]\n",
        "ny = train_Y.shape[0]\n",
        "\n",
        "parameters, history = create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations=50, learning_rate=0.01, batch_size=32)\n",
        "\n",
        "# Test accuracy\n",
        "test_accuracy = evaluate_model(parameters, test_X, test_Y)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot losses\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predict and print sample results\n",
        "def predict(parameters, X):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predicted_labels = tf.argmax(Yhat, axis=0)\n",
        "    return predicted_labels\n",
        "\n",
        "predicted_labels = predict(parameters, test_X)\n",
        "true_labels = tf.argmax(test_Y, axis=0)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true_labels), tf.float32))\n",
        "print(f\"Test Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "\n",
        "num_examples = 5\n",
        "for i in range(num_examples):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"  Features: {val_X[:, i]}\")\n",
        "    print(f\"  True Label: {true_labels[i].numpy()}\")\n",
        "    print(f\"  Predicted Label: {predicted_labels[i].numpy()}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVrib9RPuKnG"
      },
      "source": [
        "#4\n",
        "4- Reduce Learning Rate on Plateau(2pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TMzqxjBzdBcY",
        "outputId": "42993550-f591-40f3-c9c7-a974b35b4518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train X shape: (9, 3200)\n",
            "Train Y shape: (4, 3200)\n",
            "Validation X shape: (9, 1000)\n",
            "Validation Y shape: (4, 1000)\n",
            "Test X shape: (9, 800)\n",
            "Test Y shape: (4, 800)\n",
            "Iteration 0: Train Loss = 1.2912, Val Loss = 1.2841,  Learning Rate = 0.0100\n",
            "Iteration 1: Train Loss = 1.2840, Val Loss = 1.2800,  Learning Rate = 0.0100\n",
            "Iteration 2: Train Loss = 1.2829, Val Loss = 1.2809,  Learning Rate = 0.0100\n",
            "Iteration 3: Train Loss = 1.2824, Val Loss = 1.2805,  Learning Rate = 0.0100\n",
            "Iteration 4: Train Loss = 1.2829, Val Loss = 1.2791,  Learning Rate = 0.0100\n",
            "Iteration 5: Train Loss = 1.2830, Val Loss = 1.2790,  Learning Rate = 0.0100\n",
            "Iteration 6: Train Loss = 1.2826, Val Loss = 1.2789,  Learning Rate = 0.0100\n",
            "Iteration 7: Train Loss = 1.2825, Val Loss = 1.2798,  Learning Rate = 0.0100\n",
            "Iteration 8: Train Loss = 1.2827, Val Loss = 1.2792,  Learning Rate = 0.0100\n",
            "Iteration 9: Train Loss = 1.2821, Val Loss = 1.2791,  Learning Rate = 0.0100\n",
            "Iteration 10: Train Loss = 1.2819, Val Loss = 1.2789,  Learning Rate = 0.0100\n",
            "Iteration 11: Train Loss = 1.2824, Val Loss = 1.2789,  Learning Rate = 0.0100\n",
            "Iteration 12: Train Loss = 1.2824, Val Loss = 1.2788,  Learning Rate = 0.0100\n",
            "Iteration 13: Train Loss = 1.2827, Val Loss = 1.2791,  Learning Rate = 0.0100\n",
            "Iteration 14: Train Loss = 1.2826, Val Loss = 1.2789,  Learning Rate = 0.0100\n",
            "Iteration 15: Train Loss = 1.2820, Val Loss = 1.2799,  Learning Rate = 0.0100\n",
            "Iteration 16: Train Loss = 1.2822, Val Loss = 1.2794,  Learning Rate = 0.0100\n",
            "Iteration 17: Train Loss = 1.2826, Val Loss = 1.2789,  Learning Rate = 0.0100\n",
            "Iteration 18: Train Loss = 1.2823, Val Loss = 1.2791,  Learning Rate = 0.0100\n",
            "Iteration 19: Train Loss = 1.2828, Val Loss = 1.2789,  Learning Rate = 0.0100\n",
            "Iteration 20: Train Loss = 1.2820, Val Loss = 1.2789,  Learning Rate = 0.0100\n",
            "Iteration 21: Train Loss = 1.2826, Val Loss = 1.2789,  Learning Rate = 0.0100\n",
            "Iteration 22: Train Loss = 1.2820, Val Loss = 1.2791,  Learning Rate = 0.0100\n",
            "Reducing learning rate to 0.005\n",
            "Iteration 23: Train Loss = 1.2818, Val Loss = 1.2788,  Learning Rate = 0.0050\n",
            "Iteration 24: Train Loss = 1.2816, Val Loss = 1.2788,  Learning Rate = 0.0050\n",
            "Iteration 25: Train Loss = 1.2819, Val Loss = 1.2789,  Learning Rate = 0.0050\n",
            "Iteration 26: Train Loss = 1.2818, Val Loss = 1.2788,  Learning Rate = 0.0050\n",
            "Iteration 27: Train Loss = 1.2817, Val Loss = 1.2787,  Learning Rate = 0.0050\n",
            "Iteration 28: Train Loss = 1.2816, Val Loss = 1.2787,  Learning Rate = 0.0050\n",
            "Iteration 29: Train Loss = 1.2818, Val Loss = 1.2788,  Learning Rate = 0.0050\n",
            "Iteration 30: Train Loss = 1.2816, Val Loss = 1.2788,  Learning Rate = 0.0050\n",
            "Iteration 31: Train Loss = 1.2815, Val Loss = 1.2789,  Learning Rate = 0.0050\n",
            "Iteration 32: Train Loss = 1.2817, Val Loss = 1.2787,  Learning Rate = 0.0050\n",
            "Iteration 33: Train Loss = 1.2817, Val Loss = 1.2788,  Learning Rate = 0.0050\n",
            "Iteration 34: Train Loss = 1.2816, Val Loss = 1.2788,  Learning Rate = 0.0050\n",
            "Iteration 35: Train Loss = 1.2816, Val Loss = 1.2787,  Learning Rate = 0.0050\n",
            "Iteration 36: Train Loss = 1.2818, Val Loss = 1.2787,  Learning Rate = 0.0050\n",
            "Iteration 37: Train Loss = 1.2816, Val Loss = 1.2789,  Learning Rate = 0.0050\n",
            "Reducing learning rate to 0.0025\n",
            "Iteration 38: Train Loss = 1.2814, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 39: Train Loss = 1.2815, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 40: Train Loss = 1.2814, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 41: Train Loss = 1.2815, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 42: Train Loss = 1.2816, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 43: Train Loss = 1.2815, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 44: Train Loss = 1.2813, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 45: Train Loss = 1.2815, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 46: Train Loss = 1.2814, Val Loss = 1.2788,  Learning Rate = 0.0025\n",
            "Iteration 47: Train Loss = 1.2815, Val Loss = 1.2787,  Learning Rate = 0.0025\n",
            "Reducing learning rate to 0.00125\n",
            "Iteration 48: Train Loss = 1.2813, Val Loss = 1.2787,  Learning Rate = 0.0013\n",
            "Iteration 49: Train Loss = 1.2814, Val Loss = 1.2787,  Learning Rate = 0.0013\n",
            "Iteration 50: Train Loss = 1.2813, Val Loss = 1.2788,  Learning Rate = 0.0013\n",
            "Iteration 51: Train Loss = 1.2813, Val Loss = 1.2788,  Learning Rate = 0.0013\n",
            "Iteration 52: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0013\n",
            "Iteration 53: Train Loss = 1.2813, Val Loss = 1.2788,  Learning Rate = 0.0013\n",
            "Iteration 54: Train Loss = 1.2814, Val Loss = 1.2787,  Learning Rate = 0.0013\n",
            "Iteration 55: Train Loss = 1.2813, Val Loss = 1.2788,  Learning Rate = 0.0013\n",
            "Iteration 56: Train Loss = 1.2814, Val Loss = 1.2787,  Learning Rate = 0.0013\n",
            "Iteration 57: Train Loss = 1.2813, Val Loss = 1.2787,  Learning Rate = 0.0013\n",
            "Reducing learning rate to 0.000625\n",
            "Iteration 58: Train Loss = 1.2813, Val Loss = 1.2787,  Learning Rate = 0.0006\n",
            "Iteration 59: Train Loss = 1.2813, Val Loss = 1.2787,  Learning Rate = 0.0006\n",
            "Iteration 60: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0006\n",
            "Iteration 61: Train Loss = 1.2813, Val Loss = 1.2787,  Learning Rate = 0.0006\n",
            "Iteration 62: Train Loss = 1.2813, Val Loss = 1.2788,  Learning Rate = 0.0006\n",
            "Iteration 63: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0006\n",
            "Iteration 64: Train Loss = 1.2813, Val Loss = 1.2788,  Learning Rate = 0.0006\n",
            "Iteration 65: Train Loss = 1.2813, Val Loss = 1.2787,  Learning Rate = 0.0006\n",
            "Iteration 66: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0006\n",
            "Iteration 67: Train Loss = 1.2813, Val Loss = 1.2787,  Learning Rate = 0.0006\n",
            "Reducing learning rate to 0.0003125\n",
            "Iteration 68: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 69: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 70: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 71: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 72: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 73: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 74: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 75: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 76: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Iteration 77: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0003\n",
            "Reducing learning rate to 0.00015625\n",
            "Iteration 78: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 79: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 80: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 81: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 82: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 83: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 84: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 85: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 86: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Iteration 87: Train Loss = 1.2812, Val Loss = 1.2787,  Learning Rate = 0.0002\n",
            "Reducing learning rate to 7.8125e-05\n",
            "Learning rate has reached a minimum. Stopping training.\n",
            "Test Accuracy: 39.00%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 39.00%\n",
            "Sample 0:\n",
            "  Features: [ 0.23460178  0.36875884 -0.66104846 -0.64409844 -0.79400121 -0.51824907\n",
            "  0.11646977 -0.72303939 -1.1813074 ]\n",
            "  True Label: 0\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 1:\n",
            "  Features: [-1.26743862  0.46932479 -0.50321423 -0.69150571 -0.13393883 -0.74244565\n",
            " -1.02049155  1.16992062  0.31553988]\n",
            "  True Label: 1\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 2:\n",
            "  Features: [ 1.23596204  1.10414735  0.73922446  1.219372    2.1259358   1.05112697\n",
            "  1.82191174 -1.30762998  0.90126273]\n",
            "  True Label: 3\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 3:\n",
            "  Features: [ 0.23460178  1.17957181 -0.53963751 -0.42164893 -0.38006378  0.42337655\n",
            " -0.1769396  -0.05493586  0.28299972]\n",
            "  True Label: 3\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 4:\n",
            "  Features: [ 0.48494184  1.79553826  0.03099394  0.23840615  0.97362349  0.90166258\n",
            "  0.42821722 -0.91790292  0.51728886]\n",
            "  True Label: 1\n",
            "  Predicted Label: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#q4\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/updated_pollution_dataset.csv\")\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_test, val = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "train, test = train_test_split(train_test, test_size=0.2, random_state=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train.iloc[:, :-1])\n",
        "test_scaled = scaler.transform(test.iloc[:, :-1])\n",
        "val_scaled = scaler.transform(val.iloc[:, :-1])\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder()\n",
        "train_labels = encoder.fit_transform(train.iloc[:, -1].values.reshape(-1, 1))\n",
        "val_labels = encoder.transform(val.iloc[:, -1].values.reshape(-1, 1))\n",
        "test_labels = encoder.transform(test.iloc[:, -1].values.reshape(-1, 1))\n",
        "\n",
        "# Transpose data for neural network\n",
        "train_X = train_scaled.T\n",
        "train_Y = train_labels.toarray().T\n",
        "val_X = val_scaled.T\n",
        "val_Y = val_labels.toarray().T\n",
        "test_X = test_scaled.T\n",
        "test_Y = test_labels.toarray().T\n",
        "\n",
        "# Print shapes\n",
        "print(\"Train X shape:\", train_X.shape)\n",
        "print(\"Train Y shape:\", train_Y.shape)\n",
        "print(\"Validation X shape:\", val_X.shape)\n",
        "print(\"Validation Y shape:\", val_Y.shape)\n",
        "print(\"Test X shape:\", test_X.shape)\n",
        "print(\"Test Y shape:\", test_Y.shape)\n",
        "\n",
        "# Initialize parameters for multiple hidden layers\n",
        "def initialize_parameters(nx, nh, ny):\n",
        "    tf.random.set_seed(1)\n",
        "    W = []\n",
        "    b = []\n",
        "\n",
        "    for i in range(len(nh)):\n",
        "        if i == 0:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nx), minval=-0.01, maxval=0.01)))\n",
        "        else:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nh[i - 1]), minval=-0.01, maxval=0.01)))\n",
        "        b.append(tf.Variable(tf.zeros(shape=(nh[i], 1))))\n",
        "\n",
        "    W2 = tf.Variable(tf.random.uniform(shape=(ny, nh[-1]), minval=-0.01, maxval=0.01))\n",
        "    b2 = tf.Variable(tf.zeros(shape=(ny, 1)))\n",
        "\n",
        "    parameters = {\"W\": W,\n",
        "                  \"b\": b,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters\n",
        "\n",
        "# Log-softmax for numerical stability\n",
        "def log_softmax(x):\n",
        "    x_max = tf.reduce_max(x, axis=0, keepdims=True)\n",
        "    return x - x_max - tf.math.log(tf.reduce_sum(tf.exp(x - x_max), axis=0, keepdims=True))\n",
        "\n",
        "# Forward pass with multiple hidden layers\n",
        "def forward_pass(parameters, X):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "    Z = []\n",
        "    A = [X]\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        Z.append(tf.matmul(parameters[\"W\"][i], A[-1]) + parameters[\"b\"][i])\n",
        "        A.append(tf.nn.relu(Z[-1]))\n",
        "\n",
        "    Yhat = tf.matmul(parameters[\"W2\"], A[-1]) + parameters[\"b2\"]\n",
        "    return tf.nn.softmax(Yhat, axis=0)\n",
        "\n",
        "# Compute loss using log-softmax\n",
        "def compute_loss(Y, Yhat):\n",
        "    Yhat = tf.clip_by_value(Yhat, 1e-10, 1.0)  #debugged using ai\n",
        "    individual_losses = -tf.reduce_sum(Y * tf.math.log(Yhat), axis=0)\n",
        "    total_loss = tf.reduce_mean(individual_losses)\n",
        "    return total_loss\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters)\n",
        "    return gradients\n",
        "\n",
        "# Update parameters using AdamW\n",
        "def update_parameters_adamw(parameters, gradients, learning_rate, m, v, weight_decay=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        m[\"W\"][i] = beta1 * m[\"W\"][i] + (1 - beta1) * gradients[\"W\"][i]\n",
        "        m[\"b\"][i] = beta1 * m[\"b\"][i] + (1 - beta1) * gradients[\"b\"][i]\n",
        "        v[\"W\"][i] = beta2 * v[\"W\"][i] + (1 - beta2) * (gradients[\"W\"][i] ** 2)\n",
        "        v[\"b\"][i] = beta2 * v[\"b\"][i] + (1 - beta2) * (gradients[\"b\"][i] ** 2)\n",
        "\n",
        "    m[\"W2\"] = beta1 * m[\"W2\"] + (1 - beta1) * gradients[\"W2\"]\n",
        "    m[\"b2\"] = beta1 * m[\"b2\"] + (1 - beta1) * gradients[\"b2\"]\n",
        "    v[\"W2\"] = beta2 * v[\"W2\"] + (1 - beta2) * (gradients[\"W2\"] ** 2)\n",
        "    v[\"b2\"] = beta2 * v[\"b2\"] + (1 - beta2) * (gradients[\"b2\"] ** 2)\n",
        "\n",
        "    # Update parameters with weight decay\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        parameters[\"W\"][i].assign_sub(learning_rate * (m[\"W\"][i] / (tf.sqrt(v[\"W\"][i]) + epsilon) + weight_decay * parameters[\"W\"][i]))\n",
        "        parameters[\"b\"][i].assign_sub(learning_rate * (m[\"b\"][i] / (tf.sqrt(v[\"b\"][i]) + epsilon) + weight_decay * parameters[\"b\"][i]))\n",
        "\n",
        "    parameters[\"W2\"].assign_sub(learning_rate * (m[\"W2\"] / (tf.sqrt(v[\"W2\"]) + epsilon) + weight_decay * parameters[\"W2\"]))\n",
        "    parameters[\"b2\"].assign_sub(learning_rate * (m[\"b2\"] / (tf.sqrt(v[\"b2\"]) + epsilon) + weight_decay * parameters[\"b2\"]))\n",
        "\n",
        "    return parameters, m, v\n",
        "\n",
        "# Create and train the model\n",
        "def create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations, learning_rate, batch_size):\n",
        "    # Safety checks\n",
        "    assert train_X.shape[0] == val_X.shape[0], \"train_X and val_X must have the same number of features\"\n",
        "    assert train_X.shape[1] == train_Y.shape[1], \"train_X and train_Y must have the same number of examples\"\n",
        "    assert val_X.shape[1] == val_Y.shape[1], \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "    # Get the number of features and classes\n",
        "    nx = train_X.shape[0]\n",
        "    ny = train_Y.shape[0]  # Number of classes\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(nx, nh, ny)\n",
        "\n",
        "    # Initialize moments for AdamW\n",
        "    m = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "    v = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "    #checking number of epochs since last improvement for reducing learning rate\n",
        "    last_improvement = 0\n",
        "    patience = 10\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iterations):\n",
        "        permuted_indices = np.random.permutation(train_X.shape[1])\n",
        "        train_X_shuffled = train_X[:, permuted_indices]\n",
        "        train_Y_shuffled = train_Y[:, permuted_indices]\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = train_X_shuffled.shape[1] // batch_size\n",
        "\n",
        "        for j in range(num_batches):\n",
        "            start_idx = j * batch_size\n",
        "            end_idx = (j + 1) * batch_size\n",
        "            batch_X = train_X_shuffled[:, start_idx:end_idx]\n",
        "            batch_Y = train_Y_shuffled[:, start_idx:end_idx]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass on batch\n",
        "                batch_Yhat = forward_pass(parameters, batch_X)\n",
        "                batch_loss = compute_loss(batch_Y, batch_Yhat)\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            gradients = backward_pass(parameters, batch_loss, tape)\n",
        "            parameters, m, v = update_parameters_adamw(parameters, gradients, learning_rate, m, v)\n",
        "\n",
        "            #print(f\"Iteration {i}, Batch {j + 1}/{num_batches}: Batch Loss = {batch_loss:.4f}\")\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        train_loss = epoch_loss.numpy()\n",
        "\n",
        "        # Forward pass on validation data\n",
        "        val_Yhat = forward_pass(parameters, val_X)\n",
        "        val_loss = compute_loss(val_Y, val_Yhat)\n",
        "\n",
        "        # Print losses\n",
        "        print(f\"Iteration {i}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f},  Learning Rate = {learning_rate:.4f}\")\n",
        "\n",
        "        # Append losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Check for improvement\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_parameters = parameters\n",
        "            last_improvement = 0\n",
        "        else:\n",
        "            last_improvement += 1\n",
        "        if last_improvement >= patience:\n",
        "            # Reduce learning rate\n",
        "            learning_rate /= 2\n",
        "            print(f\"Reducing learning rate to {learning_rate}\")\n",
        "            last_improvement = 0\n",
        "\n",
        "            if learning_rate < 1e-4:\n",
        "                print(\"Learning rate has reached a minimum. Stopping training.\")\n",
        "                break\n",
        "\n",
        "\n",
        "    # Return parameters and history\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return best_parameters, history\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(parameters, X, Y):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predictions = tf.argmax(Yhat, axis=0)\n",
        "    actual = tf.argmax(Y, axis=0)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, actual), tf.float32))\n",
        "    return accuracy.numpy()\n",
        "\n",
        "# Train the model\n",
        "nx = train_X.shape[0]\n",
        "nh = [10, 8, 5, 4, 2]\n",
        "ny = train_Y.shape[0]\n",
        "\n",
        "parameters, history = create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations=150, learning_rate=0.01, batch_size=32)\n",
        "\n",
        "# Test accuracy\n",
        "test_accuracy = evaluate_model(parameters, test_X, test_Y)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot losses\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predict and print sample results\n",
        "def predict(parameters, X):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predicted_labels = tf.argmax(Yhat, axis=0)\n",
        "    return predicted_labels\n",
        "\n",
        "predicted_labels = predict(parameters, test_X)\n",
        "true_labels = tf.argmax(test_Y, axis=0)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true_labels), tf.float32))\n",
        "print(f\"Test Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "\n",
        "num_examples = 5\n",
        "for i in range(num_examples):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"  Features: {val_X[:, i]}\")\n",
        "    print(f\"  True Label: {true_labels[i].numpy()}\")\n",
        "    print(f\"  Predicted Label: {predicted_labels[i].numpy()}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kik4rlPfufAT"
      },
      "source": [
        "#5\n",
        "5- Add Dropout Regularization (3pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wswFCX_cuguq",
        "outputId": "ee4c1c62-98e3-4dc4-e20b-096f92793e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train X shape: (9, 3200)\n",
            "Train Y shape: (4, 3200)\n",
            "Validation X shape: (9, 1000)\n",
            "Validation Y shape: (4, 1000)\n",
            "Test X shape: (9, 800)\n",
            "Test Y shape: (4, 800)\n",
            "Iteration 0: Train Loss = 0.8768, Val Loss = 0.6558,  Learning Rate = 0.0100\n",
            "Iteration 1: Train Loss = 0.7278, Val Loss = 0.6378,  Learning Rate = 0.0100\n",
            "Iteration 2: Train Loss = 0.7313, Val Loss = 0.6479,  Learning Rate = 0.0100\n",
            "Iteration 3: Train Loss = 0.7300, Val Loss = 0.6377,  Learning Rate = 0.0100\n",
            "Iteration 4: Train Loss = 0.6999, Val Loss = 0.6446,  Learning Rate = 0.0100\n",
            "Iteration 5: Train Loss = 0.6990, Val Loss = 0.6365,  Learning Rate = 0.0100\n",
            "Iteration 6: Train Loss = 0.6978, Val Loss = 0.6285,  Learning Rate = 0.0100\n",
            "Iteration 7: Train Loss = 0.7008, Val Loss = 0.6317,  Learning Rate = 0.0100\n",
            "Iteration 8: Train Loss = 0.6875, Val Loss = 0.6244,  Learning Rate = 0.0100\n",
            "Iteration 9: Train Loss = 0.7110, Val Loss = 0.6275,  Learning Rate = 0.0100\n",
            "Iteration 10: Train Loss = 0.6880, Val Loss = 0.6184,  Learning Rate = 0.0100\n",
            "Iteration 11: Train Loss = 0.6873, Val Loss = 0.5978,  Learning Rate = 0.0100\n",
            "Iteration 12: Train Loss = 0.6470, Val Loss = 0.4790,  Learning Rate = 0.0100\n",
            "Iteration 13: Train Loss = 0.6229, Val Loss = 0.4173,  Learning Rate = 0.0100\n",
            "Iteration 14: Train Loss = 0.5685, Val Loss = 0.4066,  Learning Rate = 0.0100\n",
            "Iteration 15: Train Loss = 0.5917, Val Loss = 0.4100,  Learning Rate = 0.0100\n",
            "Iteration 16: Train Loss = 0.5670, Val Loss = 0.3895,  Learning Rate = 0.0100\n",
            "Iteration 17: Train Loss = 0.5587, Val Loss = 0.3984,  Learning Rate = 0.0100\n",
            "Iteration 18: Train Loss = 0.5132, Val Loss = 0.3630,  Learning Rate = 0.0100\n",
            "Iteration 19: Train Loss = 0.5130, Val Loss = 0.3433,  Learning Rate = 0.0100\n",
            "Iteration 20: Train Loss = 0.4801, Val Loss = 0.3415,  Learning Rate = 0.0100\n",
            "Iteration 21: Train Loss = 0.4707, Val Loss = 0.3319,  Learning Rate = 0.0100\n",
            "Iteration 22: Train Loss = 0.4808, Val Loss = 0.3431,  Learning Rate = 0.0100\n",
            "Iteration 23: Train Loss = 0.4758, Val Loss = 0.3524,  Learning Rate = 0.0100\n",
            "Iteration 24: Train Loss = 0.4635, Val Loss = 0.3324,  Learning Rate = 0.0100\n",
            "Iteration 25: Train Loss = 0.4731, Val Loss = 0.3395,  Learning Rate = 0.0100\n",
            "Iteration 26: Train Loss = 0.4625, Val Loss = 0.3198,  Learning Rate = 0.0100\n",
            "Iteration 27: Train Loss = 0.4502, Val Loss = 0.3192,  Learning Rate = 0.0100\n",
            "Iteration 28: Train Loss = 0.4574, Val Loss = 0.3360,  Learning Rate = 0.0100\n",
            "Iteration 29: Train Loss = 0.4454, Val Loss = 0.3203,  Learning Rate = 0.0100\n",
            "Iteration 30: Train Loss = 0.4412, Val Loss = 0.3274,  Learning Rate = 0.0100\n",
            "Iteration 31: Train Loss = 0.4463, Val Loss = 0.3227,  Learning Rate = 0.0100\n",
            "Iteration 32: Train Loss = 0.4821, Val Loss = 0.3146,  Learning Rate = 0.0100\n",
            "Iteration 33: Train Loss = 0.4518, Val Loss = 0.3177,  Learning Rate = 0.0100\n",
            "Iteration 34: Train Loss = 0.4587, Val Loss = 0.3516,  Learning Rate = 0.0100\n",
            "Iteration 35: Train Loss = 0.4297, Val Loss = 0.3440,  Learning Rate = 0.0100\n",
            "Iteration 36: Train Loss = 0.4850, Val Loss = 0.3382,  Learning Rate = 0.0100\n",
            "Iteration 37: Train Loss = 0.4341, Val Loss = 0.3410,  Learning Rate = 0.0100\n",
            "Iteration 38: Train Loss = 0.4482, Val Loss = 0.3421,  Learning Rate = 0.0100\n",
            "Iteration 39: Train Loss = 0.4724, Val Loss = 0.3262,  Learning Rate = 0.0100\n",
            "Iteration 40: Train Loss = 0.4549, Val Loss = 0.3602,  Learning Rate = 0.0100\n",
            "Iteration 41: Train Loss = 0.4607, Val Loss = 0.3237,  Learning Rate = 0.0100\n",
            "Iteration 42: Train Loss = 0.4137, Val Loss = 0.3322,  Learning Rate = 0.0100\n",
            "Reducing learning rate to 0.005\n",
            "Iteration 43: Train Loss = 0.4396, Val Loss = 0.3309,  Learning Rate = 0.0050\n",
            "Iteration 44: Train Loss = 0.4177, Val Loss = 0.3353,  Learning Rate = 0.0050\n",
            "Iteration 45: Train Loss = 0.4110, Val Loss = 0.3197,  Learning Rate = 0.0050\n",
            "Iteration 46: Train Loss = 0.3888, Val Loss = 0.3190,  Learning Rate = 0.0050\n",
            "Iteration 47: Train Loss = 0.4044, Val Loss = 0.3026,  Learning Rate = 0.0050\n",
            "Iteration 48: Train Loss = 0.4072, Val Loss = 0.3083,  Learning Rate = 0.0050\n",
            "Iteration 49: Train Loss = 0.4035, Val Loss = 0.2960,  Learning Rate = 0.0050\n",
            "Iteration 50: Train Loss = 0.3855, Val Loss = 0.3063,  Learning Rate = 0.0050\n",
            "Iteration 51: Train Loss = 0.3945, Val Loss = 0.3050,  Learning Rate = 0.0050\n",
            "Iteration 52: Train Loss = 0.3591, Val Loss = 0.2956,  Learning Rate = 0.0050\n",
            "Iteration 53: Train Loss = 0.3833, Val Loss = 0.3012,  Learning Rate = 0.0050\n",
            "Iteration 54: Train Loss = 0.3871, Val Loss = 0.3039,  Learning Rate = 0.0050\n",
            "Iteration 55: Train Loss = 0.3945, Val Loss = 0.3051,  Learning Rate = 0.0050\n",
            "Iteration 56: Train Loss = 0.3870, Val Loss = 0.2997,  Learning Rate = 0.0050\n",
            "Iteration 57: Train Loss = 0.3921, Val Loss = 0.3119,  Learning Rate = 0.0050\n",
            "Iteration 58: Train Loss = 0.3807, Val Loss = 0.3237,  Learning Rate = 0.0050\n",
            "Iteration 59: Train Loss = 0.3675, Val Loss = 0.3108,  Learning Rate = 0.0050\n",
            "Iteration 60: Train Loss = 0.3874, Val Loss = 0.3074,  Learning Rate = 0.0050\n",
            "Iteration 61: Train Loss = 0.3765, Val Loss = 0.3090,  Learning Rate = 0.0050\n",
            "Iteration 62: Train Loss = 0.4095, Val Loss = 0.2885,  Learning Rate = 0.0050\n",
            "Iteration 63: Train Loss = 0.3880, Val Loss = 0.2946,  Learning Rate = 0.0050\n",
            "Iteration 64: Train Loss = 0.3884, Val Loss = 0.2996,  Learning Rate = 0.0050\n",
            "Iteration 65: Train Loss = 0.3751, Val Loss = 0.2883,  Learning Rate = 0.0050\n",
            "Iteration 66: Train Loss = 0.3776, Val Loss = 0.3034,  Learning Rate = 0.0050\n",
            "Iteration 67: Train Loss = 0.3692, Val Loss = 0.2892,  Learning Rate = 0.0050\n",
            "Iteration 68: Train Loss = 0.3797, Val Loss = 0.3119,  Learning Rate = 0.0050\n",
            "Iteration 69: Train Loss = 0.3836, Val Loss = 0.2959,  Learning Rate = 0.0050\n",
            "Iteration 70: Train Loss = 0.3849, Val Loss = 0.2949,  Learning Rate = 0.0050\n",
            "Iteration 71: Train Loss = 0.3912, Val Loss = 0.2987,  Learning Rate = 0.0050\n",
            "Iteration 72: Train Loss = 0.3786, Val Loss = 0.3044,  Learning Rate = 0.0050\n",
            "Iteration 73: Train Loss = 0.4014, Val Loss = 0.2875,  Learning Rate = 0.0050\n",
            "Iteration 74: Train Loss = 0.3734, Val Loss = 0.3115,  Learning Rate = 0.0050\n",
            "Iteration 75: Train Loss = 0.3947, Val Loss = 0.3145,  Learning Rate = 0.0050\n",
            "Iteration 76: Train Loss = 0.3781, Val Loss = 0.3010,  Learning Rate = 0.0050\n",
            "Iteration 77: Train Loss = 0.3440, Val Loss = 0.2996,  Learning Rate = 0.0050\n",
            "Iteration 78: Train Loss = 0.3812, Val Loss = 0.3005,  Learning Rate = 0.0050\n",
            "Iteration 79: Train Loss = 0.3696, Val Loss = 0.2853,  Learning Rate = 0.0050\n",
            "Iteration 80: Train Loss = 0.3844, Val Loss = 0.3071,  Learning Rate = 0.0050\n",
            "Iteration 81: Train Loss = 0.3774, Val Loss = 0.3072,  Learning Rate = 0.0050\n",
            "Iteration 82: Train Loss = 0.3761, Val Loss = 0.3021,  Learning Rate = 0.0050\n",
            "Iteration 83: Train Loss = 0.3828, Val Loss = 0.2880,  Learning Rate = 0.0050\n",
            "Iteration 84: Train Loss = 0.3993, Val Loss = 0.3021,  Learning Rate = 0.0050\n",
            "Iteration 85: Train Loss = 0.3745, Val Loss = 0.2964,  Learning Rate = 0.0050\n",
            "Iteration 86: Train Loss = 0.3589, Val Loss = 0.2942,  Learning Rate = 0.0050\n",
            "Iteration 87: Train Loss = 0.3901, Val Loss = 0.2876,  Learning Rate = 0.0050\n",
            "Iteration 88: Train Loss = 0.3596, Val Loss = 0.2985,  Learning Rate = 0.0050\n",
            "Iteration 89: Train Loss = 0.3850, Val Loss = 0.2928,  Learning Rate = 0.0050\n",
            "Reducing learning rate to 0.0025\n",
            "Iteration 90: Train Loss = 0.3710, Val Loss = 0.2837,  Learning Rate = 0.0025\n",
            "Iteration 91: Train Loss = 0.3591, Val Loss = 0.2747,  Learning Rate = 0.0025\n",
            "Iteration 92: Train Loss = 0.3495, Val Loss = 0.2926,  Learning Rate = 0.0025\n",
            "Iteration 93: Train Loss = 0.3537, Val Loss = 0.3024,  Learning Rate = 0.0025\n",
            "Iteration 94: Train Loss = 0.3745, Val Loss = 0.2915,  Learning Rate = 0.0025\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-10a6f4883015>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_nn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;31m# Test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-10a6f4883015>\u001b[0m in \u001b[0;36mcreate_nn_model\u001b[0;34m(train_X, train_Y, nh, val_X, val_Y, num_iterations, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# Forward pass on batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mbatch_Yhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Yhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-10a6f4883015>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(parameters, X, training, dropout_rate)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m           \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m           \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m           \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m           \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \"\"\"\n\u001b[1;32m   1204\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_and_control_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_sample_and_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_sample_n\u001b[0;34m(self, sample_shape, seed, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     sample_shape, n = self._expand_sample_shape_to_vector(\n\u001b[1;32m   1181\u001b[0m         sample_shape, 'sample_shape')\n\u001b[0;32m-> 1182\u001b[0;31m     samples = self._sample_n(\n\u001b[0m\u001b[1;32m   1183\u001b[0m         n, seed=seed() if callable(seed) else seed, **kwargs)\n\u001b[1;32m   1184\u001b[0m     samples = tf.nest.map_structure(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/distributions/bernoulli.py\u001b[0m in \u001b[0;36m_sample_n\u001b[0;34m(self, n, seed)\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probs_parameter_no_checks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0muniform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-148>\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/prefer_static.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_static\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;34m[\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mstatic_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/_utils.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_fn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/numpy_array.py\u001b[0m in \u001b[0;36m_concat\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m   \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_args_to_matching_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/numpy_array.py\u001b[0m in \u001b[0;36m_args_to_matching_arrays\u001b[0;34m(args_list, dtype_hint)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/numpy_array.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/_utils.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_fn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/ops.py\u001b[0m in \u001b[0;36m_convert_to_tensor\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/ops.py\u001b[0m in \u001b[0;36m_default_convert_to_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m   \u001b[0minferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m   \u001b[0;31m# When a dtype is provided, we can go ahead and try converting to the dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0;31m# and force overflow/underflow if an int64 is converted to an int32.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/ops.py\u001b[0m in \u001b[0;36m_infer_dtype\u001b[0;34m(value, default_dtype)\u001b[0m\n\u001b[1;32m    186\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Try inferring the type from items in the object if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/internal/backend/numpy/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree_util\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_leaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdm_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tree/__init__.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m   r\"\"\"Flattens a possibly nested structure into a list.\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#q5\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/updated_pollution_dataset.csv\")\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_test, val = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "train, test = train_test_split(train_test, test_size=0.2, random_state=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train.iloc[:, :-1])\n",
        "test_scaled = scaler.transform(test.iloc[:, :-1])\n",
        "val_scaled = scaler.transform(val.iloc[:, :-1])\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder()\n",
        "train_labels = encoder.fit_transform(train.iloc[:, -1].values.reshape(-1, 1))\n",
        "val_labels = encoder.transform(val.iloc[:, -1].values.reshape(-1, 1))\n",
        "test_labels = encoder.transform(test.iloc[:, -1].values.reshape(-1, 1))\n",
        "\n",
        "# Transpose data for neural network\n",
        "train_X = train_scaled.T\n",
        "train_Y = train_labels.toarray().T\n",
        "val_X = val_scaled.T\n",
        "val_Y = val_labels.toarray().T\n",
        "test_X = test_scaled.T\n",
        "test_Y = test_labels.toarray().T\n",
        "\n",
        "# Print shapes\n",
        "print(\"Train X shape:\", train_X.shape)\n",
        "print(\"Train Y shape:\", train_Y.shape)\n",
        "print(\"Validation X shape:\", val_X.shape)\n",
        "print(\"Validation Y shape:\", val_Y.shape)\n",
        "print(\"Test X shape:\", test_X.shape)\n",
        "print(\"Test Y shape:\", test_Y.shape)\n",
        "\n",
        "# Initialize parameters for multiple hidden layers\n",
        "def initialize_parameters(nx, nh, ny):\n",
        "    tf.random.set_seed(1)\n",
        "    W = []\n",
        "    b = []\n",
        "\n",
        "    for i in range(len(nh)):\n",
        "        if i == 0:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nx), minval=-0.01, maxval=0.01)))\n",
        "        else:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nh[i - 1]), minval=-0.01, maxval=0.01)))\n",
        "        b.append(tf.Variable(tf.zeros(shape=(nh[i], 1))))\n",
        "\n",
        "    W2 = tf.Variable(tf.random.uniform(shape=(ny, nh[-1]), minval=-0.01, maxval=0.01))\n",
        "    b2 = tf.Variable(tf.zeros(shape=(ny, 1)))\n",
        "\n",
        "    parameters = {\"W\": W,\n",
        "                  \"b\": b,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters\n",
        "\n",
        "# Log-softmax for numerical stability\n",
        "def log_softmax(x):\n",
        "    x_max = tf.reduce_max(x, axis=0, keepdims=True)\n",
        "    return x - x_max - tf.math.log(tf.reduce_sum(tf.exp(x - x_max), axis=0, keepdims=True))\n",
        "\n",
        "# Forward pass with multiple hidden layers\n",
        "def forward_pass(parameters, X,training,dropout_rate=0.2):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "    Z = []\n",
        "    A = [X]\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        Z.append(tf.matmul(parameters[\"W\"][i], A[-1]) + parameters[\"b\"][i])\n",
        "        A.append(tf.nn.relu(Z[-1]))\n",
        "\n",
        "        #ai\n",
        "        if training and i<len(parameters[\"W\"])-1:\n",
        "          keep_prob = 1 - dropout_rate\n",
        "          mask = tfp.distributions.Bernoulli(probs=keep_prob).sample(tf.shape(A[-1]))\n",
        "          mask = tf.cast(mask, tf.float32)\n",
        "          A[-1] = A[-1] * mask / keep_prob\n",
        "\n",
        "    Yhat = tf.matmul(parameters[\"W2\"], A[-1]) + parameters[\"b2\"]\n",
        "    return tf.nn.softmax(Yhat, axis=0)\n",
        "\n",
        "# Compute loss using log-softmax\n",
        "def compute_loss(Y, Yhat):\n",
        "    Yhat = tf.clip_by_value(Yhat, 1e-10, 1.0)#debugged using ai\n",
        "    individual_losses = -tf.reduce_sum(Y * tf.math.log(Yhat), axis=0)\n",
        "    total_loss = tf.reduce_mean(individual_losses)\n",
        "    return total_loss\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters)\n",
        "    return gradients\n",
        "\n",
        "# Update parameters using AdamW\n",
        "def update_parameters_adamw(parameters, gradients, learning_rate, m, v, weight_decay=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        m[\"W\"][i] = beta1 * m[\"W\"][i] + (1 - beta1) * gradients[\"W\"][i]\n",
        "        m[\"b\"][i] = beta1 * m[\"b\"][i] + (1 - beta1) * gradients[\"b\"][i]\n",
        "        v[\"W\"][i] = beta2 * v[\"W\"][i] + (1 - beta2) * (gradients[\"W\"][i] ** 2)\n",
        "        v[\"b\"][i] = beta2 * v[\"b\"][i] + (1 - beta2) * (gradients[\"b\"][i] ** 2)\n",
        "\n",
        "    m[\"W2\"] = beta1 * m[\"W2\"] + (1 - beta1) * gradients[\"W2\"]\n",
        "    m[\"b2\"] = beta1 * m[\"b2\"] + (1 - beta1) * gradients[\"b2\"]\n",
        "    v[\"W2\"] = beta2 * v[\"W2\"] + (1 - beta2) * (gradients[\"W2\"] ** 2)\n",
        "    v[\"b2\"] = beta2 * v[\"b2\"] + (1 - beta2) * (gradients[\"b2\"] ** 2)\n",
        "\n",
        "    # Update parameters with weight decay\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        parameters[\"W\"][i].assign_sub(learning_rate * (m[\"W\"][i] / (tf.sqrt(v[\"W\"][i]) + epsilon) + weight_decay * parameters[\"W\"][i]))\n",
        "        parameters[\"b\"][i].assign_sub(learning_rate * (m[\"b\"][i] / (tf.sqrt(v[\"b\"][i]) + epsilon) + weight_decay * parameters[\"b\"][i]))\n",
        "\n",
        "    parameters[\"W2\"].assign_sub(learning_rate * (m[\"W2\"] / (tf.sqrt(v[\"W2\"]) + epsilon) + weight_decay * parameters[\"W2\"]))\n",
        "    parameters[\"b2\"].assign_sub(learning_rate * (m[\"b2\"] / (tf.sqrt(v[\"b2\"]) + epsilon) + weight_decay * parameters[\"b2\"]))\n",
        "\n",
        "    return parameters, m, v\n",
        "\n",
        "# Create and train the model\n",
        "def create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations, learning_rate, batch_size):\n",
        "    # Safety checks\n",
        "    assert train_X.shape[0] == val_X.shape[0], \"train_X and val_X must have the same number of features\"\n",
        "    assert train_X.shape[1] == train_Y.shape[1], \"train_X and train_Y must have the same number of examples\"\n",
        "    assert val_X.shape[1] == val_Y.shape[1], \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "    # Get the number of features and classes\n",
        "    nx = train_X.shape[0]\n",
        "    ny = train_Y.shape[0]  # Number of classes\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(nx, nh, ny)\n",
        "\n",
        "    # Initialize moments for AdamW\n",
        "    m = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "    v = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "    #checking number of epochs since last improvement for reducing learning rate\n",
        "    last_improvement = 0\n",
        "    patience = 10\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iterations):\n",
        "        permuted_indices = np.random.permutation(train_X.shape[1])\n",
        "        train_X_shuffled = train_X[:, permuted_indices]\n",
        "        train_Y_shuffled = train_Y[:, permuted_indices]\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = train_X_shuffled.shape[1] // batch_size\n",
        "\n",
        "        for j in range(num_batches):\n",
        "            start_idx = j * batch_size\n",
        "            end_idx = (j + 1) * batch_size\n",
        "            batch_X = train_X_shuffled[:, start_idx:end_idx]\n",
        "            batch_Y = train_Y_shuffled[:, start_idx:end_idx]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass on batch\n",
        "                batch_Yhat = forward_pass(parameters, batch_X,training=True,dropout_rate=0.2)\n",
        "                batch_loss = compute_loss(batch_Y, batch_Yhat)\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            gradients = backward_pass(parameters, batch_loss, tape)\n",
        "            parameters, m, v = update_parameters_adamw(parameters, gradients, learning_rate, m, v)\n",
        "\n",
        "            #print(f\"Iteration {i}, Batch {j + 1}/{num_batches}: Batch Loss = {batch_loss:.4f}\")\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        train_loss = epoch_loss.numpy()\n",
        "\n",
        "        # Forward pass on validation data\n",
        "        val_Yhat = forward_pass(parameters, val_X,training=False)\n",
        "        val_loss = compute_loss(val_Y, val_Yhat)\n",
        "\n",
        "        # Print losses\n",
        "        print(f\"Iteration {i}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f},  Learning Rate = {learning_rate:.4f}\")\n",
        "\n",
        "        # Append losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Check for improvement\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_parameters = parameters\n",
        "            last_improvement = 0\n",
        "        else:\n",
        "            last_improvement += 1\n",
        "        if last_improvement >= patience:\n",
        "            # Reduce learning rate\n",
        "            learning_rate /= 2\n",
        "            print(f\"Reducing learning rate to {learning_rate}\")\n",
        "            last_improvement = 0\n",
        "\n",
        "            if learning_rate < 1e-4:\n",
        "                print(\"Learning rate has reached a minimum. Stopping training.\")\n",
        "                break\n",
        "\n",
        "\n",
        "    # Return parameters and history\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return best_parameters, history\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(parameters, X, Y):\n",
        "    Yhat = forward_pass(parameters, X,training=False)\n",
        "    predictions = tf.argmax(Yhat, axis=0)\n",
        "    actual = tf.argmax(Y, axis=0)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, actual), tf.float32))\n",
        "    return accuracy.numpy()\n",
        "\n",
        "# Train the model\n",
        "nx = train_X.shape[0]\n",
        "nh = [10, 8, 5, 4, 2]\n",
        "ny = train_Y.shape[0]\n",
        "\n",
        "parameters, history = create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations=150, learning_rate=0.01, batch_size=32)\n",
        "\n",
        "# Test accuracy\n",
        "test_accuracy = evaluate_model(parameters, test_X, test_Y)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot losses\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predict and print sample results\n",
        "def predict(parameters, X):\n",
        "    Yhat = forward_pass(parameters, X,training=False)\n",
        "    predicted_labels = tf.argmax(Yhat, axis=0)\n",
        "    return predicted_labels\n",
        "\n",
        "predicted_labels = predict(parameters, test_X)\n",
        "true_labels = tf.argmax(test_Y, axis=0)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true_labels), tf.float32))\n",
        "print(f\"Test Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "\n",
        "num_examples = 5\n",
        "for i in range(num_examples):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"  Features: {val_X[:, i]}\")\n",
        "    print(f\"  True Label: {true_labels[i].numpy()}\")\n",
        "    print(f\"  Predicted Label: {predicted_labels[i].numpy()}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADrOeidroDRo"
      },
      "source": [
        "#6\n",
        "6- Add Batch Normalization (5 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnJ9F_skoHV5",
        "outputId": "44ac80a1-57ca-4ac6-d2ee-2939d825e8ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train X shape: (9, 3200)\n",
            "Train Y shape: (4, 3200)\n",
            "Validation X shape: (9, 1000)\n",
            "Validation Y shape: (4, 1000)\n",
            "Test X shape: (9, 800)\n",
            "Test Y shape: (4, 800)\n",
            "Iteration 0: Train Loss = 0.6696, Val Loss = 0.4215,  Learning Rate = 0.0100\n",
            "Iteration 1: Train Loss = 0.5120, Val Loss = 0.3584,  Learning Rate = 0.0100\n",
            "Iteration 2: Train Loss = 0.4909, Val Loss = 0.3426,  Learning Rate = 0.0100\n",
            "Iteration 3: Train Loss = 0.4975, Val Loss = 0.3262,  Learning Rate = 0.0100\n",
            "Iteration 4: Train Loss = 0.4751, Val Loss = 0.3291,  Learning Rate = 0.0100\n",
            "Iteration 5: Train Loss = 0.4795, Val Loss = 0.3114,  Learning Rate = 0.0100\n",
            "Iteration 6: Train Loss = 0.4569, Val Loss = 0.3076,  Learning Rate = 0.0100\n",
            "Iteration 7: Train Loss = 0.4766, Val Loss = 0.3015,  Learning Rate = 0.0100\n",
            "Iteration 8: Train Loss = 0.4791, Val Loss = 0.3004,  Learning Rate = 0.0100\n",
            "Iteration 9: Train Loss = 0.5003, Val Loss = 0.2986,  Learning Rate = 0.0100\n",
            "Iteration 10: Train Loss = 0.4766, Val Loss = 0.2954,  Learning Rate = 0.0100\n",
            "Iteration 11: Train Loss = 0.4694, Val Loss = 0.2896,  Learning Rate = 0.0100\n",
            "Iteration 12: Train Loss = 0.4557, Val Loss = 0.2912,  Learning Rate = 0.0100\n",
            "Iteration 13: Train Loss = 0.4655, Val Loss = 0.2857,  Learning Rate = 0.0100\n",
            "Iteration 14: Train Loss = 0.4725, Val Loss = 0.2914,  Learning Rate = 0.0100\n",
            "Iteration 15: Train Loss = 0.4539, Val Loss = 0.2905,  Learning Rate = 0.0100\n",
            "Iteration 16: Train Loss = 0.4570, Val Loss = 0.2843,  Learning Rate = 0.0100\n",
            "Iteration 17: Train Loss = 0.4439, Val Loss = 0.2861,  Learning Rate = 0.0100\n",
            "Iteration 18: Train Loss = 0.4562, Val Loss = 0.2828,  Learning Rate = 0.0100\n",
            "Iteration 19: Train Loss = 0.4655, Val Loss = 0.2880,  Learning Rate = 0.0100\n",
            "Iteration 20: Train Loss = 0.4718, Val Loss = 0.2869,  Learning Rate = 0.0100\n",
            "Iteration 21: Train Loss = 0.4754, Val Loss = 0.2993,  Learning Rate = 0.0100\n",
            "Iteration 22: Train Loss = 0.4839, Val Loss = 0.2843,  Learning Rate = 0.0100\n",
            "Iteration 23: Train Loss = 0.4466, Val Loss = 0.2862,  Learning Rate = 0.0100\n",
            "Iteration 24: Train Loss = 0.4664, Val Loss = 0.2890,  Learning Rate = 0.0100\n",
            "Iteration 25: Train Loss = 0.4754, Val Loss = 0.2836,  Learning Rate = 0.0100\n",
            "Iteration 26: Train Loss = 0.4364, Val Loss = 0.2794,  Learning Rate = 0.0100\n",
            "Iteration 27: Train Loss = 0.4929, Val Loss = 0.2863,  Learning Rate = 0.0100\n",
            "Iteration 28: Train Loss = 0.4672, Val Loss = 0.2934,  Learning Rate = 0.0100\n",
            "Iteration 29: Train Loss = 0.4756, Val Loss = 0.2850,  Learning Rate = 0.0100\n",
            "Iteration 30: Train Loss = 0.4702, Val Loss = 0.2817,  Learning Rate = 0.0100\n",
            "Iteration 31: Train Loss = 0.4567, Val Loss = 0.2817,  Learning Rate = 0.0100\n",
            "Iteration 32: Train Loss = 0.4537, Val Loss = 0.2820,  Learning Rate = 0.0100\n",
            "Iteration 33: Train Loss = 0.4641, Val Loss = 0.2930,  Learning Rate = 0.0100\n",
            "Iteration 34: Train Loss = 0.4561, Val Loss = 0.2811,  Learning Rate = 0.0100\n",
            "Iteration 35: Train Loss = 0.4513, Val Loss = 0.2866,  Learning Rate = 0.0100\n",
            "Iteration 36: Train Loss = 0.4807, Val Loss = 0.2773,  Learning Rate = 0.0100\n",
            "Iteration 37: Train Loss = 0.4606, Val Loss = 0.2760,  Learning Rate = 0.0100\n",
            "Iteration 38: Train Loss = 0.4469, Val Loss = 0.2880,  Learning Rate = 0.0100\n",
            "Iteration 39: Train Loss = 0.4602, Val Loss = 0.2809,  Learning Rate = 0.0100\n",
            "Iteration 40: Train Loss = 0.4493, Val Loss = 0.2778,  Learning Rate = 0.0100\n",
            "Iteration 41: Train Loss = 0.4751, Val Loss = 0.2820,  Learning Rate = 0.0100\n",
            "Iteration 42: Train Loss = 0.4482, Val Loss = 0.2854,  Learning Rate = 0.0100\n",
            "Iteration 43: Train Loss = 0.4662, Val Loss = 0.2949,  Learning Rate = 0.0100\n",
            "Iteration 44: Train Loss = 0.4498, Val Loss = 0.2767,  Learning Rate = 0.0100\n",
            "Iteration 45: Train Loss = 0.4720, Val Loss = 0.2964,  Learning Rate = 0.0100\n",
            "Iteration 46: Train Loss = 0.4648, Val Loss = 0.2941,  Learning Rate = 0.0100\n",
            "Iteration 47: Train Loss = 0.4564, Val Loss = 0.2830,  Learning Rate = 0.0100\n",
            "Reducing learning rate to 0.005\n",
            "Iteration 48: Train Loss = 0.4816, Val Loss = 0.2835,  Learning Rate = 0.0050\n",
            "Iteration 49: Train Loss = 0.4646, Val Loss = 0.2849,  Learning Rate = 0.0050\n",
            "Iteration 50: Train Loss = 0.4768, Val Loss = 0.2801,  Learning Rate = 0.0050\n",
            "Iteration 51: Train Loss = 0.4493, Val Loss = 0.2781,  Learning Rate = 0.0050\n",
            "Iteration 52: Train Loss = 0.4713, Val Loss = 0.2774,  Learning Rate = 0.0050\n",
            "Iteration 53: Train Loss = 0.4602, Val Loss = 0.2787,  Learning Rate = 0.0050\n",
            "Iteration 54: Train Loss = 0.4815, Val Loss = 0.2759,  Learning Rate = 0.0050\n",
            "Iteration 55: Train Loss = 0.4496, Val Loss = 0.2817,  Learning Rate = 0.0050\n",
            "Iteration 56: Train Loss = 0.4333, Val Loss = 0.2760,  Learning Rate = 0.0050\n",
            "Iteration 57: Train Loss = 0.4657, Val Loss = 0.2791,  Learning Rate = 0.0050\n",
            "Iteration 58: Train Loss = 0.4613, Val Loss = 0.2763,  Learning Rate = 0.0050\n",
            "Iteration 59: Train Loss = 0.4644, Val Loss = 0.2753,  Learning Rate = 0.0050\n",
            "Iteration 60: Train Loss = 0.4488, Val Loss = 0.2798,  Learning Rate = 0.0050\n",
            "Iteration 61: Train Loss = 0.4684, Val Loss = 0.2799,  Learning Rate = 0.0050\n",
            "Iteration 62: Train Loss = 0.4307, Val Loss = 0.2829,  Learning Rate = 0.0050\n",
            "Iteration 63: Train Loss = 0.4620, Val Loss = 0.2783,  Learning Rate = 0.0050\n",
            "Iteration 64: Train Loss = 0.4717, Val Loss = 0.2740,  Learning Rate = 0.0050\n",
            "Iteration 65: Train Loss = 0.4649, Val Loss = 0.2802,  Learning Rate = 0.0050\n",
            "Iteration 66: Train Loss = 0.4583, Val Loss = 0.2792,  Learning Rate = 0.0050\n",
            "Iteration 67: Train Loss = 0.4673, Val Loss = 0.2766,  Learning Rate = 0.0050\n",
            "Iteration 68: Train Loss = 0.4424, Val Loss = 0.2796,  Learning Rate = 0.0050\n",
            "Iteration 69: Train Loss = 0.4620, Val Loss = 0.2814,  Learning Rate = 0.0050\n",
            "Iteration 70: Train Loss = 0.4454, Val Loss = 0.2822,  Learning Rate = 0.0050\n",
            "Iteration 71: Train Loss = 0.4347, Val Loss = 0.2825,  Learning Rate = 0.0050\n",
            "Iteration 72: Train Loss = 0.4450, Val Loss = 0.2799,  Learning Rate = 0.0050\n",
            "Iteration 73: Train Loss = 0.4856, Val Loss = 0.2778,  Learning Rate = 0.0050\n",
            "Iteration 74: Train Loss = 0.4527, Val Loss = 0.2770,  Learning Rate = 0.0050\n",
            "Reducing learning rate to 0.0025\n",
            "Iteration 75: Train Loss = 0.4493, Val Loss = 0.2755,  Learning Rate = 0.0025\n",
            "Iteration 76: Train Loss = 0.4698, Val Loss = 0.2772,  Learning Rate = 0.0025\n",
            "Iteration 77: Train Loss = 0.4573, Val Loss = 0.2862,  Learning Rate = 0.0025\n",
            "Iteration 78: Train Loss = 0.4698, Val Loss = 0.2759,  Learning Rate = 0.0025\n",
            "Iteration 79: Train Loss = 0.4649, Val Loss = 0.2787,  Learning Rate = 0.0025\n",
            "Iteration 80: Train Loss = 0.4569, Val Loss = 0.2790,  Learning Rate = 0.0025\n",
            "Iteration 81: Train Loss = 0.4516, Val Loss = 0.2789,  Learning Rate = 0.0025\n",
            "Iteration 82: Train Loss = 0.4485, Val Loss = 0.2777,  Learning Rate = 0.0025\n",
            "Iteration 83: Train Loss = 0.4552, Val Loss = 0.2761,  Learning Rate = 0.0025\n",
            "Iteration 84: Train Loss = 0.4771, Val Loss = 0.2767,  Learning Rate = 0.0025\n",
            "Reducing learning rate to 0.00125\n",
            "Iteration 85: Train Loss = 0.4595, Val Loss = 0.2786,  Learning Rate = 0.0013\n",
            "Iteration 86: Train Loss = 0.4429, Val Loss = 0.2816,  Learning Rate = 0.0013\n",
            "Iteration 87: Train Loss = 0.4398, Val Loss = 0.2794,  Learning Rate = 0.0013\n",
            "Iteration 88: Train Loss = 0.4414, Val Loss = 0.2793,  Learning Rate = 0.0013\n",
            "Iteration 89: Train Loss = 0.4575, Val Loss = 0.2761,  Learning Rate = 0.0013\n",
            "Iteration 90: Train Loss = 0.4489, Val Loss = 0.2791,  Learning Rate = 0.0013\n",
            "Iteration 91: Train Loss = 0.4544, Val Loss = 0.2753,  Learning Rate = 0.0013\n",
            "Iteration 92: Train Loss = 0.4678, Val Loss = 0.2792,  Learning Rate = 0.0013\n",
            "Iteration 93: Train Loss = 0.4656, Val Loss = 0.2807,  Learning Rate = 0.0013\n",
            "Iteration 94: Train Loss = 0.4804, Val Loss = 0.2877,  Learning Rate = 0.0013\n",
            "Reducing learning rate to 0.000625\n",
            "Iteration 95: Train Loss = 0.4429, Val Loss = 0.2752,  Learning Rate = 0.0006\n",
            "Iteration 96: Train Loss = 0.4714, Val Loss = 0.2746,  Learning Rate = 0.0006\n",
            "Iteration 97: Train Loss = 0.4559, Val Loss = 0.2743,  Learning Rate = 0.0006\n",
            "Iteration 98: Train Loss = 0.4512, Val Loss = 0.2802,  Learning Rate = 0.0006\n",
            "Iteration 99: Train Loss = 0.4678, Val Loss = 0.2806,  Learning Rate = 0.0006\n",
            "Iteration 100: Train Loss = 0.4833, Val Loss = 0.2927,  Learning Rate = 0.0006\n",
            "Iteration 101: Train Loss = 0.4627, Val Loss = 0.2758,  Learning Rate = 0.0006\n",
            "Iteration 102: Train Loss = 0.4432, Val Loss = 0.2770,  Learning Rate = 0.0006\n",
            "Iteration 103: Train Loss = 0.4819, Val Loss = 0.2769,  Learning Rate = 0.0006\n",
            "Iteration 104: Train Loss = 0.4542, Val Loss = 0.2762,  Learning Rate = 0.0006\n",
            "Reducing learning rate to 0.0003125\n",
            "Iteration 105: Train Loss = 0.4576, Val Loss = 0.2750,  Learning Rate = 0.0003\n",
            "Iteration 106: Train Loss = 0.4622, Val Loss = 0.2875,  Learning Rate = 0.0003\n",
            "Iteration 107: Train Loss = 0.4553, Val Loss = 0.2800,  Learning Rate = 0.0003\n",
            "Iteration 108: Train Loss = 0.4549, Val Loss = 0.2766,  Learning Rate = 0.0003\n",
            "Iteration 109: Train Loss = 0.4731, Val Loss = 0.2746,  Learning Rate = 0.0003\n",
            "Iteration 110: Train Loss = 0.4556, Val Loss = 0.2831,  Learning Rate = 0.0003\n",
            "Iteration 111: Train Loss = 0.4418, Val Loss = 0.2752,  Learning Rate = 0.0003\n",
            "Iteration 112: Train Loss = 0.4597, Val Loss = 0.2834,  Learning Rate = 0.0003\n",
            "Iteration 113: Train Loss = 0.4674, Val Loss = 0.2779,  Learning Rate = 0.0003\n",
            "Iteration 114: Train Loss = 0.4708, Val Loss = 0.2867,  Learning Rate = 0.0003\n",
            "Reducing learning rate to 0.00015625\n",
            "Iteration 115: Train Loss = 0.4675, Val Loss = 0.2798,  Learning Rate = 0.0002\n",
            "Iteration 116: Train Loss = 0.4594, Val Loss = 0.2777,  Learning Rate = 0.0002\n",
            "Iteration 117: Train Loss = 0.4618, Val Loss = 0.2765,  Learning Rate = 0.0002\n",
            "Iteration 118: Train Loss = 0.4419, Val Loss = 0.2750,  Learning Rate = 0.0002\n",
            "Iteration 119: Train Loss = 0.4460, Val Loss = 0.2824,  Learning Rate = 0.0002\n",
            "Iteration 120: Train Loss = 0.4610, Val Loss = 0.2864,  Learning Rate = 0.0002\n",
            "Iteration 121: Train Loss = 0.4528, Val Loss = 0.2783,  Learning Rate = 0.0002\n",
            "Iteration 122: Train Loss = 0.4493, Val Loss = 0.2800,  Learning Rate = 0.0002\n",
            "Iteration 123: Train Loss = 0.4450, Val Loss = 0.2896,  Learning Rate = 0.0002\n",
            "Iteration 124: Train Loss = 0.4684, Val Loss = 0.2770,  Learning Rate = 0.0002\n",
            "Reducing learning rate to 7.8125e-05\n",
            "Learning rate has reached a minimum. Stopping training.\n",
            "Test Accuracy: 92.37%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 92.37%\n",
            "Sample 0:\n",
            "  Features: [ 0.23460178  0.36875884 -0.66104846 -0.64409844 -0.79400121 -0.51824907\n",
            "  0.11646977 -0.72303939 -1.1813074 ]\n",
            "  True Label: 0\n",
            "  Predicted Label: 0\n",
            "\n",
            "Sample 1:\n",
            "  Features: [-1.26743862  0.46932479 -0.50321423 -0.69150571 -0.13393883 -0.74244565\n",
            " -1.02049155  1.16992062  0.31553988]\n",
            "  True Label: 1\n",
            "  Predicted Label: 3\n",
            "\n",
            "Sample 2:\n",
            "  Features: [ 1.23596204  1.10414735  0.73922446  1.219372    2.1259358   1.05112697\n",
            "  1.82191174 -1.30762998  0.90126273]\n",
            "  True Label: 3\n",
            "  Predicted Label: 3\n",
            "\n",
            "Sample 3:\n",
            "  Features: [ 0.23460178  1.17957181 -0.53963751 -0.42164893 -0.38006378  0.42337655\n",
            " -0.1769396  -0.05493586  0.28299972]\n",
            "  True Label: 3\n",
            "  Predicted Label: 3\n",
            "\n",
            "Sample 4:\n",
            "  Features: [ 0.48494184  1.79553826  0.03099394  0.23840615  0.97362349  0.90166258\n",
            "  0.42821722 -0.91790292  0.51728886]\n",
            "  True Label: 1\n",
            "  Predicted Label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#q6\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/updated_pollution_dataset.csv\")\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_test, val = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "train, test = train_test_split(train_test, test_size=0.2, random_state=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train.iloc[:, :-1])\n",
        "test_scaled = scaler.transform(test.iloc[:, :-1])\n",
        "val_scaled = scaler.transform(val.iloc[:, :-1])\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder()\n",
        "train_labels = encoder.fit_transform(train.iloc[:, -1].values.reshape(-1, 1))\n",
        "val_labels = encoder.transform(val.iloc[:, -1].values.reshape(-1, 1))\n",
        "test_labels = encoder.transform(test.iloc[:, -1].values.reshape(-1, 1))\n",
        "\n",
        "# Transpose data for neural network\n",
        "train_X = train_scaled.T\n",
        "train_Y = train_labels.toarray().T\n",
        "val_X = val_scaled.T\n",
        "val_Y = val_labels.toarray().T\n",
        "test_X = test_scaled.T\n",
        "test_Y = test_labels.toarray().T\n",
        "\n",
        "# Print shapes\n",
        "print(\"Train X shape:\", train_X.shape)\n",
        "print(\"Train Y shape:\", train_Y.shape)\n",
        "print(\"Validation X shape:\", val_X.shape)\n",
        "print(\"Validation Y shape:\", val_Y.shape)\n",
        "print(\"Test X shape:\", test_X.shape)\n",
        "print(\"Test Y shape:\", test_Y.shape)\n",
        "\n",
        "# Initialize parameters for multiple hidden layers\n",
        "def initialize_parameters(nx, nh, ny):\n",
        "    tf.random.set_seed(1)\n",
        "    W = []\n",
        "    b = []\n",
        "    gamma = []\n",
        "    beta = []\n",
        "    running_mean = []\n",
        "    running_var = []\n",
        "\n",
        "    for i in range(len(nh)):\n",
        "        if i == 0:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nx), minval=-0.01, maxval=0.01)))\n",
        "        else:\n",
        "            W.append(tf.Variable(tf.random.uniform(shape=(nh[i], nh[i - 1]), minval=-0.01, maxval=0.01)))\n",
        "        b.append(tf.Variable(tf.zeros(shape=(nh[i], 1))))\n",
        "        gamma.append(tf.Variable(tf.ones(shape=(nh[i], 1))))\n",
        "        beta.append(tf.Variable(tf.zeros(shape=(nh[i], 1))))\n",
        "        running_mean.append(tf.Variable(tf.zeros(shape=(nh[i], 1)),trainable=False))\n",
        "        running_var.append(tf.Variable(tf.ones(shape=(nh[i], 1)),trainable=False))\n",
        "\n",
        "    W2 = tf.Variable(tf.random.uniform(shape=(ny, nh[-1]), minval=-0.01, maxval=0.01))\n",
        "    b2 = tf.Variable(tf.zeros(shape=(ny, 1)))\n",
        "\n",
        "    parameters = {\"W\": W,\n",
        "                  \"b\": b,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"gamma\": gamma,\n",
        "                  \"beta\": beta,\n",
        "                  \"running_mean\": running_mean,\n",
        "                  \"running_var\": running_var}\n",
        "    return parameters\n",
        "\n",
        "def batch_norm(Z, gamma, beta, running_mean, running_var, training, alpha=0.9, epsilon=1e-5):\n",
        "  if training:\n",
        "    mean = tf.reduce_mean(Z, axis=1, keepdims=True)\n",
        "    var = tf.reduce_mean(tf.square(Z - mean), axis=1,keepdims=True)\n",
        "    running_mean.assign(alpha * running_mean + (1 - alpha) * mean)\n",
        "    running_var.assign(alpha * running_var + (1 - alpha) * var)\n",
        "  else:\n",
        "    mean = running_mean\n",
        "    var = running_var\n",
        "\n",
        "  Z_norm = (Z - mean) / tf.sqrt(var + epsilon)\n",
        "  return gamma * Z_norm + beta\n",
        "\n",
        "\n",
        "# Log-softmax for numerical stability\n",
        "def log_softmax(x):\n",
        "    x_max = tf.reduce_max(x, axis=0, keepdims=True)\n",
        "    return x - x_max - tf.math.log(tf.reduce_sum(tf.exp(x - x_max), axis=0, keepdims=True))\n",
        "\n",
        "# Forward pass with multiple hidden layers\n",
        "def forward_pass(parameters, X,training,dropout_rate=0.2):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "    Z = []\n",
        "    A = [X]\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        Z.append(tf.matmul(parameters[\"W\"][i], A[-1]) + parameters[\"b\"][i])\n",
        "        #A.append(tf.nn.relu(Z[-1]))\n",
        "        Z_norm = batch_norm(Z[-1], parameters[\"gamma\"][i], parameters[\"beta\"][i], parameters[\"running_mean\"][i], parameters[\"running_var\"][i], training)\n",
        "        A.append(Z_norm)\n",
        "\n",
        "        #this code segment was generated by ai\n",
        "        if training and i<len(parameters[\"W\"])-1:\n",
        "          keep_prob = 1 - dropout_rate\n",
        "          mask = tfp.distributions.Bernoulli(probs=keep_prob).sample(tf.shape(A[-1]))\n",
        "          mask = tf.cast(mask, tf.float32)\n",
        "          A[-1] = A[-1] * mask / keep_prob\n",
        "\n",
        "    Yhat = tf.matmul(parameters[\"W2\"], A[-1]) + parameters[\"b2\"]\n",
        "    return tf.nn.softmax(Yhat, axis=0)\n",
        "\n",
        "# Compute loss using log-softmax\n",
        "def compute_loss(Y, Yhat):\n",
        "    Yhat = tf.clip_by_value(Yhat, 1e-10, 1.0) #debugged using ai\n",
        "    individual_losses = -tf.reduce_sum(Y * tf.math.log(Yhat), axis=0)\n",
        "    total_loss = tf.reduce_mean(individual_losses)\n",
        "    return total_loss\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters)\n",
        "    return gradients\n",
        "\n",
        "# Update parameters using AdamW\n",
        "def update_parameters_adamw(parameters, gradients, learning_rate, m, v, weight_decay=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        m[\"W\"][i] = beta1 * m[\"W\"][i] + (1 - beta1) * gradients[\"W\"][i]\n",
        "        m[\"b\"][i] = beta1 * m[\"b\"][i] + (1 - beta1) * gradients[\"b\"][i]\n",
        "        v[\"W\"][i] = beta2 * v[\"W\"][i] + (1 - beta2) * (gradients[\"W\"][i] ** 2)\n",
        "        v[\"b\"][i] = beta2 * v[\"b\"][i] + (1 - beta2) * (gradients[\"b\"][i] ** 2)\n",
        "\n",
        "    m[\"W2\"] = beta1 * m[\"W2\"] + (1 - beta1) * gradients[\"W2\"]\n",
        "    m[\"b2\"] = beta1 * m[\"b2\"] + (1 - beta1) * gradients[\"b2\"]\n",
        "    v[\"W2\"] = beta2 * v[\"W2\"] + (1 - beta2) * (gradients[\"W2\"] ** 2)\n",
        "    v[\"b2\"] = beta2 * v[\"b2\"] + (1 - beta2) * (gradients[\"b2\"] ** 2)\n",
        "\n",
        "    # Update parameters with weight decay\n",
        "    for i in range(len(parameters[\"W\"])):\n",
        "        parameters[\"W\"][i].assign_sub(learning_rate * (m[\"W\"][i] / (tf.sqrt(v[\"W\"][i]) + epsilon) + weight_decay * parameters[\"W\"][i]))\n",
        "        parameters[\"b\"][i].assign_sub(learning_rate * (m[\"b\"][i] / (tf.sqrt(v[\"b\"][i]) + epsilon) + weight_decay * parameters[\"b\"][i]))\n",
        "\n",
        "    parameters[\"W2\"].assign_sub(learning_rate * (m[\"W2\"] / (tf.sqrt(v[\"W2\"]) + epsilon) + weight_decay * parameters[\"W2\"]))\n",
        "    parameters[\"b2\"].assign_sub(learning_rate * (m[\"b2\"] / (tf.sqrt(v[\"b2\"]) + epsilon) + weight_decay * parameters[\"b2\"]))\n",
        "\n",
        "    return parameters, m, v\n",
        "\n",
        "# Create and train the model\n",
        "def create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations, learning_rate, batch_size):\n",
        "    # Safety checks\n",
        "    assert train_X.shape[0] == val_X.shape[0], \"train_X and val_X must have the same number of features\"\n",
        "    assert train_X.shape[1] == train_Y.shape[1], \"train_X and train_Y must have the same number of examples\"\n",
        "    assert val_X.shape[1] == val_Y.shape[1], \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "    # Get the number of features and classes\n",
        "    nx = train_X.shape[0]\n",
        "    ny = train_Y.shape[0]  # Number of classes\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(nx, nh, ny)\n",
        "\n",
        "    # Initialize moments for AdamW\n",
        "    m = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "    v = {\"W\": [tf.zeros_like(w) for w in parameters[\"W\"]],\n",
        "         \"b\": [tf.zeros_like(b) for b in parameters[\"b\"]],\n",
        "         \"W2\": tf.zeros_like(parameters[\"W2\"]),\n",
        "         \"b2\": tf.zeros_like(parameters[\"b2\"])}\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "    #checking number of epochs since last improvement for reducing learning rate\n",
        "    last_improvement = 0\n",
        "    patience = 10\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iterations):\n",
        "        permuted_indices = np.random.permutation(train_X.shape[1])\n",
        "        train_X_shuffled = train_X[:, permuted_indices]\n",
        "        train_Y_shuffled = train_Y[:, permuted_indices]\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = train_X_shuffled.shape[1] // batch_size\n",
        "\n",
        "        for j in range(num_batches):\n",
        "            start_idx = j * batch_size\n",
        "            end_idx = (j + 1) * batch_size\n",
        "            batch_X = train_X_shuffled[:, start_idx:end_idx]\n",
        "            batch_Y = train_Y_shuffled[:, start_idx:end_idx]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass on batch\n",
        "                batch_Yhat = forward_pass(parameters, batch_X,training=True,dropout_rate=0.2)\n",
        "                batch_loss = compute_loss(batch_Y, batch_Yhat)\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            gradients = backward_pass(parameters, batch_loss, tape)\n",
        "            parameters, m, v = update_parameters_adamw(parameters, gradients, learning_rate, m, v)\n",
        "\n",
        "            #print(f\"Iteration {i}, Batch {j + 1}/{num_batches}: Batch Loss = {batch_loss:.4f}\")\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        train_loss = epoch_loss.numpy()\n",
        "\n",
        "        # Forward pass on validation data\n",
        "        val_Yhat = forward_pass(parameters, val_X,training=False)\n",
        "        val_loss = compute_loss(val_Y, val_Yhat)\n",
        "\n",
        "        # Print losses\n",
        "        print(f\"Iteration {i}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f},  Learning Rate = {learning_rate:.4f}\")\n",
        "\n",
        "        # Append losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Check for improvement\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_parameters = parameters\n",
        "            last_improvement = 0\n",
        "        else:\n",
        "            last_improvement += 1\n",
        "        if last_improvement >= patience:\n",
        "            # Reduce learning rate\n",
        "            learning_rate /= 2\n",
        "            print(f\"Reducing learning rate to {learning_rate}\")\n",
        "            last_improvement = 0\n",
        "\n",
        "            if learning_rate < 1e-4:\n",
        "                print(\"Learning rate has reached a minimum. Stopping training.\")\n",
        "                break\n",
        "\n",
        "\n",
        "    # Return parameters and history\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return best_parameters, history\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(parameters, X, Y):\n",
        "    Yhat = forward_pass(parameters, X,training=False)\n",
        "    predictions = tf.argmax(Yhat, axis=0)\n",
        "    actual = tf.argmax(Y, axis=0)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, actual), tf.float32))\n",
        "    return accuracy.numpy()\n",
        "\n",
        "# Train the model\n",
        "nx = train_X.shape[0]\n",
        "nh = [10, 8, 5, 4, 2]\n",
        "ny = train_Y.shape[0]\n",
        "\n",
        "parameters, history = create_nn_model(train_X, train_Y, nh, val_X, val_Y, num_iterations=150, learning_rate=0.01, batch_size=32)\n",
        "\n",
        "# Test accuracy\n",
        "test_accuracy = evaluate_model(parameters, test_X, test_Y)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot losses\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predict and print sample results\n",
        "def predict(parameters, X):\n",
        "    Yhat = forward_pass(parameters, X,training=False)\n",
        "    predicted_labels = tf.argmax(Yhat, axis=0)\n",
        "    return predicted_labels\n",
        "\n",
        "predicted_labels = predict(parameters, test_X)\n",
        "true_labels = tf.argmax(test_Y, axis=0)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true_labels), tf.float32))\n",
        "print(f\"Test Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "\n",
        "num_examples = 5\n",
        "for i in range(num_examples):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"  Features: {val_X[:, i]}\")\n",
        "    print(f\"  True Label: {true_labels[i].numpy()}\")\n",
        "    print(f\"  Predicted Label: {predicted_labels[i].numpy()}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "TUwNhdPCf8Xb",
        "Er-m2lLAgE-w",
        "aZGlrIKTgI8P",
        "bVrib9RPuKnG",
        "kik4rlPfufAT",
        "ADrOeidroDRo",
        "Qt7Cu0gH-eRI"
      ],
      "authorship_tag": "ABX9TyPCLzgT035IJYduXjRSDBaQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}