{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saf02Sam/general/blob/main/Saffa_Samreen_assignment1_problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50xgi-vH7voV"
      },
      "source": [
        "#Problem1â€”Creating a simple neural network for regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_wfWfbo71wX"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtdzHUDseXXG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62WKQra974f3"
      },
      "source": [
        "Initializing parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_K-Iug2ebyV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "nx is the number of neurons in the input layer (i.e., the number of features in the dataset)\n",
        "nh is the number of neurons in the hidden layer\n",
        "ny is the number of neurons in the output layer (For this example we are using one nueron in the output layer so ny=1)\n",
        "\"\"\"\n",
        "def initialize_parameters(nx,nh,ny):\n",
        "    #set tensorflow global random seed\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "    #initialize weights to small random numbers and biases to zeros for each layer. Note that weights and biases are defined as tensorflow variables instead of numpy arrays\n",
        "    W1=tf.Variable(tf.random.uniform(shape=(nh,nx), minval=-0.01, maxval=0.01), name=\"W1\")\n",
        "    b1=tf.Variable(tf.zeros(shape=(nh,1),name=\"b1\" ))\n",
        "    W2=tf.Variable(tf.random.uniform(shape=(ny,nh), minval=-0.01, maxval=0.01), name=\"W2\")\n",
        "    b2=tf.Variable(tf.zeros(shape=(ny,1), name=\"b2\"))\n",
        "\n",
        "    #create a dictionary of network parameters\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeHMopq87-0m"
      },
      "source": [
        "Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVKN9Fbged7J"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In forward pass we do the computations in the computational graph. We cache the intermediate nodes we will later need in the backward pass\n",
        "\"\"\"\n",
        "def forward_pass(parameters,X):\n",
        "    #the input image is read as an integer, use tf.cast to cast it to float before using it in fowrard pass computation.\n",
        "    X= tf.cast(X, tf.float32)\n",
        "    #not using any activation function because this is a regression problem\n",
        "    Z1= tf.matmul(parameters[\"W1\"],X)+ parameters[\"b1\"]\n",
        "    A1=Z1\n",
        "    Z2=tf.matmul(parameters[\"W2\"],A1)+parameters[\"b2\"]\n",
        "\n",
        "    Yhat=Z2\n",
        "    # Print intermediate values\n",
        "    #print(\"Z1:\", Z1)\n",
        "    #print(\"A1:\", A1)\n",
        "    #print(\"Z2:\", Z2)\n",
        "    #print(\"Yhat:\", Yhat)\n",
        "\n",
        "    return Yhat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZHknxNU8BPR"
      },
      "source": [
        "Computing loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySshTEKuegLM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "n is the number of examples, y is a vector of actual/observed outputs and yhat is a vector of predicted outputs\n",
        "\"\"\"\n",
        "def compute_loss(Y,Yhat):\n",
        "  #print(\"Y:\", Y)\n",
        "  #print(\"Yhat:\", Yhat)\n",
        "  per_sample_losses = abs(Y-Yhat)\n",
        "\n",
        "  loss=tf.reduce_mean(per_sample_losses)\n",
        "    #print(\"Loss:\", loss)\n",
        "  return loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuIfOMnF8Dnn"
      },
      "source": [
        "Backward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw5hHnIzeiU7"
      },
      "outputs": [],
      "source": [
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients= tape.gradient(loss,parameters)\n",
        "    #print(\"gradients:\", gradients)\n",
        "    return gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz9DUfze8FNH"
      },
      "source": [
        "Updating parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x38LX61welFk"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters[\"W1\"].assign_sub(learning_rate*gradients[\"W1\"])\n",
        "    parameters[\"W2\"].assign_sub(learning_rate*gradients[\"W2\"])\n",
        "    parameters[\"b1\"].assign_sub(learning_rate*gradients[\"b1\"])\n",
        "    parameters[\"b2\"].assign_sub(learning_rate*gradients[\"b2\"])\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCrr_WO08HbE"
      },
      "source": [
        "Creating the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5CY5e5wenC1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Arguments: train_X: is the training dataset (features)\n",
        "           train_Y: is the vector of labels for training_X\n",
        "           val_X: is the vector of validation dataset (features)\n",
        "           val_y: is the vector of labels for val_X\n",
        "           nh: is the number of neurons in the hidden layer\n",
        "           num_iterations: The number of iterations of gradient descent\n",
        "\"\"\"\n",
        "def create_nn_model(train_X,train_Y, val_X, val_Y,nh, num_iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    Do some safety check on the data before proceeding.\n",
        "    train_X and val_X must have the same number of features (i.e., same number of rows)\n",
        "    train_X must have the same number of examples as train_Y (i.e., same number of columns )\n",
        "    val_X must have the same number of examples as Val_Y\n",
        "    \"\"\"\n",
        "    assert(train_X.shape[0]==val_X.shape[0]), \"train_X and val_X must have the same number of features\"\n",
        "    assert(train_X.shape[1]==train_Y.size), \"train_X and train_Y must have the same number of examples\"\n",
        "    assert(val_X.shape[1]==val_Y.size), \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "\n",
        "    #getting the number of features\n",
        "    nx=train_X.shape[0]\n",
        "\n",
        "    # We want to use this network for regression, so we have only one neuron in the output layer with no activation\n",
        "    ny=1\n",
        "\n",
        "    # initializing the parameteres\n",
        "    parameters=initialize_parameters(nx,nh,ny)\n",
        "\n",
        "\n",
        "    #initialize lists to store the training and valideation losses.\n",
        "    val_losses=[]\n",
        "    train_losses=[]\n",
        "\n",
        "    #run num_iterations of gradient descent\n",
        "    for i in range (0, num_iterations):\n",
        "\n",
        "      \"\"\"\n",
        "        run forward pass and compute the loss function on training and validation data.\n",
        "        Note that the forward pass and loss computations on the training data are enclosed inside the gradient tape context in order to build the computational graph.\n",
        "        The gradients are only computed on the training data and used to update the parameter. Validation data is not used for training and updating the parameters.\n",
        "        \"\"\"\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        train_Yhat=forward_pass(parameters,train_X)\n",
        "        train_loss=compute_loss(train_Y,train_Yhat)\n",
        "\n",
        "\n",
        "       #compute validation loss\n",
        "      Yhat_val= forward_pass(parameters,val_X)\n",
        "      val_loss=compute_loss(val_Y,Yhat_val)\n",
        "\n",
        "      #print the trianing loss and validation loss for each iteration.\n",
        "      print(\"iteration {} :train_loss:{} val_loss{}\".format(i,train_loss,val_loss))\n",
        "\n",
        "       # append the train and validation loss for the current iteration to the train_losses and val_losses\n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "\n",
        "      \"\"\"\n",
        "      Compute the gradients and update the parameters\n",
        "      \"\"\"\n",
        "      #compute the gradients on the training data\n",
        "      gradients=backward_pass(parameters,train_loss,tape)\n",
        "\n",
        "      # update the parameters\n",
        "      parameters=update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "    #create a dictionary history and put train_loss and validaiton_loss in it\n",
        "    history={\"val_loss\": val_losses,\n",
        "             \"train_loss\": train_losses}\n",
        "\n",
        "\n",
        "    #return the parameters and the history\n",
        "    return parameters, history\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTXrzvUE8cny"
      },
      "source": [
        "Predicting the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vStIpTgYepn3"
      },
      "outputs": [],
      "source": [
        "def predict(parameters,X, prob_threshold=0.5):\n",
        "    Yhat=forward_pass(parameters, X)\n",
        "    # predicted label is the same as predicted value, because this is a regression problem\n",
        "    predicted_label=Yhat\n",
        "    return predicted_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBvQpurA8ucv"
      },
      "source": [
        "Uploading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToPczR9De1fJ",
        "outputId": "9f260a09-74ff-42d4-9339-1d16ce9a0a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300153, 12)\n"
          ]
        }
      ],
      "source": [
        "# reading the input datasets train.csv and validation.csv\n",
        "dataset=pd.read_csv(\"/content/Clean_Dataset.csv\")\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayNAl6898yTW"
      },
      "source": [
        "One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeU3i8jy2UHR",
        "outputId": "f1009758-c2a8-45ed-ca98-8b92d3235ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   airline_AirAsia  airline_Air_India  airline_GO_FIRST  airline_Indigo  \\\n",
            "0              0.0                0.0               0.0             0.0   \n",
            "1              0.0                0.0               0.0             0.0   \n",
            "2              1.0                0.0               0.0             0.0   \n",
            "3              0.0                0.0               0.0             0.0   \n",
            "4              0.0                0.0               0.0             0.0   \n",
            "\n",
            "   airline_Other  airline_Vistara  flight_Other  source_city_Bangalore  \\\n",
            "0            1.0              0.0           1.0                    0.0   \n",
            "1            1.0              0.0           1.0                    0.0   \n",
            "2            0.0              0.0           1.0                    0.0   \n",
            "3            0.0              1.0           1.0                    0.0   \n",
            "4            0.0              1.0           1.0                    0.0   \n",
            "\n",
            "   source_city_Chennai  source_city_Delhi  ...  destination_city_Delhi  \\\n",
            "0                  0.0                1.0  ...                     0.0   \n",
            "1                  0.0                1.0  ...                     0.0   \n",
            "2                  0.0                1.0  ...                     0.0   \n",
            "3                  0.0                1.0  ...                     0.0   \n",
            "4                  0.0                1.0  ...                     0.0   \n",
            "\n",
            "   destination_city_Hyderabad  destination_city_Kolkata  \\\n",
            "0                         0.0                       0.0   \n",
            "1                         0.0                       0.0   \n",
            "2                         0.0                       0.0   \n",
            "3                         0.0                       0.0   \n",
            "4                         0.0                       0.0   \n",
            "\n",
            "   destination_city_Mumbai  class_Business  class_Economy  Unnamed: 0  \\\n",
            "0                      1.0             0.0            1.0           0   \n",
            "1                      1.0             0.0            1.0           1   \n",
            "2                      1.0             0.0            1.0           2   \n",
            "3                      1.0             0.0            1.0           3   \n",
            "4                      1.0             0.0            1.0           4   \n",
            "\n",
            "   duration  days_left  price  \n",
            "0      2.17          1   5953  \n",
            "1      2.33          1   5953  \n",
            "2      2.17          1   5956  \n",
            "3      2.25          1   5955  \n",
            "4      2.33          1   5955  \n",
            "\n",
            "[5 rows x 40 columns]\n"
          ]
        }
      ],
      "source": [
        "#Processing categorical data using one hot encoding\n",
        "categorical_cols = dataset.select_dtypes(include=['object']).columns\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "threshold = 0.05\n",
        "for col in categorical_cols:\n",
        "    freq = dataset[col].value_counts(normalize=True)\n",
        "    rare_categories = freq[freq < threshold].index\n",
        "    dataset[col] = dataset[col].replace(rare_categories, \"Other\")\n",
        "    dataset[col] = dataset[col].replace(rare_categories, \"Other\")\n",
        "    dataset[col] = dataset[col].replace(rare_categories, \"Other\")\n",
        "dataset_encoded = pd.DataFrame(encoder.fit_transform(dataset[categorical_cols].astype(str)))\n",
        "#test_encoded = pd.DataFrame(encoder.transform(test[categorical_cols].astype(str)))\n",
        "#val_encoded = pd.DataFrame(encoder.transform(val[categorical_cols].astype(str)))\n",
        "\n",
        "# Get feature names after encoding\n",
        "categorical_cols_str = [str(col) for col in categorical_cols]\n",
        "feature_names = encoder.get_feature_names_out(categorical_cols_str)\n",
        "dataset_encoded.columns = feature_names\n",
        "#test_encoded.columns = feature_names\n",
        "#val_encoded.columns = feature_names\n",
        "\n",
        "dataset_encoded = dataset_encoded.join(dataset.drop(columns=categorical_cols, axis=1))\n",
        "#test_encoded = test_encoded.join(test.drop(columns=categorical_cols, axis=1))\n",
        "#val_encoded = val_encoded.join(val.drop(columns=categorical_cols, axis=1))\n",
        "\n",
        "print(dataset_encoded.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmfE1Wgt84Ul"
      },
      "source": [
        "Splitting dataset into train, test and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7tjKyyzSA_8",
        "outputId": "fe00b054-ea88-4044-a97f-f18c8daad684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(192097, 40)\n",
            "(48025, 40)\n",
            "(60031, 40)\n"
          ]
        }
      ],
      "source": [
        "#splitting into train, test and validation\n",
        "train_test, val= train_test_split(dataset_encoded, test_size=0.2, random_state=1)\n",
        "train, test= train_test_split(train_test, test_size=0.2, random_state=1)\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(val.shape)\n",
        "#print(train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbEYgLBx9BuH"
      },
      "source": [
        "Normalizaation and scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtU1unCnxoA9"
      },
      "outputs": [],
      "source": [
        "#normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train[train.columns] = scaler.fit_transform(train[train.columns])\n",
        "test[test.columns] = scaler.transform(test[test.columns])\n",
        "val[val.columns] = scaler.transform(val[val.columns])\n",
        "\n",
        "#scaling target variable\n",
        "train['price'] = scaler.fit_transform(train['price'].values.reshape(-1, 1))\n",
        "test['price'] = scaler.transform(test['price'].values.reshape(-1, 1))\n",
        "val['price'] = scaler.transform(val['price'].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gZCb9aTvt0h"
      },
      "outputs": [],
      "source": [
        "train=train.to_numpy()\n",
        "test=test.to_numpy()\n",
        "val=val.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPhoiUGY9Gkp"
      },
      "source": [
        "Splitting into features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnz_QxPq-Ps6",
        "outputId": "9f35dc0a-1517-45d6-c058-efbe3691fe2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(39, 192097)\n",
            "(1, 192097)\n",
            "(39, 48025)\n",
            "(1, 48025)\n",
            "(39, 60031)\n",
            "(1, 60031)\n"
          ]
        }
      ],
      "source": [
        "train_X=train[:,:-1,]\n",
        "test_X=test[:,:-1,]\n",
        "val_X=val[:,:-1,]\n",
        "train_Y=train[:, -1]\n",
        "test_Y=test[:, -1]\n",
        "val_Y=val[:, -1]\n",
        "\n",
        "train_X=np.transpose(train_X)\n",
        "val_X=np.transpose(val_X)\n",
        "test_X=np.transpose(test_X)\n",
        "train_Y=np.reshape(train_Y, (1,train_Y.size))\n",
        "val_Y=np.reshape(val_Y, (1,val_Y.size))\n",
        "test_Y=np.reshape(test_Y, (1,test_Y.size))\n",
        "#train_Y=np.transpose(train_X)\n",
        "#val_Y=np.transpose(val_X)\n",
        "\n",
        "\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_Y.shape)\n",
        "print(val_X.shape)\n",
        "print(val_Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U29EZrZ9NSv"
      },
      "source": [
        "Training the model and computing loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebVFPagNvs7K",
        "outputId": "adc9bba6-9551-422b-9c81-7d9ba500f153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:0.8710129261016846 val_loss0.8688726425170898\n",
            "iteration 1 :train_loss:0.8694311380386353 val_loss0.8672743439674377\n",
            "iteration 2 :train_loss:0.8678706884384155 val_loss0.8656988143920898\n",
            "iteration 3 :train_loss:0.8663241863250732 val_loss0.8641376495361328\n",
            "iteration 4 :train_loss:0.8647802472114563 val_loss0.862579882144928\n",
            "iteration 5 :train_loss:0.863237202167511 val_loss0.8610233664512634\n",
            "iteration 6 :train_loss:0.8616973161697388 val_loss0.859469473361969\n",
            "iteration 7 :train_loss:0.860162615776062 val_loss0.857921302318573\n",
            "iteration 8 :train_loss:0.8586311340332031 val_loss0.8563764095306396\n",
            "iteration 9 :train_loss:0.8571000099182129 val_loss0.8548319339752197\n",
            "iteration 10 :train_loss:0.8555684089660645 val_loss0.8532875180244446\n",
            "iteration 11 :train_loss:0.8540379405021667 val_loss0.8517444729804993\n",
            "iteration 12 :train_loss:0.8525108098983765 val_loss0.850204348564148\n",
            "iteration 13 :train_loss:0.8509848117828369 val_loss0.8486658334732056\n",
            "iteration 14 :train_loss:0.8494589328765869 val_loss0.8471282124519348\n",
            "iteration 15 :train_loss:0.8479323983192444 val_loss0.8455900549888611\n",
            "iteration 16 :train_loss:0.846405565738678 val_loss0.8440517783164978\n",
            "iteration 17 :train_loss:0.8448787331581116 val_loss0.8425130844116211\n",
            "iteration 18 :train_loss:0.8433513045310974 val_loss0.8409735560417175\n",
            "iteration 19 :train_loss:0.8418231010437012 val_loss0.8394336104393005\n",
            "iteration 20 :train_loss:0.8402934074401855 val_loss0.837892472743988\n",
            "iteration 21 :train_loss:0.8387627005577087 val_loss0.8363502025604248\n",
            "iteration 22 :train_loss:0.8372324705123901 val_loss0.834808349609375\n",
            "iteration 23 :train_loss:0.8357023000717163 val_loss0.8332661986351013\n",
            "iteration 24 :train_loss:0.8341705203056335 val_loss0.83172208070755\n",
            "iteration 25 :train_loss:0.8326375484466553 val_loss0.8301768898963928\n",
            "iteration 26 :train_loss:0.8311033248901367 val_loss0.8286303281784058\n",
            "iteration 27 :train_loss:0.8295672535896301 val_loss0.8270823359489441\n",
            "iteration 28 :train_loss:0.8280282616615295 val_loss0.8255322575569153\n",
            "iteration 29 :train_loss:0.8264861702919006 val_loss0.8239790797233582\n",
            "iteration 30 :train_loss:0.8249406814575195 val_loss0.8224226832389832\n",
            "iteration 31 :train_loss:0.8233917951583862 val_loss0.8208639025688171\n",
            "iteration 32 :train_loss:0.8218392133712769 val_loss0.8193021416664124\n",
            "iteration 33 :train_loss:0.8202829957008362 val_loss0.8177365064620972\n",
            "iteration 34 :train_loss:0.8187245726585388 val_loss0.8161680102348328\n",
            "iteration 35 :train_loss:0.8171629905700684 val_loss0.8145968317985535\n",
            "iteration 36 :train_loss:0.8155977725982666 val_loss0.8130229115486145\n",
            "iteration 37 :train_loss:0.8140285015106201 val_loss0.8114448189735413\n",
            "iteration 38 :train_loss:0.8124558925628662 val_loss0.8098623156547546\n",
            "iteration 39 :train_loss:0.8108806014060974 val_loss0.8082771301269531\n",
            "iteration 40 :train_loss:0.8093032240867615 val_loss0.8066899180412292\n",
            "iteration 41 :train_loss:0.8077263832092285 val_loss0.8051034212112427\n",
            "iteration 42 :train_loss:0.8061453104019165 val_loss0.8035131692886353\n",
            "iteration 43 :train_loss:0.8045605421066284 val_loss0.8019185662269592\n",
            "iteration 44 :train_loss:0.802975594997406 val_loss0.8003237247467041\n",
            "iteration 45 :train_loss:0.8013858199119568 val_loss0.7987241148948669\n",
            "iteration 46 :train_loss:0.7997903823852539 val_loss0.797119140625\n",
            "iteration 47 :train_loss:0.7981901168823242 val_loss0.7955100536346436\n",
            "iteration 48 :train_loss:0.7965835332870483 val_loss0.7938953042030334\n",
            "iteration 49 :train_loss:0.7949692010879517 val_loss0.7922734618186951\n",
            "iteration 50 :train_loss:0.7933472394943237 val_loss0.7906450033187866\n",
            "iteration 51 :train_loss:0.7917192578315735 val_loss0.7890115976333618\n",
            "iteration 52 :train_loss:0.7900864481925964 val_loss0.7873736619949341\n",
            "iteration 53 :train_loss:0.7884498238563538 val_loss0.7857322096824646\n",
            "iteration 54 :train_loss:0.7868096232414246 val_loss0.784087061882019\n",
            "iteration 55 :train_loss:0.7851629257202148 val_loss0.782435417175293\n",
            "iteration 56 :train_loss:0.7835085988044739 val_loss0.7807760834693909\n",
            "iteration 57 :train_loss:0.7818447351455688 val_loss0.7791076898574829\n",
            "iteration 58 :train_loss:0.7801734209060669 val_loss0.777432382106781\n",
            "iteration 59 :train_loss:0.7784942984580994 val_loss0.7757487297058105\n",
            "iteration 60 :train_loss:0.7768076062202454 val_loss0.7740570306777954\n",
            "iteration 61 :train_loss:0.7751124501228333 val_loss0.7723568677902222\n",
            "iteration 62 :train_loss:0.7734066247940063 val_loss0.7706464529037476\n",
            "iteration 63 :train_loss:0.7716894745826721 val_loss0.7689240574836731\n",
            "iteration 64 :train_loss:0.7699611186981201 val_loss0.7671904563903809\n",
            "iteration 65 :train_loss:0.768221914768219 val_loss0.7654473781585693\n",
            "iteration 66 :train_loss:0.766473114490509 val_loss0.7636942267417908\n",
            "iteration 67 :train_loss:0.7647124528884888 val_loss0.7619287371635437\n",
            "iteration 68 :train_loss:0.762939989566803 val_loss0.7601513862609863\n",
            "iteration 69 :train_loss:0.7611581087112427 val_loss0.7583644986152649\n",
            "iteration 70 :train_loss:0.7593650817871094 val_loss0.7565661668777466\n",
            "iteration 71 :train_loss:0.757560133934021 val_loss0.7547556161880493\n",
            "iteration 72 :train_loss:0.7557427287101746 val_loss0.7529310584068298\n",
            "iteration 73 :train_loss:0.7539114952087402 val_loss0.7510925531387329\n",
            "iteration 74 :train_loss:0.7520648837089539 val_loss0.7492387294769287\n",
            "iteration 75 :train_loss:0.750203013420105 val_loss0.7473691701889038\n",
            "iteration 76 :train_loss:0.7483261823654175 val_loss0.7454849481582642\n",
            "iteration 77 :train_loss:0.7464357018470764 val_loss0.7435874342918396\n",
            "iteration 78 :train_loss:0.7445291876792908 val_loss0.7416757941246033\n",
            "iteration 79 :train_loss:0.742606520652771 val_loss0.7397490739822388\n",
            "iteration 80 :train_loss:0.7406672835350037 val_loss0.7378063201904297\n",
            "iteration 81 :train_loss:0.7387095093727112 val_loss0.7358461022377014\n",
            "iteration 82 :train_loss:0.7367333173751831 val_loss0.7338687777519226\n",
            "iteration 83 :train_loss:0.7347406148910522 val_loss0.7318752408027649\n",
            "iteration 84 :train_loss:0.7327338457107544 val_loss0.7298677563667297\n",
            "iteration 85 :train_loss:0.7307071685791016 val_loss0.72784024477005\n",
            "iteration 86 :train_loss:0.7286598682403564 val_loss0.7257912158966064\n",
            "iteration 87 :train_loss:0.7265907526016235 val_loss0.7237200140953064\n",
            "iteration 88 :train_loss:0.7245030403137207 val_loss0.7216294407844543\n",
            "iteration 89 :train_loss:0.7223946452140808 val_loss0.7195192575454712\n",
            "iteration 90 :train_loss:0.7202689051628113 val_loss0.7173917293548584\n",
            "iteration 91 :train_loss:0.718123197555542 val_loss0.7152447700500488\n",
            "iteration 92 :train_loss:0.7159554958343506 val_loss0.7130769491195679\n",
            "iteration 93 :train_loss:0.7137638926506042 val_loss0.7108863592147827\n",
            "iteration 94 :train_loss:0.7115490436553955 val_loss0.7086725831031799\n",
            "iteration 95 :train_loss:0.7093105912208557 val_loss0.7064349055290222\n",
            "iteration 96 :train_loss:0.7070443034172058 val_loss0.7041676640510559\n",
            "iteration 97 :train_loss:0.7047494053840637 val_loss0.7018709778785706\n",
            "iteration 98 :train_loss:0.7024272084236145 val_loss0.6995454430580139\n",
            "iteration 99 :train_loss:0.7000750303268433 val_loss0.6971898078918457\n",
            "iteration 100 :train_loss:0.6976941227912903 val_loss0.6948052048683167\n",
            "iteration 101 :train_loss:0.6952853202819824 val_loss0.6923928260803223\n",
            "iteration 102 :train_loss:0.6928495168685913 val_loss0.6899552941322327\n",
            "iteration 103 :train_loss:0.6903877258300781 val_loss0.6874915361404419\n",
            "iteration 104 :train_loss:0.6878966093063354 val_loss0.6849976181983948\n",
            "iteration 105 :train_loss:0.6853746175765991 val_loss0.6824730634689331\n",
            "iteration 106 :train_loss:0.6828134655952454 val_loss0.6799100637435913\n",
            "iteration 107 :train_loss:0.6802135705947876 val_loss0.6773077249526978\n",
            "iteration 108 :train_loss:0.6775777339935303 val_loss0.6746701598167419\n",
            "iteration 109 :train_loss:0.6749061942100525 val_loss0.671996533870697\n",
            "iteration 110 :train_loss:0.6721965074539185 val_loss0.6692847609519958\n",
            "iteration 111 :train_loss:0.669457197189331 val_loss0.6665425896644592\n",
            "iteration 112 :train_loss:0.6666825413703918 val_loss0.6637663245201111\n",
            "iteration 113 :train_loss:0.6638692021369934 val_loss0.6609511971473694\n",
            "iteration 114 :train_loss:0.6610205173492432 val_loss0.6581024527549744\n",
            "iteration 115 :train_loss:0.6581353545188904 val_loss0.6552191376686096\n",
            "iteration 116 :train_loss:0.6552145481109619 val_loss0.6523024439811707\n",
            "iteration 117 :train_loss:0.6522597670555115 val_loss0.6493514776229858\n",
            "iteration 118 :train_loss:0.6492655873298645 val_loss0.6463601589202881\n",
            "iteration 119 :train_loss:0.6462298631668091 val_loss0.6433255672454834\n",
            "iteration 120 :train_loss:0.6431511640548706 val_loss0.64024817943573\n",
            "iteration 121 :train_loss:0.6400274634361267 val_loss0.637129545211792\n",
            "iteration 122 :train_loss:0.6368562579154968 val_loss0.6339672207832336\n",
            "iteration 123 :train_loss:0.633634626865387 val_loss0.6307547092437744\n",
            "iteration 124 :train_loss:0.6303657293319702 val_loss0.6274933218955994\n",
            "iteration 125 :train_loss:0.6270590424537659 val_loss0.6241925954818726\n",
            "iteration 126 :train_loss:0.6237125396728516 val_loss0.6208539605140686\n",
            "iteration 127 :train_loss:0.6203265190124512 val_loss0.6174774169921875\n",
            "iteration 128 :train_loss:0.6169018745422363 val_loss0.6140618920326233\n",
            "iteration 129 :train_loss:0.6134396195411682 val_loss0.6106101870536804\n",
            "iteration 130 :train_loss:0.609952986240387 val_loss0.6071364879608154\n",
            "iteration 131 :train_loss:0.6064448952674866 val_loss0.6036399602890015\n",
            "iteration 132 :train_loss:0.6029123067855835 val_loss0.6001189947128296\n",
            "iteration 133 :train_loss:0.599351704120636 val_loss0.5965720415115356\n",
            "iteration 134 :train_loss:0.5957815051078796 val_loss0.5930124521255493\n",
            "iteration 135 :train_loss:0.5921977162361145 val_loss0.589434802532196\n",
            "iteration 136 :train_loss:0.5886004567146301 val_loss0.585841715335846\n",
            "iteration 137 :train_loss:0.5850024819374084 val_loss0.5822470784187317\n",
            "iteration 138 :train_loss:0.5814177989959717 val_loss0.5786700248718262\n",
            "iteration 139 :train_loss:0.5778494477272034 val_loss0.575110137462616\n",
            "iteration 140 :train_loss:0.5743045210838318 val_loss0.5715746283531189\n",
            "iteration 141 :train_loss:0.5707802176475525 val_loss0.5680659413337708\n",
            "iteration 142 :train_loss:0.567283034324646 val_loss0.5645864009857178\n",
            "iteration 143 :train_loss:0.5638188123703003 val_loss0.5611404180526733\n",
            "iteration 144 :train_loss:0.5603870749473572 val_loss0.5577256083488464\n",
            "iteration 145 :train_loss:0.5569933652877808 val_loss0.5543484091758728\n",
            "iteration 146 :train_loss:0.5536342263221741 val_loss0.5510085225105286\n",
            "iteration 147 :train_loss:0.5503069162368774 val_loss0.5476968288421631\n",
            "iteration 148 :train_loss:0.5470104217529297 val_loss0.5444127917289734\n",
            "iteration 149 :train_loss:0.5437426567077637 val_loss0.5411638617515564\n",
            "iteration 150 :train_loss:0.5405101180076599 val_loss0.537949800491333\n",
            "iteration 151 :train_loss:0.537315845489502 val_loss0.5347782969474792\n",
            "iteration 152 :train_loss:0.5341581702232361 val_loss0.5316501259803772\n",
            "iteration 153 :train_loss:0.5310385823249817 val_loss0.528561532497406\n",
            "iteration 154 :train_loss:0.5279461145401001 val_loss0.5255029201507568\n",
            "iteration 155 :train_loss:0.5248791575431824 val_loss0.5224677324295044\n",
            "iteration 156 :train_loss:0.5218390822410583 val_loss0.5194607973098755\n",
            "iteration 157 :train_loss:0.5188155770301819 val_loss0.5164710283279419\n",
            "iteration 158 :train_loss:0.5158061385154724 val_loss0.513494074344635\n",
            "iteration 159 :train_loss:0.5128018260002136 val_loss0.510519802570343\n",
            "iteration 160 :train_loss:0.5097963213920593 val_loss0.5075467228889465\n",
            "iteration 161 :train_loss:0.5067853331565857 val_loss0.5045682787895203\n",
            "iteration 162 :train_loss:0.5037673115730286 val_loss0.5015823841094971\n",
            "iteration 163 :train_loss:0.5007403492927551 val_loss0.4985881447792053\n",
            "iteration 164 :train_loss:0.4977039694786072 val_loss0.495584636926651\n",
            "iteration 165 :train_loss:0.4946582615375519 val_loss0.4925738275051117\n",
            "iteration 166 :train_loss:0.4916023015975952 val_loss0.48955491185188293\n",
            "iteration 167 :train_loss:0.48853549361228943 val_loss0.4865257143974304\n",
            "iteration 168 :train_loss:0.48545828461647034 val_loss0.4834861755371094\n",
            "iteration 169 :train_loss:0.482371062040329 val_loss0.48043760657310486\n",
            "iteration 170 :train_loss:0.47927236557006836 val_loss0.4773784279823303\n",
            "iteration 171 :train_loss:0.47615882754325867 val_loss0.4743059277534485\n",
            "iteration 172 :train_loss:0.47303152084350586 val_loss0.4712196886539459\n",
            "iteration 173 :train_loss:0.46988964080810547 val_loss0.46811801195144653\n",
            "iteration 174 :train_loss:0.4667317271232605 val_loss0.4649989902973175\n",
            "iteration 175 :train_loss:0.46355628967285156 val_loss0.46186286211013794\n",
            "iteration 176 :train_loss:0.4603668749332428 val_loss0.45871424674987793\n",
            "iteration 177 :train_loss:0.4571682810783386 val_loss0.4555555582046509\n",
            "iteration 178 :train_loss:0.45396238565444946 val_loss0.4523921012878418\n",
            "iteration 179 :train_loss:0.450747549533844 val_loss0.4492211937904358\n",
            "iteration 180 :train_loss:0.4475240707397461 val_loss0.4460432827472687\n",
            "iteration 181 :train_loss:0.44429174065589905 val_loss0.4428592026233673\n",
            "iteration 182 :train_loss:0.4410512149333954 val_loss0.43966662883758545\n",
            "iteration 183 :train_loss:0.43780517578125 val_loss0.43646830320358276\n",
            "iteration 184 :train_loss:0.43454864621162415 val_loss0.4332597553730011\n",
            "iteration 185 :train_loss:0.4312826693058014 val_loss0.4300391376018524\n",
            "iteration 186 :train_loss:0.4280069172382355 val_loss0.426809161901474\n",
            "iteration 187 :train_loss:0.4247187077999115 val_loss0.423565149307251\n",
            "iteration 188 :train_loss:0.4214164912700653 val_loss0.42030537128448486\n",
            "iteration 189 :train_loss:0.4180956482887268 val_loss0.41702619194984436\n",
            "iteration 190 :train_loss:0.41475236415863037 val_loss0.41372549533843994\n",
            "iteration 191 :train_loss:0.411387175321579 val_loss0.41040170192718506\n",
            "iteration 192 :train_loss:0.40800321102142334 val_loss0.4070563018321991\n",
            "iteration 193 :train_loss:0.4046019911766052 val_loss0.4036921560764313\n",
            "iteration 194 :train_loss:0.401185542345047 val_loss0.40030938386917114\n",
            "iteration 195 :train_loss:0.3977549374103546 val_loss0.39691197872161865\n",
            "iteration 196 :train_loss:0.3943134844303131 val_loss0.39350461959838867\n",
            "iteration 197 :train_loss:0.3908631503582001 val_loss0.3900872468948364\n",
            "iteration 198 :train_loss:0.38740673661231995 val_loss0.38666337728500366\n",
            "iteration 199 :train_loss:0.3839462995529175 val_loss0.3832332193851471\n",
            "iteration 200 :train_loss:0.3804795444011688 val_loss0.37979668378829956\n",
            "iteration 201 :train_loss:0.37700870633125305 val_loss0.376354843378067\n",
            "iteration 202 :train_loss:0.37353235483169556 val_loss0.37290918827056885\n",
            "iteration 203 :train_loss:0.37005311250686646 val_loss0.3694605529308319\n",
            "iteration 204 :train_loss:0.3665756285190582 val_loss0.3660130500793457\n",
            "iteration 205 :train_loss:0.3630955219268799 val_loss0.3625635504722595\n",
            "iteration 206 :train_loss:0.35961461067199707 val_loss0.3591121733188629\n",
            "iteration 207 :train_loss:0.3561285138130188 val_loss0.3556552529335022\n",
            "iteration 208 :train_loss:0.35263943672180176 val_loss0.35219165682792664\n",
            "iteration 209 :train_loss:0.3491465747356415 val_loss0.34872692823410034\n",
            "iteration 210 :train_loss:0.3456474244594574 val_loss0.34525835514068604\n",
            "iteration 211 :train_loss:0.34214404225349426 val_loss0.34178653359413147\n",
            "iteration 212 :train_loss:0.33863365650177 val_loss0.33831068873405457\n",
            "iteration 213 :train_loss:0.3351196050643921 val_loss0.33483028411865234\n",
            "iteration 214 :train_loss:0.3316061496734619 val_loss0.33134838938713074\n",
            "iteration 215 :train_loss:0.3280971348285675 val_loss0.32786986231803894\n",
            "iteration 216 :train_loss:0.3245982825756073 val_loss0.32440245151519775\n",
            "iteration 217 :train_loss:0.3211125433444977 val_loss0.3209461271762848\n",
            "iteration 218 :train_loss:0.3176450729370117 val_loss0.3175026476383209\n",
            "iteration 219 :train_loss:0.31419774889945984 val_loss0.31407374143600464\n",
            "iteration 220 :train_loss:0.31077417731285095 val_loss0.3106699585914612\n",
            "iteration 221 :train_loss:0.3073733448982239 val_loss0.3072926700115204\n",
            "iteration 222 :train_loss:0.30399635434150696 val_loss0.3039364516735077\n",
            "iteration 223 :train_loss:0.3006468117237091 val_loss0.3006055951118469\n",
            "iteration 224 :train_loss:0.29732683300971985 val_loss0.2973027527332306\n",
            "iteration 225 :train_loss:0.294035941362381 val_loss0.2940274477005005\n",
            "iteration 226 :train_loss:0.29078158736228943 val_loss0.29078978300094604\n",
            "iteration 227 :train_loss:0.28756654262542725 val_loss0.28759321570396423\n",
            "iteration 228 :train_loss:0.28439632058143616 val_loss0.2844386398792267\n",
            "iteration 229 :train_loss:0.28127431869506836 val_loss0.28132933378219604\n",
            "iteration 230 :train_loss:0.2782036364078522 val_loss0.2782677412033081\n",
            "iteration 231 :train_loss:0.27518635988235474 val_loss0.27525460720062256\n",
            "iteration 232 :train_loss:0.2722248435020447 val_loss0.27229073643684387\n",
            "iteration 233 :train_loss:0.26931917667388916 val_loss0.26938188076019287\n",
            "iteration 234 :train_loss:0.2664671838283539 val_loss0.2665235996246338\n",
            "iteration 235 :train_loss:0.26367324590682983 val_loss0.2637263238430023\n",
            "iteration 236 :train_loss:0.26093795895576477 val_loss0.2609863579273224\n",
            "iteration 237 :train_loss:0.2582660913467407 val_loss0.2583075761795044\n",
            "iteration 238 :train_loss:0.2556580603122711 val_loss0.2556925117969513\n",
            "iteration 239 :train_loss:0.25311407446861267 val_loss0.25314316153526306\n",
            "iteration 240 :train_loss:0.2506352663040161 val_loss0.25065967440605164\n",
            "iteration 241 :train_loss:0.24821944534778595 val_loss0.24824300408363342\n",
            "iteration 242 :train_loss:0.24586673080921173 val_loss0.24588914215564728\n",
            "iteration 243 :train_loss:0.2435755729675293 val_loss0.24359814822673798\n",
            "iteration 244 :train_loss:0.24134334921836853 val_loss0.2413688749074936\n",
            "iteration 245 :train_loss:0.239172101020813 val_loss0.2392013520002365\n",
            "iteration 246 :train_loss:0.23705695569515228 val_loss0.23709096014499664\n",
            "iteration 247 :train_loss:0.2349976748228073 val_loss0.2350418120622635\n",
            "iteration 248 :train_loss:0.23298905789852142 val_loss0.23304761946201324\n",
            "iteration 249 :train_loss:0.23103167116641998 val_loss0.23110967874526978\n",
            "iteration 250 :train_loss:0.22912642359733582 val_loss0.229220449924469\n",
            "iteration 251 :train_loss:0.2272777557373047 val_loss0.22738531231880188\n",
            "iteration 252 :train_loss:0.2254798710346222 val_loss0.22559694945812225\n",
            "iteration 253 :train_loss:0.2237357795238495 val_loss0.22385862469673157\n",
            "iteration 254 :train_loss:0.22204358875751495 val_loss0.22216978669166565\n",
            "iteration 255 :train_loss:0.22040235996246338 val_loss0.22052986919879913\n",
            "iteration 256 :train_loss:0.21881262958049774 val_loss0.21893411874771118\n",
            "iteration 257 :train_loss:0.2172745019197464 val_loss0.21738405525684357\n",
            "iteration 258 :train_loss:0.21578428149223328 val_loss0.21587781608104706\n",
            "iteration 259 :train_loss:0.21434246003627777 val_loss0.2144201397895813\n",
            "iteration 260 :train_loss:0.21294352412223816 val_loss0.2130124419927597\n",
            "iteration 261 :train_loss:0.2115897834300995 val_loss0.21165168285369873\n",
            "iteration 262 :train_loss:0.21028687059879303 val_loss0.21034285426139832\n",
            "iteration 263 :train_loss:0.20903338491916656 val_loss0.2090819776058197\n",
            "iteration 264 :train_loss:0.2078295797109604 val_loss0.20787277817726135\n",
            "iteration 265 :train_loss:0.20667512714862823 val_loss0.20671446621418\n",
            "iteration 266 :train_loss:0.20557057857513428 val_loss0.20560522377490997\n",
            "iteration 267 :train_loss:0.20451420545578003 val_loss0.20454329252243042\n",
            "iteration 268 :train_loss:0.20350678265094757 val_loss0.2035265564918518\n",
            "iteration 269 :train_loss:0.20254398882389069 val_loss0.20255540311336517\n",
            "iteration 270 :train_loss:0.20162524282932281 val_loss0.20162387192249298\n",
            "iteration 271 :train_loss:0.20074811577796936 val_loss0.20073561370372772\n",
            "iteration 272 :train_loss:0.1999114602804184 val_loss0.19988936185836792\n",
            "iteration 273 :train_loss:0.19911620020866394 val_loss0.19908443093299866\n",
            "iteration 274 :train_loss:0.19836196303367615 val_loss0.19832037389278412\n",
            "iteration 275 :train_loss:0.19764484465122223 val_loss0.19759303331375122\n",
            "iteration 276 :train_loss:0.19696378707885742 val_loss0.19690321385860443\n",
            "iteration 277 :train_loss:0.19631777703762054 val_loss0.19624945521354675\n",
            "iteration 278 :train_loss:0.19570672512054443 val_loss0.19562967121601105\n",
            "iteration 279 :train_loss:0.19512851536273956 val_loss0.19504337012767792\n",
            "iteration 280 :train_loss:0.19458237290382385 val_loss0.19448843598365784\n",
            "iteration 281 :train_loss:0.19406628608703613 val_loss0.19396397471427917\n",
            "iteration 282 :train_loss:0.1935797482728958 val_loss0.19346711039543152\n",
            "iteration 283 :train_loss:0.19312146306037903 val_loss0.19299767911434174\n",
            "iteration 284 :train_loss:0.19268964231014252 val_loss0.19255484640598297\n",
            "iteration 285 :train_loss:0.19228211045265198 val_loss0.192138209939003\n",
            "iteration 286 :train_loss:0.19189853966236115 val_loss0.19174472987651825\n",
            "iteration 287 :train_loss:0.19153885543346405 val_loss0.19137367606163025\n",
            "iteration 288 :train_loss:0.19120202958583832 val_loss0.1910245269536972\n",
            "iteration 289 :train_loss:0.19088472425937653 val_loss0.190695121884346\n",
            "iteration 290 :train_loss:0.19058653712272644 val_loss0.19038425385951996\n",
            "iteration 291 :train_loss:0.1903073936700821 val_loss0.19009104371070862\n",
            "iteration 292 :train_loss:0.1900457888841629 val_loss0.18981501460075378\n",
            "iteration 293 :train_loss:0.1898011714220047 val_loss0.18955498933792114\n",
            "iteration 294 :train_loss:0.18957144021987915 val_loss0.18931055068969727\n",
            "iteration 295 :train_loss:0.18935626745224 val_loss0.18908080458641052\n",
            "iteration 296 :train_loss:0.1891535073518753 val_loss0.18886345624923706\n",
            "iteration 297 :train_loss:0.1889634132385254 val_loss0.18865966796875\n",
            "iteration 298 :train_loss:0.1887849122285843 val_loss0.18846771121025085\n",
            "iteration 299 :train_loss:0.18861787021160126 val_loss0.18828736245632172\n",
            "iteration 300 :train_loss:0.18846146762371063 val_loss0.18811756372451782\n",
            "iteration 301 :train_loss:0.18831562995910645 val_loss0.18795819580554962\n",
            "iteration 302 :train_loss:0.18817934393882751 val_loss0.1878076195716858\n",
            "iteration 303 :train_loss:0.1880522072315216 val_loss0.18766658008098602\n",
            "iteration 304 :train_loss:0.18793268501758575 val_loss0.18753363192081451\n",
            "iteration 305 :train_loss:0.18782112002372742 val_loss0.1874093860387802\n",
            "iteration 306 :train_loss:0.18771705031394958 val_loss0.1872926652431488\n",
            "iteration 307 :train_loss:0.18761911988258362 val_loss0.18718278408050537\n",
            "iteration 308 :train_loss:0.18752798438072205 val_loss0.18707996606826782\n",
            "iteration 309 :train_loss:0.18744251132011414 val_loss0.18698279559612274\n",
            "iteration 310 :train_loss:0.1873624175786972 val_loss0.186891108751297\n",
            "iteration 311 :train_loss:0.1872875839471817 val_loss0.18680495023727417\n",
            "iteration 312 :train_loss:0.18721705675125122 val_loss0.18672369420528412\n",
            "iteration 313 :train_loss:0.18715111911296844 val_loss0.18664734065532684\n",
            "iteration 314 :train_loss:0.18708907067775726 val_loss0.18657508492469788\n",
            "iteration 315 :train_loss:0.1870308816432953 val_loss0.1865067183971405\n",
            "iteration 316 :train_loss:0.1869763731956482 val_loss0.18644224107265472\n",
            "iteration 317 :train_loss:0.18692514300346375 val_loss0.18638135492801666\n",
            "iteration 318 :train_loss:0.1868768334388733 val_loss0.18632379174232483\n",
            "iteration 319 :train_loss:0.18683163821697235 val_loss0.18626947700977325\n",
            "iteration 320 :train_loss:0.1867891550064087 val_loss0.1862182468175888\n",
            "iteration 321 :train_loss:0.18674899637699127 val_loss0.18616940081119537\n",
            "iteration 322 :train_loss:0.18671146035194397 val_loss0.1861235350370407\n",
            "iteration 323 :train_loss:0.1866760104894638 val_loss0.18608026206493378\n",
            "iteration 324 :train_loss:0.186642587184906 val_loss0.18603943288326263\n",
            "iteration 325 :train_loss:0.18661092221736908 val_loss0.18600068986415863\n",
            "iteration 326 :train_loss:0.18658119440078735 val_loss0.18596412241458893\n",
            "iteration 327 :train_loss:0.18655332922935486 val_loss0.18592971563339233\n",
            "iteration 328 :train_loss:0.1865270584821701 val_loss0.18589694797992706\n",
            "iteration 329 :train_loss:0.1865021288394928 val_loss0.18586575984954834\n",
            "iteration 330 :train_loss:0.1864783614873886 val_loss0.18583588302135468\n",
            "iteration 331 :train_loss:0.18645600974559784 val_loss0.1858077198266983\n",
            "iteration 332 :train_loss:0.18643474578857422 val_loss0.18578089773654938\n",
            "iteration 333 :train_loss:0.18641456961631775 val_loss0.1857554316520691\n",
            "iteration 334 :train_loss:0.18639543652534485 val_loss0.18573112785816193\n",
            "iteration 335 :train_loss:0.18637725710868835 val_loss0.18570789694786072\n",
            "iteration 336 :train_loss:0.1863599568605423 val_loss0.18568561971187592\n",
            "iteration 337 :train_loss:0.18634353578090668 val_loss0.1856643557548523\n",
            "iteration 338 :train_loss:0.1863279491662979 val_loss0.18564411997795105\n",
            "iteration 339 :train_loss:0.18631313741207123 val_loss0.185624897480011\n",
            "iteration 340 :train_loss:0.18629895150661469 val_loss0.18560631573200226\n",
            "iteration 341 :train_loss:0.18628545105457306 val_loss0.1855885535478592\n",
            "iteration 342 :train_loss:0.186272531747818 val_loss0.18557143211364746\n",
            "iteration 343 :train_loss:0.18626020848751068 val_loss0.18555516004562378\n",
            "iteration 344 :train_loss:0.18624848127365112 val_loss0.18553954362869263\n",
            "iteration 345 :train_loss:0.18623721599578857 val_loss0.18552453815937042\n",
            "iteration 346 :train_loss:0.18622639775276184 val_loss0.18551011383533478\n",
            "iteration 347 :train_loss:0.18621598184108734 val_loss0.1854962557554245\n",
            "iteration 348 :train_loss:0.18620599806308746 val_loss0.1854829490184784\n",
            "iteration 349 :train_loss:0.18619641661643982 val_loss0.18547013401985168\n",
            "iteration 350 :train_loss:0.1861872673034668 val_loss0.18545788526535034\n",
            "iteration 351 :train_loss:0.18617849051952362 val_loss0.18544606864452362\n",
            "iteration 352 :train_loss:0.18617002665996552 val_loss0.18543462455272675\n",
            "iteration 353 :train_loss:0.18616187572479248 val_loss0.1854235678911209\n",
            "iteration 354 :train_loss:0.18615399301052094 val_loss0.18541289865970612\n",
            "iteration 355 :train_loss:0.1861463487148285 val_loss0.18540263175964355\n",
            "iteration 356 :train_loss:0.18613901734352112 val_loss0.18539270758628845\n",
            "iteration 357 :train_loss:0.18613190948963165 val_loss0.18538309633731842\n",
            "iteration 358 :train_loss:0.1861250102519989 val_loss0.1853737235069275\n",
            "iteration 359 :train_loss:0.18611828982830048 val_loss0.18536457419395447\n",
            "iteration 360 :train_loss:0.18611182272434235 val_loss0.1853557825088501\n",
            "iteration 361 :train_loss:0.18610557913780212 val_loss0.185347318649292\n",
            "iteration 362 :train_loss:0.18609949946403503 val_loss0.18533912301063538\n",
            "iteration 363 :train_loss:0.18609359860420227 val_loss0.18533118069171906\n",
            "iteration 364 :train_loss:0.18608787655830383 val_loss0.18532346189022064\n",
            "iteration 365 :train_loss:0.18608225882053375 val_loss0.18531590700149536\n",
            "iteration 366 :train_loss:0.18607677519321442 val_loss0.1853085607290268\n",
            "iteration 367 :train_loss:0.1860714554786682 val_loss0.18530137836933136\n",
            "iteration 368 :train_loss:0.18606631457805634 val_loss0.18529443442821503\n",
            "iteration 369 :train_loss:0.18606127798557281 val_loss0.18528759479522705\n",
            "iteration 370 :train_loss:0.18605627119541168 val_loss0.1852809488773346\n",
            "iteration 371 :train_loss:0.18605145812034607 val_loss0.18527443706989288\n",
            "iteration 372 :train_loss:0.18604667484760284 val_loss0.1852680742740631\n",
            "iteration 373 :train_loss:0.18604204058647156 val_loss0.18526190519332886\n",
            "iteration 374 :train_loss:0.18603748083114624 val_loss0.18525581061840057\n",
            "iteration 375 :train_loss:0.1860329806804657 val_loss0.18524986505508423\n",
            "iteration 376 :train_loss:0.18602856993675232 val_loss0.18524396419525146\n",
            "iteration 377 :train_loss:0.18602421879768372 val_loss0.18523818254470825\n",
            "iteration 378 :train_loss:0.1860199272632599 val_loss0.1852324903011322\n",
            "iteration 379 :train_loss:0.1860157698392868 val_loss0.18522688746452332\n",
            "iteration 380 :train_loss:0.1860116571187973 val_loss0.18522143363952637\n",
            "iteration 381 :train_loss:0.18600763380527496 val_loss0.18521608412265778\n",
            "iteration 382 :train_loss:0.1860036700963974 val_loss0.18521086871623993\n",
            "iteration 383 :train_loss:0.1859997808933258 val_loss0.18520571291446686\n",
            "iteration 384 :train_loss:0.185995951294899 val_loss0.18520070612430573\n",
            "iteration 385 :train_loss:0.18599218130111694 val_loss0.18519577383995056\n",
            "iteration 386 :train_loss:0.1859884411096573 val_loss0.18519094586372375\n",
            "iteration 387 :train_loss:0.1859847754240036 val_loss0.18518619239330292\n",
            "iteration 388 :train_loss:0.1859811544418335 val_loss0.18518149852752686\n",
            "iteration 389 :train_loss:0.18597754836082458 val_loss0.18517690896987915\n",
            "iteration 390 :train_loss:0.18597400188446045 val_loss0.18517237901687622\n",
            "iteration 391 :train_loss:0.1859704554080963 val_loss0.18516789376735687\n",
            "iteration 392 :train_loss:0.18596699833869934 val_loss0.1851634830236435\n",
            "iteration 393 :train_loss:0.18596354126930237 val_loss0.1851591169834137\n",
            "iteration 394 :train_loss:0.18596011400222778 val_loss0.18515481054782867\n",
            "iteration 395 :train_loss:0.18595671653747559 val_loss0.18515056371688843\n",
            "iteration 396 :train_loss:0.18595336377620697 val_loss0.18514633178710938\n",
            "iteration 397 :train_loss:0.18595004081726074 val_loss0.1851421743631363\n",
            "iteration 398 :train_loss:0.18594670295715332 val_loss0.1851380318403244\n",
            "iteration 399 :train_loss:0.18594345450401306 val_loss0.18513396382331848\n",
            "iteration 400 :train_loss:0.1859402060508728 val_loss0.18512994050979614\n",
            "iteration 401 :train_loss:0.18593698740005493 val_loss0.18512596189975739\n",
            "iteration 402 :train_loss:0.18593379855155945 val_loss0.1851220577955246\n",
            "iteration 403 :train_loss:0.18593063950538635 val_loss0.18511825799942017\n",
            "iteration 404 :train_loss:0.18592751026153564 val_loss0.18511444330215454\n",
            "iteration 405 :train_loss:0.18592438101768494 val_loss0.18511070311069489\n",
            "iteration 406 :train_loss:0.1859212964773178 val_loss0.18510699272155762\n",
            "iteration 407 :train_loss:0.1859181821346283 val_loss0.18510332703590393\n",
            "iteration 408 :train_loss:0.18591511249542236 val_loss0.18509969115257263\n",
            "iteration 409 :train_loss:0.18591205775737762 val_loss0.18509607017040253\n",
            "iteration 410 :train_loss:0.18590900301933289 val_loss0.1850925087928772\n",
            "iteration 411 :train_loss:0.18590599298477173 val_loss0.18508896231651306\n",
            "iteration 412 :train_loss:0.18590295314788818 val_loss0.18508543074131012\n",
            "iteration 413 :train_loss:0.18589994311332703 val_loss0.18508194386959076\n",
            "iteration 414 :train_loss:0.18589694797992706 val_loss0.18507850170135498\n",
            "iteration 415 :train_loss:0.1858939826488495 val_loss0.185075044631958\n",
            "iteration 416 :train_loss:0.1858910322189331 val_loss0.18507161736488342\n",
            "iteration 417 :train_loss:0.18588806688785553 val_loss0.18506823480129242\n",
            "iteration 418 :train_loss:0.18588513135910034 val_loss0.1850648671388626\n",
            "iteration 419 :train_loss:0.18588221073150635 val_loss0.1850615292787552\n",
            "iteration 420 :train_loss:0.18587931990623474 val_loss0.18505819141864777\n",
            "iteration 421 :train_loss:0.18587642908096313 val_loss0.18505491316318512\n",
            "iteration 422 :train_loss:0.18587353825569153 val_loss0.18505162000656128\n",
            "iteration 423 :train_loss:0.18587066233158112 val_loss0.18504834175109863\n",
            "iteration 424 :train_loss:0.18586783111095428 val_loss0.18504515290260315\n",
            "iteration 425 :train_loss:0.18586495518684387 val_loss0.18504196405410767\n",
            "iteration 426 :train_loss:0.18586210906505585 val_loss0.185038760304451\n",
            "iteration 427 :train_loss:0.1858593076467514 val_loss0.1850355714559555\n",
            "iteration 428 :train_loss:0.18585646152496338 val_loss0.1850324273109436\n",
            "iteration 429 :train_loss:0.18585366010665894 val_loss0.1850292682647705\n",
            "iteration 430 :train_loss:0.1858508437871933 val_loss0.185026153922081\n",
            "iteration 431 :train_loss:0.18584805727005005 val_loss0.18502302467823029\n",
            "iteration 432 :train_loss:0.1858452558517456 val_loss0.18501994013786316\n",
            "iteration 433 :train_loss:0.18584246933460236 val_loss0.18501687049865723\n",
            "iteration 434 :train_loss:0.1858397126197815 val_loss0.1850137859582901\n",
            "iteration 435 :train_loss:0.18583694100379944 val_loss0.18501073122024536\n",
            "iteration 436 :train_loss:0.1858341544866562 val_loss0.185007706284523\n",
            "iteration 437 :train_loss:0.18583138287067413 val_loss0.18500463664531708\n",
            "iteration 438 :train_loss:0.18582864105701447 val_loss0.18500162661075592\n",
            "iteration 439 :train_loss:0.185825914144516 val_loss0.18499861657619476\n",
            "iteration 440 :train_loss:0.18582315742969513 val_loss0.184995636343956\n",
            "iteration 441 :train_loss:0.18582043051719666 val_loss0.18499267101287842\n",
            "iteration 442 :train_loss:0.18581770360469818 val_loss0.18498970568180084\n",
            "iteration 443 :train_loss:0.1858149766921997 val_loss0.18498674035072327\n",
            "iteration 444 :train_loss:0.18581224977970123 val_loss0.18498381972312927\n",
            "iteration 445 :train_loss:0.18580952286720276 val_loss0.18498091399669647\n",
            "iteration 446 :train_loss:0.18580682575702667 val_loss0.18497799336910248\n",
            "iteration 447 :train_loss:0.1858041137456894 val_loss0.18497510254383087\n",
            "iteration 448 :train_loss:0.1858014166355133 val_loss0.18497218191623688\n",
            "iteration 449 :train_loss:0.18579870462417603 val_loss0.18496930599212646\n",
            "iteration 450 :train_loss:0.18579602241516113 val_loss0.18496644496917725\n",
            "iteration 451 :train_loss:0.18579329550266266 val_loss0.18496356904506683\n",
            "iteration 452 :train_loss:0.18579061329364777 val_loss0.18496069312095642\n",
            "iteration 453 :train_loss:0.18578793108463287 val_loss0.184957817196846\n",
            "iteration 454 :train_loss:0.18578524887561798 val_loss0.1849549412727356\n",
            "iteration 455 :train_loss:0.1857825666666031 val_loss0.18495208024978638\n",
            "iteration 456 :train_loss:0.1857798993587494 val_loss0.18494921922683716\n",
            "iteration 457 :train_loss:0.1857772171497345 val_loss0.18494634330272675\n",
            "iteration 458 :train_loss:0.1857745349407196 val_loss0.18494349718093872\n",
            "iteration 459 :train_loss:0.1857718676328659 val_loss0.1849406510591507\n",
            "iteration 460 :train_loss:0.1857692003250122 val_loss0.18493781983852386\n",
            "iteration 461 :train_loss:0.18576651811599731 val_loss0.18493495881557465\n",
            "iteration 462 :train_loss:0.185763880610466 val_loss0.184932142496109\n",
            "iteration 463 :train_loss:0.1857612133026123 val_loss0.18492929637432098\n",
            "iteration 464 :train_loss:0.1857585459947586 val_loss0.18492649495601654\n",
            "iteration 465 :train_loss:0.1857558786869049 val_loss0.1849236786365509\n",
            "iteration 466 :train_loss:0.1857532411813736 val_loss0.18492089211940765\n",
            "iteration 467 :train_loss:0.18575060367584229 val_loss0.1849181056022644\n",
            "iteration 468 :train_loss:0.18574795126914978 val_loss0.18491531908512115\n",
            "iteration 469 :train_loss:0.18574531376361847 val_loss0.1849125474691391\n",
            "iteration 470 :train_loss:0.18574266135692596 val_loss0.18490980565547943\n",
            "iteration 471 :train_loss:0.18574003875255585 val_loss0.18490703403949738\n",
            "iteration 472 :train_loss:0.18573740124702454 val_loss0.1849043071269989\n",
            "iteration 473 :train_loss:0.18573477864265442 val_loss0.18490153551101685\n",
            "iteration 474 :train_loss:0.1857321560382843 val_loss0.18489883840084076\n",
            "iteration 475 :train_loss:0.18572953343391418 val_loss0.18489611148834229\n",
            "iteration 476 :train_loss:0.18572691082954407 val_loss0.184893399477005\n",
            "iteration 477 :train_loss:0.18572428822517395 val_loss0.18489070236682892\n",
            "iteration 478 :train_loss:0.18572168052196503 val_loss0.18488799035549164\n",
            "iteration 479 :train_loss:0.1857190579175949 val_loss0.18488530814647675\n",
            "iteration 480 :train_loss:0.18571648001670837 val_loss0.18488258123397827\n",
            "iteration 481 :train_loss:0.18571387231349945 val_loss0.18487989902496338\n",
            "iteration 482 :train_loss:0.18571123480796814 val_loss0.1848772019147873\n",
            "iteration 483 :train_loss:0.1857086420059204 val_loss0.1848745197057724\n",
            "iteration 484 :train_loss:0.18570604920387268 val_loss0.1848718374967575\n",
            "iteration 485 :train_loss:0.18570342659950256 val_loss0.18486911058425903\n",
            "iteration 486 :train_loss:0.18570081889629364 val_loss0.18486644327640533\n",
            "iteration 487 :train_loss:0.1856982409954071 val_loss0.18486376106739044\n",
            "iteration 488 :train_loss:0.18569564819335938 val_loss0.18486107885837555\n",
            "iteration 489 :train_loss:0.18569305539131165 val_loss0.18485839664936066\n",
            "iteration 490 :train_loss:0.18569046258926392 val_loss0.18485572934150696\n",
            "iteration 491 :train_loss:0.18568788468837738 val_loss0.18485306203365326\n",
            "iteration 492 :train_loss:0.18568527698516846 val_loss0.18485040962696075\n",
            "iteration 493 :train_loss:0.1856827288866043 val_loss0.18484775722026825\n",
            "iteration 494 :train_loss:0.18568013608455658 val_loss0.18484511971473694\n",
            "iteration 495 :train_loss:0.18567755818367004 val_loss0.18484245240688324\n",
            "iteration 496 :train_loss:0.1856749951839447 val_loss0.18483981490135193\n",
            "iteration 497 :train_loss:0.18567243218421936 val_loss0.18483716249465942\n",
            "iteration 498 :train_loss:0.18566985428333282 val_loss0.18483451008796692\n",
            "iteration 499 :train_loss:0.18566729128360748 val_loss0.18483184278011322\n",
            "iteration 500 :train_loss:0.18566472828388214 val_loss0.1848292052745819\n",
            "iteration 501 :train_loss:0.1856621652841568 val_loss0.1848265379667282\n",
            "iteration 502 :train_loss:0.18565961718559265 val_loss0.1848239153623581\n",
            "iteration 503 :train_loss:0.1856570541858673 val_loss0.1848212629556656\n",
            "iteration 504 :train_loss:0.18565449118614197 val_loss0.18481862545013428\n",
            "iteration 505 :train_loss:0.18565192818641663 val_loss0.18481600284576416\n",
            "iteration 506 :train_loss:0.18564939498901367 val_loss0.18481336534023285\n",
            "iteration 507 :train_loss:0.18564684689044952 val_loss0.18481072783470154\n",
            "iteration 508 :train_loss:0.18564428389072418 val_loss0.18480809032917023\n",
            "iteration 509 :train_loss:0.18564172089099884 val_loss0.1848054975271225\n",
            "iteration 510 :train_loss:0.18563920259475708 val_loss0.18480287492275238\n",
            "iteration 511 :train_loss:0.18563666939735413 val_loss0.18480025231838226\n",
            "iteration 512 :train_loss:0.18563410639762878 val_loss0.18479765951633453\n",
            "iteration 513 :train_loss:0.18563157320022583 val_loss0.1847950518131256\n",
            "iteration 514 :train_loss:0.18562904000282288 val_loss0.1847924441099167\n",
            "iteration 515 :train_loss:0.18562649190425873 val_loss0.18478982150554657\n",
            "iteration 516 :train_loss:0.18562395870685577 val_loss0.18478724360466003\n",
            "iteration 517 :train_loss:0.18562142550945282 val_loss0.1847846359014511\n",
            "iteration 518 :train_loss:0.18561890721321106 val_loss0.18478204309940338\n",
            "iteration 519 :train_loss:0.1856163889169693 val_loss0.18477946519851685\n",
            "iteration 520 :train_loss:0.18561387062072754 val_loss0.18477685749530792\n",
            "iteration 521 :train_loss:0.18561135232448578 val_loss0.18477429449558258\n",
            "iteration 522 :train_loss:0.18560881912708282 val_loss0.18477171659469604\n",
            "iteration 523 :train_loss:0.18560630083084106 val_loss0.1847691535949707\n",
            "iteration 524 :train_loss:0.1856037974357605 val_loss0.18476656079292297\n",
            "iteration 525 :train_loss:0.18560129404067993 val_loss0.18476398289203644\n",
            "iteration 526 :train_loss:0.18559877574443817 val_loss0.1847614049911499\n",
            "iteration 527 :train_loss:0.1855962574481964 val_loss0.18475884199142456\n",
            "iteration 528 :train_loss:0.18559373915195465 val_loss0.18475624918937683\n",
            "iteration 529 :train_loss:0.18559125065803528 val_loss0.18475370109081268\n",
            "iteration 530 :train_loss:0.18558873236179352 val_loss0.18475113809108734\n",
            "iteration 531 :train_loss:0.18558625876903534 val_loss0.1847485899925232\n",
            "iteration 532 :train_loss:0.18558375537395477 val_loss0.18474601209163666\n",
            "iteration 533 :train_loss:0.1855812519788742 val_loss0.1847434937953949\n",
            "iteration 534 :train_loss:0.18557874858379364 val_loss0.18474094569683075\n",
            "iteration 535 :train_loss:0.18557624518871307 val_loss0.184738427400589\n",
            "iteration 536 :train_loss:0.1855737715959549 val_loss0.18473589420318604\n",
            "iteration 537 :train_loss:0.18557128310203552 val_loss0.18473336100578308\n",
            "iteration 538 :train_loss:0.18556876480579376 val_loss0.18473082780838013\n",
            "iteration 539 :train_loss:0.18556629121303558 val_loss0.18472826480865479\n",
            "iteration 540 :train_loss:0.1855638027191162 val_loss0.18472574651241302\n",
            "iteration 541 :train_loss:0.18556132912635803 val_loss0.18472319841384888\n",
            "iteration 542 :train_loss:0.18555882573127747 val_loss0.18472066521644592\n",
            "iteration 543 :train_loss:0.1855563372373581 val_loss0.18471813201904297\n",
            "iteration 544 :train_loss:0.18555383384227753 val_loss0.18471559882164001\n",
            "iteration 545 :train_loss:0.18555136024951935 val_loss0.18471309542655945\n",
            "iteration 546 :train_loss:0.18554887175559998 val_loss0.1847105622291565\n",
            "iteration 547 :train_loss:0.1855463981628418 val_loss0.18470804393291473\n",
            "iteration 548 :train_loss:0.1855439394712448 val_loss0.18470554053783417\n",
            "iteration 549 :train_loss:0.18554145097732544 val_loss0.1847030371427536\n",
            "iteration 550 :train_loss:0.18553899228572845 val_loss0.18470051884651184\n",
            "iteration 551 :train_loss:0.18553651869297028 val_loss0.18469803035259247\n",
            "iteration 552 :train_loss:0.1855340301990509 val_loss0.1846955269575119\n",
            "iteration 553 :train_loss:0.18553157150745392 val_loss0.18469303846359253\n",
            "iteration 554 :train_loss:0.18552911281585693 val_loss0.18469052016735077\n",
            "iteration 555 :train_loss:0.18552665412425995 val_loss0.1846880465745926\n",
            "iteration 556 :train_loss:0.18552419543266296 val_loss0.18468554317951202\n",
            "iteration 557 :train_loss:0.18552173674106598 val_loss0.18468302488327026\n",
            "iteration 558 :train_loss:0.1855192631483078 val_loss0.1846805214881897\n",
            "iteration 559 :train_loss:0.18551680445671082 val_loss0.18467804789543152\n",
            "iteration 560 :train_loss:0.18551434576511383 val_loss0.18467554450035095\n",
            "iteration 561 :train_loss:0.18551190197467804 val_loss0.18467307090759277\n",
            "iteration 562 :train_loss:0.18550944328308105 val_loss0.1846705973148346\n",
            "iteration 563 :train_loss:0.18550698459148407 val_loss0.18466812372207642\n",
            "iteration 564 :train_loss:0.18550454080104828 val_loss0.18466565012931824\n",
            "iteration 565 :train_loss:0.18550211191177368 val_loss0.18466320633888245\n",
            "iteration 566 :train_loss:0.1854996681213379 val_loss0.18466073274612427\n",
            "iteration 567 :train_loss:0.1854972243309021 val_loss0.1846582591533661\n",
            "iteration 568 :train_loss:0.18549476563930511 val_loss0.1846558004617691\n",
            "iteration 569 :train_loss:0.1854923516511917 val_loss0.1846533566713333\n",
            "iteration 570 :train_loss:0.18548990786075592 val_loss0.18465091288089752\n",
            "iteration 571 :train_loss:0.18548746407032013 val_loss0.18464843928813934\n",
            "iteration 572 :train_loss:0.18548502027988434 val_loss0.18464601039886475\n",
            "iteration 573 :train_loss:0.18548260629177094 val_loss0.18464356660842896\n",
            "iteration 574 :train_loss:0.18548016250133514 val_loss0.18464112281799316\n",
            "iteration 575 :train_loss:0.18547773361206055 val_loss0.18463866412639618\n",
            "iteration 576 :train_loss:0.18547530472278595 val_loss0.18463623523712158\n",
            "iteration 577 :train_loss:0.18547290563583374 val_loss0.1846337914466858\n",
            "iteration 578 :train_loss:0.18547046184539795 val_loss0.1846313327550888\n",
            "iteration 579 :train_loss:0.18546803295612335 val_loss0.18462888896465302\n",
            "iteration 580 :train_loss:0.18546563386917114 val_loss0.1846264749765396\n",
            "iteration 581 :train_loss:0.18546319007873535 val_loss0.18462403118610382\n",
            "iteration 582 :train_loss:0.18546079099178314 val_loss0.18462160229682922\n",
            "iteration 583 :train_loss:0.18545836210250854 val_loss0.18461915850639343\n",
            "iteration 584 :train_loss:0.18545594811439514 val_loss0.18461672961711884\n",
            "iteration 585 :train_loss:0.18545354902744293 val_loss0.18461427092552185\n",
            "iteration 586 :train_loss:0.18545114994049072 val_loss0.18461182713508606\n",
            "iteration 587 :train_loss:0.1854487508535385 val_loss0.18460936844348907\n",
            "iteration 588 :train_loss:0.1854463517665863 val_loss0.18460696935653687\n",
            "iteration 589 :train_loss:0.1854439526796341 val_loss0.18460452556610107\n",
            "iteration 590 :train_loss:0.18544155359268188 val_loss0.18460208177566528\n",
            "iteration 591 :train_loss:0.18543915450572968 val_loss0.1845996379852295\n",
            "iteration 592 :train_loss:0.18543675541877747 val_loss0.18459723889827728\n",
            "iteration 593 :train_loss:0.18543435633182526 val_loss0.18459482491016388\n",
            "iteration 594 :train_loss:0.18543195724487305 val_loss0.18459242582321167\n",
            "iteration 595 :train_loss:0.18542958796024323 val_loss0.18459001183509827\n",
            "iteration 596 :train_loss:0.18542718887329102 val_loss0.18458764255046844\n",
            "iteration 597 :train_loss:0.1854248195886612 val_loss0.18458522856235504\n",
            "iteration 598 :train_loss:0.18542243540287018 val_loss0.18458282947540283\n",
            "iteration 599 :train_loss:0.18542003631591797 val_loss0.18458043038845062\n",
            "iteration 600 :train_loss:0.18541768193244934 val_loss0.18457801640033722\n",
            "iteration 601 :train_loss:0.18541528284549713 val_loss0.184575617313385\n",
            "iteration 602 :train_loss:0.1854129135608673 val_loss0.1845732033252716\n",
            "iteration 603 :train_loss:0.1854105442762375 val_loss0.1845708042383194\n",
            "iteration 604 :train_loss:0.18540816009044647 val_loss0.184568390250206\n",
            "iteration 605 :train_loss:0.18540577590465546 val_loss0.18456600606441498\n",
            "iteration 606 :train_loss:0.18540340662002563 val_loss0.18456362187862396\n",
            "iteration 607 :train_loss:0.1854010373353958 val_loss0.18456122279167175\n",
            "iteration 608 :train_loss:0.18539868295192719 val_loss0.18455882370471954\n",
            "iteration 609 :train_loss:0.18539629876613617 val_loss0.18455643951892853\n",
            "iteration 610 :train_loss:0.18539394438266754 val_loss0.18455404043197632\n",
            "iteration 611 :train_loss:0.1853915899991989 val_loss0.1845516562461853\n",
            "iteration 612 :train_loss:0.1853892058134079 val_loss0.1845492422580719\n",
            "iteration 613 :train_loss:0.18538685142993927 val_loss0.18454685807228088\n",
            "iteration 614 :train_loss:0.18538449704647064 val_loss0.18454447388648987\n",
            "iteration 615 :train_loss:0.18538212776184082 val_loss0.18454208970069885\n",
            "iteration 616 :train_loss:0.1853797733783722 val_loss0.18453972041606903\n",
            "iteration 617 :train_loss:0.18537741899490356 val_loss0.18453733623027802\n",
            "iteration 618 :train_loss:0.18537504971027374 val_loss0.1845349371433258\n",
            "iteration 619 :train_loss:0.18537269532680511 val_loss0.1845325529575348\n",
            "iteration 620 :train_loss:0.1853703409433365 val_loss0.18453015387058258\n",
            "iteration 621 :train_loss:0.18536798655986786 val_loss0.18452773988246918\n",
            "iteration 622 :train_loss:0.18536564707756042 val_loss0.18452535569667816\n",
            "iteration 623 :train_loss:0.185363307595253 val_loss0.18452295660972595\n",
            "iteration 624 :train_loss:0.18536093831062317 val_loss0.18452057242393494\n",
            "iteration 625 :train_loss:0.18535859882831573 val_loss0.18451817333698273\n",
            "iteration 626 :train_loss:0.1853562444448471 val_loss0.18451577425003052\n",
            "iteration 627 :train_loss:0.18535390496253967 val_loss0.1845133900642395\n",
            "iteration 628 :train_loss:0.18535155057907104 val_loss0.1845109909772873\n",
            "iteration 629 :train_loss:0.18534919619560242 val_loss0.18450859189033508\n",
            "iteration 630 :train_loss:0.18534687161445618 val_loss0.18450620770454407\n",
            "iteration 631 :train_loss:0.18534451723098755 val_loss0.18450382351875305\n",
            "iteration 632 :train_loss:0.1853421926498413 val_loss0.18450143933296204\n",
            "iteration 633 :train_loss:0.18533985316753387 val_loss0.18449907004833221\n",
            "iteration 634 :train_loss:0.18533754348754883 val_loss0.1844967007637024\n",
            "iteration 635 :train_loss:0.1853352040052414 val_loss0.18449434638023376\n",
            "iteration 636 :train_loss:0.18533286452293396 val_loss0.18449197709560394\n",
            "iteration 637 :train_loss:0.18533052504062653 val_loss0.18448960781097412\n",
            "iteration 638 :train_loss:0.18532820045948029 val_loss0.1844872534275055\n",
            "iteration 639 :train_loss:0.18532589077949524 val_loss0.18448488414287567\n",
            "iteration 640 :train_loss:0.1853235512971878 val_loss0.18448254466056824\n",
            "iteration 641 :train_loss:0.18532122671604156 val_loss0.1844802051782608\n",
            "iteration 642 :train_loss:0.18531888723373413 val_loss0.18447785079479218\n",
            "iteration 643 :train_loss:0.18531657755374908 val_loss0.18447548151016235\n",
            "iteration 644 :train_loss:0.18531425297260284 val_loss0.18447312712669373\n",
            "iteration 645 :train_loss:0.1853119432926178 val_loss0.1844707727432251\n",
            "iteration 646 :train_loss:0.18530961871147156 val_loss0.18446841835975647\n",
            "iteration 647 :train_loss:0.18530729413032532 val_loss0.18446604907512665\n",
            "iteration 648 :train_loss:0.18530498445034027 val_loss0.18446367979049683\n",
            "iteration 649 :train_loss:0.18530265986919403 val_loss0.184461310505867\n",
            "iteration 650 :train_loss:0.18530035018920898 val_loss0.18445895612239838\n",
            "iteration 651 :train_loss:0.18529802560806274 val_loss0.18445657193660736\n",
            "iteration 652 :train_loss:0.1852957159280777 val_loss0.18445421755313873\n",
            "iteration 653 :train_loss:0.18529339134693146 val_loss0.18445183336734772\n",
            "iteration 654 :train_loss:0.18529106676578522 val_loss0.1844494640827179\n",
            "iteration 655 :train_loss:0.18528880178928375 val_loss0.18444709479808807\n",
            "iteration 656 :train_loss:0.18528646230697632 val_loss0.18444472551345825\n",
            "iteration 657 :train_loss:0.18528415262699127 val_loss0.18444237112998962\n",
            "iteration 658 :train_loss:0.18528184294700623 val_loss0.1844400316476822\n",
            "iteration 659 :train_loss:0.18527954816818237 val_loss0.18443766236305237\n",
            "iteration 660 :train_loss:0.18527725338935852 val_loss0.18443529307842255\n",
            "iteration 661 :train_loss:0.18527492880821228 val_loss0.18443293869495392\n",
            "iteration 662 :train_loss:0.18527263402938843 val_loss0.1844305545091629\n",
            "iteration 663 :train_loss:0.18527033925056458 val_loss0.18442821502685547\n",
            "iteration 664 :train_loss:0.18526802957057953 val_loss0.18442587554454803\n",
            "iteration 665 :train_loss:0.18526573479175568 val_loss0.1844235062599182\n",
            "iteration 666 :train_loss:0.18526344001293182 val_loss0.18442115187644958\n",
            "iteration 667 :train_loss:0.18526114523410797 val_loss0.18441881239414215\n",
            "iteration 668 :train_loss:0.1852588653564453 val_loss0.1844164878129959\n",
            "iteration 669 :train_loss:0.18525657057762146 val_loss0.18441416323184967\n",
            "iteration 670 :train_loss:0.1852542608976364 val_loss0.18441185355186462\n",
            "iteration 671 :train_loss:0.18525199592113495 val_loss0.18440952897071838\n",
            "iteration 672 :train_loss:0.1852497160434723 val_loss0.18440721929073334\n",
            "iteration 673 :train_loss:0.18524742126464844 val_loss0.1844048798084259\n",
            "iteration 674 :train_loss:0.18524514138698578 val_loss0.18440257012844086\n",
            "iteration 675 :train_loss:0.18524287641048431 val_loss0.1844002604484558\n",
            "iteration 676 :train_loss:0.18524059653282166 val_loss0.18439795076847076\n",
            "iteration 677 :train_loss:0.1852383315563202 val_loss0.1843956857919693\n",
            "iteration 678 :train_loss:0.18523605167865753 val_loss0.18439334630966187\n",
            "iteration 679 :train_loss:0.18523377180099487 val_loss0.1843910962343216\n",
            "iteration 680 :train_loss:0.18523147702217102 val_loss0.18438878655433655\n",
            "iteration 681 :train_loss:0.18522919714450836 val_loss0.1843864768743515\n",
            "iteration 682 :train_loss:0.1852269321680069 val_loss0.18438419699668884\n",
            "iteration 683 :train_loss:0.18522466719150543 val_loss0.18438191711902618\n",
            "iteration 684 :train_loss:0.18522238731384277 val_loss0.18437963724136353\n",
            "iteration 685 :train_loss:0.1852201223373413 val_loss0.18437735736370087\n",
            "iteration 686 :train_loss:0.18521787226200104 val_loss0.1843750774860382\n",
            "iteration 687 :train_loss:0.18521559238433838 val_loss0.18437276780605316\n",
            "iteration 688 :train_loss:0.18521331250667572 val_loss0.1843705028295517\n",
            "iteration 689 :train_loss:0.18521107733249664 val_loss0.18436822295188904\n",
            "iteration 690 :train_loss:0.18520884215831757 val_loss0.18436595797538757\n",
            "iteration 691 :train_loss:0.1852065622806549 val_loss0.1843636929988861\n",
            "iteration 692 :train_loss:0.18520431220531464 val_loss0.18436139822006226\n",
            "iteration 693 :train_loss:0.18520204722881317 val_loss0.18435914814472198\n",
            "iteration 694 :train_loss:0.1851998120546341 val_loss0.18435686826705933\n",
            "iteration 695 :train_loss:0.18519753217697144 val_loss0.18435460329055786\n",
            "iteration 696 :train_loss:0.18519529700279236 val_loss0.1843523383140564\n",
            "iteration 697 :train_loss:0.1851930469274521 val_loss0.18435005843639374\n",
            "iteration 698 :train_loss:0.18519079685211182 val_loss0.18434779345989227\n",
            "iteration 699 :train_loss:0.18518854677677155 val_loss0.18434551358222961\n",
            "iteration 700 :train_loss:0.18518631160259247 val_loss0.18434324860572815\n",
            "iteration 701 :train_loss:0.1851840764284134 val_loss0.1843409687280655\n",
            "iteration 702 :train_loss:0.18518184125423431 val_loss0.18433868885040283\n",
            "iteration 703 :train_loss:0.18517959117889404 val_loss0.18433643877506256\n",
            "iteration 704 :train_loss:0.18517735600471497 val_loss0.1843341588973999\n",
            "iteration 705 :train_loss:0.1851751208305359 val_loss0.18433187901973724\n",
            "iteration 706 :train_loss:0.1851728856563568 val_loss0.18432961404323578\n",
            "iteration 707 :train_loss:0.18517065048217773 val_loss0.18432733416557312\n",
            "iteration 708 :train_loss:0.18516841530799866 val_loss0.18432509899139404\n",
            "iteration 709 :train_loss:0.18516622483730316 val_loss0.18432283401489258\n",
            "iteration 710 :train_loss:0.1851639598608017 val_loss0.18432055413722992\n",
            "iteration 711 :train_loss:0.1851617395877838 val_loss0.18431830406188965\n",
            "iteration 712 :train_loss:0.18515950441360474 val_loss0.18431606888771057\n",
            "iteration 713 :train_loss:0.18515728414058685 val_loss0.1843137890100479\n",
            "iteration 714 :train_loss:0.18515504896640778 val_loss0.18431153893470764\n",
            "iteration 715 :train_loss:0.1851528435945511 val_loss0.18430928885936737\n",
            "iteration 716 :train_loss:0.1851506233215332 val_loss0.1843070089817047\n",
            "iteration 717 :train_loss:0.18514840304851532 val_loss0.18430474400520325\n",
            "iteration 718 :train_loss:0.18514618277549744 val_loss0.18430249392986298\n",
            "iteration 719 :train_loss:0.18514397740364075 val_loss0.1843002587556839\n",
            "iteration 720 :train_loss:0.18514177203178406 val_loss0.18429800868034363\n",
            "iteration 721 :train_loss:0.18513956665992737 val_loss0.18429580330848694\n",
            "iteration 722 :train_loss:0.18513734638690948 val_loss0.18429355323314667\n",
            "iteration 723 :train_loss:0.1851351410150528 val_loss0.18429133296012878\n",
            "iteration 724 :train_loss:0.1851329505443573 val_loss0.1842890977859497\n",
            "iteration 725 :train_loss:0.18513073027133942 val_loss0.18428686261177063\n",
            "iteration 726 :train_loss:0.18512850999832153 val_loss0.18428464233875275\n",
            "iteration 727 :train_loss:0.18512631952762604 val_loss0.18428242206573486\n",
            "iteration 728 :train_loss:0.18512409925460815 val_loss0.1842801719903946\n",
            "iteration 729 :train_loss:0.18512189388275146 val_loss0.1842779666185379\n",
            "iteration 730 :train_loss:0.18511971831321716 val_loss0.1842757612466812\n",
            "iteration 731 :train_loss:0.18511749804019928 val_loss0.18427354097366333\n",
            "iteration 732 :train_loss:0.1851152926683426 val_loss0.18427132070064545\n",
            "iteration 733 :train_loss:0.1851131021976471 val_loss0.18426910042762756\n",
            "iteration 734 :train_loss:0.1851109117269516 val_loss0.1842668652534485\n",
            "iteration 735 :train_loss:0.1851087063550949 val_loss0.1842646598815918\n",
            "iteration 736 :train_loss:0.18510651588439941 val_loss0.1842624545097351\n",
            "iteration 737 :train_loss:0.18510432541370392 val_loss0.18426023423671722\n",
            "iteration 738 :train_loss:0.18510212004184723 val_loss0.18425804376602173\n",
            "iteration 739 :train_loss:0.18509994447231293 val_loss0.18425580859184265\n",
            "iteration 740 :train_loss:0.18509775400161743 val_loss0.18425363302230835\n",
            "iteration 741 :train_loss:0.18509557843208313 val_loss0.18425141274929047\n",
            "iteration 742 :train_loss:0.18509337306022644 val_loss0.18424920737743378\n",
            "iteration 743 :train_loss:0.18509118258953094 val_loss0.18424703180789948\n",
            "iteration 744 :train_loss:0.18508900701999664 val_loss0.1842448115348816\n",
            "iteration 745 :train_loss:0.18508683145046234 val_loss0.1842426359653473\n",
            "iteration 746 :train_loss:0.18508464097976685 val_loss0.1842404156923294\n",
            "iteration 747 :train_loss:0.18508248031139374 val_loss0.1842382401227951\n",
            "iteration 748 :train_loss:0.18508028984069824 val_loss0.18423603475093842\n",
            "iteration 749 :train_loss:0.18507812917232513 val_loss0.18423382937908173\n",
            "iteration 750 :train_loss:0.18507593870162964 val_loss0.18423165380954742\n",
            "iteration 751 :train_loss:0.18507376313209534 val_loss0.18422946333885193\n",
            "iteration 752 :train_loss:0.18507157266139984 val_loss0.18422727286815643\n",
            "iteration 753 :train_loss:0.18506942689418793 val_loss0.18422506749629974\n",
            "iteration 754 :train_loss:0.18506722152233124 val_loss0.18422289192676544\n",
            "iteration 755 :train_loss:0.18506507575511932 val_loss0.18422068655490875\n",
            "iteration 756 :train_loss:0.18506290018558502 val_loss0.18421848118305206\n",
            "iteration 757 :train_loss:0.18506073951721191 val_loss0.18421627581119537\n",
            "iteration 758 :train_loss:0.1850585639476776 val_loss0.18421407043933868\n",
            "iteration 759 :train_loss:0.1850564032793045 val_loss0.184211865067482\n",
            "iteration 760 :train_loss:0.1850542277097702 val_loss0.1842096596956253\n",
            "iteration 761 :train_loss:0.1850520819425583 val_loss0.1842074692249298\n",
            "iteration 762 :train_loss:0.185049906373024 val_loss0.18420526385307312\n",
            "iteration 763 :train_loss:0.18504774570465088 val_loss0.18420307338237762\n",
            "iteration 764 :train_loss:0.18504558503627777 val_loss0.18420086801052094\n",
            "iteration 765 :train_loss:0.18504343926906586 val_loss0.18419867753982544\n",
            "iteration 766 :train_loss:0.18504127860069275 val_loss0.18419650197029114\n",
            "iteration 767 :train_loss:0.18503913283348083 val_loss0.18419431149959564\n",
            "iteration 768 :train_loss:0.18503697216510773 val_loss0.18419210612773895\n",
            "iteration 769 :train_loss:0.18503481149673462 val_loss0.18418993055820465\n",
            "iteration 770 :train_loss:0.1850326657295227 val_loss0.18418772518634796\n",
            "iteration 771 :train_loss:0.1850305050611496 val_loss0.18418553471565247\n",
            "iteration 772 :train_loss:0.1850283443927765 val_loss0.18418338894844055\n",
            "iteration 773 :train_loss:0.18502621352672577 val_loss0.18418121337890625\n",
            "iteration 774 :train_loss:0.18502405285835266 val_loss0.18417902290821075\n",
            "iteration 775 :train_loss:0.18502192199230194 val_loss0.18417686223983765\n",
            "iteration 776 :train_loss:0.18501976132392883 val_loss0.18417468667030334\n",
            "iteration 777 :train_loss:0.1850176304578781 val_loss0.18417251110076904\n",
            "iteration 778 :train_loss:0.1850154995918274 val_loss0.18417030572891235\n",
            "iteration 779 :train_loss:0.18501336872577667 val_loss0.18416815996170044\n",
            "iteration 780 :train_loss:0.18501122295856476 val_loss0.18416599929332733\n",
            "iteration 781 :train_loss:0.18500907719135284 val_loss0.18416383862495422\n",
            "iteration 782 :train_loss:0.18500696122646332 val_loss0.18416166305541992\n",
            "iteration 783 :train_loss:0.1850048005580902 val_loss0.184159517288208\n",
            "iteration 784 :train_loss:0.1850026398897171 val_loss0.1841573566198349\n",
            "iteration 785 :train_loss:0.18500052392482758 val_loss0.18415521085262299\n",
            "iteration 786 :train_loss:0.18499839305877686 val_loss0.18415306508541107\n",
            "iteration 787 :train_loss:0.18499626219272614 val_loss0.18415088951587677\n",
            "iteration 788 :train_loss:0.1849941462278366 val_loss0.18414874374866486\n",
            "iteration 789 :train_loss:0.1849920153617859 val_loss0.18414656817913055\n",
            "iteration 790 :train_loss:0.18498986959457397 val_loss0.18414440751075745\n",
            "iteration 791 :train_loss:0.18498775362968445 val_loss0.18414226174354553\n",
            "iteration 792 :train_loss:0.18498560786247253 val_loss0.18414010107517242\n",
            "iteration 793 :train_loss:0.184983491897583 val_loss0.1841379553079605\n",
            "iteration 794 :train_loss:0.18498137593269348 val_loss0.1841357946395874\n",
            "iteration 795 :train_loss:0.18497924506664276 val_loss0.1841336339712143\n",
            "iteration 796 :train_loss:0.18497712910175323 val_loss0.18413148820400238\n",
            "iteration 797 :train_loss:0.1849750131368637 val_loss0.18412934243679047\n",
            "iteration 798 :train_loss:0.184972882270813 val_loss0.18412718176841736\n",
            "iteration 799 :train_loss:0.18497076630592346 val_loss0.18412502110004425\n",
            "iteration 800 :train_loss:0.18496865034103394 val_loss0.18412287533283234\n",
            "iteration 801 :train_loss:0.1849665343761444 val_loss0.18412072956562042\n",
            "iteration 802 :train_loss:0.18496441841125488 val_loss0.1841185837984085\n",
            "iteration 803 :train_loss:0.18496230244636536 val_loss0.1841164231300354\n",
            "iteration 804 :train_loss:0.18496018648147583 val_loss0.1841142624616623\n",
            "iteration 805 :train_loss:0.1849580705165863 val_loss0.18411210179328918\n",
            "iteration 806 :train_loss:0.18495599925518036 val_loss0.18410994112491608\n",
            "iteration 807 :train_loss:0.18495388329029083 val_loss0.18410778045654297\n",
            "iteration 808 :train_loss:0.1849517673254013 val_loss0.18410563468933105\n",
            "iteration 809 :train_loss:0.18494965136051178 val_loss0.18410344421863556\n",
            "iteration 810 :train_loss:0.18494756519794464 val_loss0.18410131335258484\n",
            "iteration 811 :train_loss:0.1849454641342163 val_loss0.1840991973876953\n",
            "iteration 812 :train_loss:0.18494336307048798 val_loss0.1840970665216446\n",
            "iteration 813 :train_loss:0.18494124710559845 val_loss0.18409495055675507\n",
            "iteration 814 :train_loss:0.1849391609430313 val_loss0.18409283459186554\n",
            "iteration 815 :train_loss:0.18493708968162537 val_loss0.1840907335281372\n",
            "iteration 816 :train_loss:0.18493497371673584 val_loss0.1840886026620865\n",
            "iteration 817 :train_loss:0.1849329024553299 val_loss0.18408645689487457\n",
            "iteration 818 :train_loss:0.18493081629276276 val_loss0.18408435583114624\n",
            "iteration 819 :train_loss:0.18492873013019562 val_loss0.1840822398662567\n",
            "iteration 820 :train_loss:0.18492662906646729 val_loss0.1840801239013672\n",
            "iteration 821 :train_loss:0.18492455780506134 val_loss0.18407797813415527\n",
            "iteration 822 :train_loss:0.1849224716424942 val_loss0.18407586216926575\n",
            "iteration 823 :train_loss:0.18492038547992706 val_loss0.18407371640205383\n",
            "iteration 824 :train_loss:0.18491829931735992 val_loss0.1840715855360031\n",
            "iteration 825 :train_loss:0.18491622805595398 val_loss0.1840694546699524\n",
            "iteration 826 :train_loss:0.18491414189338684 val_loss0.18406729400157928\n",
            "iteration 827 :train_loss:0.1849120706319809 val_loss0.18406520783901215\n",
            "iteration 828 :train_loss:0.18490999937057495 val_loss0.18406307697296143\n",
            "iteration 829 :train_loss:0.184907928109169 val_loss0.1840609610080719\n",
            "iteration 830 :train_loss:0.18490585684776306 val_loss0.18405881524085999\n",
            "iteration 831 :train_loss:0.18490377068519592 val_loss0.18405669927597046\n",
            "iteration 832 :train_loss:0.18490169942378998 val_loss0.18405458331108093\n",
            "iteration 833 :train_loss:0.18489964306354523 val_loss0.1840524673461914\n",
            "iteration 834 :train_loss:0.18489757180213928 val_loss0.18405033648014069\n",
            "iteration 835 :train_loss:0.18489551544189453 val_loss0.18404823541641235\n",
            "iteration 836 :train_loss:0.1848934441804886 val_loss0.18404611945152283\n",
            "iteration 837 :train_loss:0.18489141762256622 val_loss0.1840440034866333\n",
            "iteration 838 :train_loss:0.18488934636116028 val_loss0.18404188752174377\n",
            "iteration 839 :train_loss:0.18488726019859314 val_loss0.18403977155685425\n",
            "iteration 840 :train_loss:0.18488521873950958 val_loss0.18403767049312592\n",
            "iteration 841 :train_loss:0.18488316237926483 val_loss0.18403559923171997\n",
            "iteration 842 :train_loss:0.18488112092018127 val_loss0.18403349816799164\n",
            "iteration 843 :train_loss:0.18487904965877533 val_loss0.1840314120054245\n",
            "iteration 844 :train_loss:0.18487702310085297 val_loss0.18402932584285736\n",
            "iteration 845 :train_loss:0.18487496674060822 val_loss0.18402723968029022\n",
            "iteration 846 :train_loss:0.18487291038036346 val_loss0.18402516841888428\n",
            "iteration 847 :train_loss:0.1848708540201187 val_loss0.18402306735515594\n",
            "iteration 848 :train_loss:0.18486879765987396 val_loss0.18402099609375\n",
            "iteration 849 :train_loss:0.1848667711019516 val_loss0.18401892483234406\n",
            "iteration 850 :train_loss:0.18486471474170685 val_loss0.1840168684720993\n",
            "iteration 851 :train_loss:0.1848626583814621 val_loss0.18401478230953217\n",
            "iteration 852 :train_loss:0.18486063182353973 val_loss0.1840127408504486\n",
            "iteration 853 :train_loss:0.18485859036445618 val_loss0.18401066958904266\n",
            "iteration 854 :train_loss:0.1848565638065338 val_loss0.18400859832763672\n",
            "iteration 855 :train_loss:0.18485452234745026 val_loss0.18400652706623077\n",
            "iteration 856 :train_loss:0.1848524957895279 val_loss0.18400445580482483\n",
            "iteration 857 :train_loss:0.18485045433044434 val_loss0.18400242924690247\n",
            "iteration 858 :train_loss:0.18484842777252197 val_loss0.18400034308433533\n",
            "iteration 859 :train_loss:0.18484638631343842 val_loss0.18399828672409058\n",
            "iteration 860 :train_loss:0.18484437465667725 val_loss0.18399624526500702\n",
            "iteration 861 :train_loss:0.18484234809875488 val_loss0.18399418890476227\n",
            "iteration 862 :train_loss:0.1848403364419937 val_loss0.18399213254451752\n",
            "iteration 863 :train_loss:0.18483829498291016 val_loss0.18399009108543396\n",
            "iteration 864 :train_loss:0.1848362684249878 val_loss0.1839880347251892\n",
            "iteration 865 :train_loss:0.18483427166938782 val_loss0.18398596346378326\n",
            "iteration 866 :train_loss:0.18483224511146545 val_loss0.1839839071035385\n",
            "iteration 867 :train_loss:0.1848302036523819 val_loss0.18398186564445496\n",
            "iteration 868 :train_loss:0.18482820689678192 val_loss0.1839798092842102\n",
            "iteration 869 :train_loss:0.18482618033885956 val_loss0.18397776782512665\n",
            "iteration 870 :train_loss:0.18482418358325958 val_loss0.1839757114648819\n",
            "iteration 871 :train_loss:0.1848222017288208 val_loss0.18397367000579834\n",
            "iteration 872 :train_loss:0.18482016026973724 val_loss0.1839716136455536\n",
            "iteration 873 :train_loss:0.18481816351413727 val_loss0.18396958708763123\n",
            "iteration 874 :train_loss:0.1848161518573761 val_loss0.18396751582622528\n",
            "iteration 875 :train_loss:0.18481415510177612 val_loss0.18396547436714172\n",
            "iteration 876 :train_loss:0.18481214344501495 val_loss0.18396341800689697\n",
            "iteration 877 :train_loss:0.18481016159057617 val_loss0.18396137654781342\n",
            "iteration 878 :train_loss:0.1848081648349762 val_loss0.18395932018756866\n",
            "iteration 879 :train_loss:0.18480616807937622 val_loss0.1839572787284851\n",
            "iteration 880 :train_loss:0.18480417132377625 val_loss0.18395523726940155\n",
            "iteration 881 :train_loss:0.18480215966701508 val_loss0.183953195810318\n",
            "iteration 882 :train_loss:0.1848001927137375 val_loss0.18395115435123444\n",
            "iteration 883 :train_loss:0.18479818105697632 val_loss0.18394912779331207\n",
            "iteration 884 :train_loss:0.18479618430137634 val_loss0.18394708633422852\n",
            "iteration 885 :train_loss:0.18479418754577637 val_loss0.18394504487514496\n",
            "iteration 886 :train_loss:0.18479220569133759 val_loss0.1839430332183838\n",
            "iteration 887 :train_loss:0.1847902238368988 val_loss0.18394099175930023\n",
            "iteration 888 :train_loss:0.18478824198246002 val_loss0.18393898010253906\n",
            "iteration 889 :train_loss:0.18478624522686005 val_loss0.1839369386434555\n",
            "iteration 890 :train_loss:0.18478429317474365 val_loss0.18393491208553314\n",
            "iteration 891 :train_loss:0.18478229641914368 val_loss0.18393288552761078\n",
            "iteration 892 :train_loss:0.1847802996635437 val_loss0.18393084406852722\n",
            "iteration 893 :train_loss:0.1847783476114273 val_loss0.18392883241176605\n",
            "iteration 894 :train_loss:0.18477636575698853 val_loss0.18392682075500488\n",
            "iteration 895 :train_loss:0.18477436900138855 val_loss0.18392476439476013\n",
            "iteration 896 :train_loss:0.18477238714694977 val_loss0.18392276763916016\n",
            "iteration 897 :train_loss:0.18477042019367218 val_loss0.1839207410812378\n",
            "iteration 898 :train_loss:0.1847684681415558 val_loss0.18391872942447662\n",
            "iteration 899 :train_loss:0.1847664713859558 val_loss0.18391673266887665\n",
            "iteration 900 :train_loss:0.18476451933383942 val_loss0.18391470611095428\n",
            "iteration 901 :train_loss:0.18476253747940063 val_loss0.18391269445419312\n",
            "iteration 902 :train_loss:0.18476058542728424 val_loss0.18391069769859314\n",
            "iteration 903 :train_loss:0.18475860357284546 val_loss0.18390868604183197\n",
            "iteration 904 :train_loss:0.18475663661956787 val_loss0.1839067041873932\n",
            "iteration 905 :train_loss:0.18475468456745148 val_loss0.1839047223329544\n",
            "iteration 906 :train_loss:0.1847527176141739 val_loss0.18390272557735443\n",
            "iteration 907 :train_loss:0.1847507357597351 val_loss0.18390075862407684\n",
            "iteration 908 :train_loss:0.1847487837076187 val_loss0.18389874696731567\n",
            "iteration 909 :train_loss:0.18474683165550232 val_loss0.18389678001403809\n",
            "iteration 910 :train_loss:0.18474487960338593 val_loss0.1838947981595993\n",
            "iteration 911 :train_loss:0.18474292755126953 val_loss0.18389281630516052\n",
            "iteration 912 :train_loss:0.18474097549915314 val_loss0.18389081954956055\n",
            "iteration 913 :train_loss:0.18473900854587555 val_loss0.18388883769512177\n",
            "iteration 914 :train_loss:0.18473705649375916 val_loss0.18388685584068298\n",
            "iteration 915 :train_loss:0.18473507463932037 val_loss0.1838848888874054\n",
            "iteration 916 :train_loss:0.18473315238952637 val_loss0.1838829219341278\n",
            "iteration 917 :train_loss:0.18473117053508759 val_loss0.1838809698820114\n",
            "iteration 918 :train_loss:0.18472924828529358 val_loss0.18387898802757263\n",
            "iteration 919 :train_loss:0.18472729623317719 val_loss0.18387700617313385\n",
            "iteration 920 :train_loss:0.18472535908222198 val_loss0.18387505412101746\n",
            "iteration 921 :train_loss:0.1847234070301056 val_loss0.18387310206890106\n",
            "iteration 922 :train_loss:0.1847214549779892 val_loss0.18387113511562347\n",
            "iteration 923 :train_loss:0.1847195029258728 val_loss0.18386919796466827\n",
            "iteration 924 :train_loss:0.1847175508737564 val_loss0.1838672161102295\n",
            "iteration 925 :train_loss:0.1847156137228012 val_loss0.1838652640581131\n",
            "iteration 926 :train_loss:0.1847136914730072 val_loss0.1838633120059967\n",
            "iteration 927 :train_loss:0.184711754322052 val_loss0.1838613599538803\n",
            "iteration 928 :train_loss:0.1847098171710968 val_loss0.18385940790176392\n",
            "iteration 929 :train_loss:0.1847078651189804 val_loss0.18385747075080872\n",
            "iteration 930 :train_loss:0.1847059577703476 val_loss0.18385553359985352\n",
            "iteration 931 :train_loss:0.1847040355205536 val_loss0.18385356664657593\n",
            "iteration 932 :train_loss:0.1847020983695984 val_loss0.18385162949562073\n",
            "iteration 933 :train_loss:0.18470019102096558 val_loss0.18384966254234314\n",
            "iteration 934 :train_loss:0.18469826877117157 val_loss0.18384771049022675\n",
            "iteration 935 :train_loss:0.18469634652137756 val_loss0.18384575843811035\n",
            "iteration 936 :train_loss:0.18469442427158356 val_loss0.18384377658367157\n",
            "iteration 937 :train_loss:0.18469250202178955 val_loss0.18384182453155518\n",
            "iteration 938 :train_loss:0.18469059467315674 val_loss0.18383987247943878\n",
            "iteration 939 :train_loss:0.18468867242336273 val_loss0.183837890625\n",
            "iteration 940 :train_loss:0.18468675017356873 val_loss0.1838359236717224\n",
            "iteration 941 :train_loss:0.18468482792377472 val_loss0.18383392691612244\n",
            "iteration 942 :train_loss:0.1846829205751419 val_loss0.18383197486400604\n",
            "iteration 943 :train_loss:0.1846810132265091 val_loss0.18383000791072845\n",
            "iteration 944 :train_loss:0.18467910587787628 val_loss0.18382804095745087\n",
            "iteration 945 :train_loss:0.18467718362808228 val_loss0.18382608890533447\n",
            "iteration 946 :train_loss:0.18467527627944946 val_loss0.18382412195205688\n",
            "iteration 947 :train_loss:0.18467336893081665 val_loss0.1838221549987793\n",
            "iteration 948 :train_loss:0.18467144668102264 val_loss0.1838202029466629\n",
            "iteration 949 :train_loss:0.18466955423355103 val_loss0.18381823599338531\n",
            "iteration 950 :train_loss:0.1846676468849182 val_loss0.18381626904010773\n",
            "iteration 951 :train_loss:0.1846657395362854 val_loss0.18381430208683014\n",
            "iteration 952 :train_loss:0.1846638321876526 val_loss0.18381235003471375\n",
            "iteration 953 :train_loss:0.18466193974018097 val_loss0.18381039798259735\n",
            "iteration 954 :train_loss:0.18466001749038696 val_loss0.18380844593048096\n",
            "iteration 955 :train_loss:0.18465813994407654 val_loss0.18380649387836456\n",
            "iteration 956 :train_loss:0.18465621769428253 val_loss0.18380457162857056\n",
            "iteration 957 :train_loss:0.18465431034564972 val_loss0.18380264937877655\n",
            "iteration 958 :train_loss:0.1846524178981781 val_loss0.18380074203014374\n",
            "iteration 959 :train_loss:0.18465052545070648 val_loss0.18379883468151093\n",
            "iteration 960 :train_loss:0.18464863300323486 val_loss0.18379691243171692\n",
            "iteration 961 :train_loss:0.18464674055576324 val_loss0.1837950199842453\n",
            "iteration 962 :train_loss:0.18464484810829163 val_loss0.18379312753677368\n",
            "iteration 963 :train_loss:0.1846429705619812 val_loss0.18379120528697968\n",
            "iteration 964 :train_loss:0.18464107811450958 val_loss0.18378931283950806\n",
            "iteration 965 :train_loss:0.18463918566703796 val_loss0.18378740549087524\n",
            "iteration 966 :train_loss:0.18463730812072754 val_loss0.18378549814224243\n",
            "iteration 967 :train_loss:0.1846354454755783 val_loss0.1837836056947708\n",
            "iteration 968 :train_loss:0.1846335530281067 val_loss0.1837817132472992\n",
            "iteration 969 :train_loss:0.18463166058063507 val_loss0.18377982079982758\n",
            "iteration 970 :train_loss:0.18462978303432465 val_loss0.18377792835235596\n",
            "iteration 971 :train_loss:0.18462792038917542 val_loss0.18377605080604553\n",
            "iteration 972 :train_loss:0.1846260279417038 val_loss0.18377414345741272\n",
            "iteration 973 :train_loss:0.18462413549423218 val_loss0.1837722361087799\n",
            "iteration 974 :train_loss:0.18462225794792175 val_loss0.18377035856246948\n",
            "iteration 975 :train_loss:0.18462039530277252 val_loss0.18376843631267548\n",
            "iteration 976 :train_loss:0.1846184879541397 val_loss0.18376652896404266\n",
            "iteration 977 :train_loss:0.18461662530899048 val_loss0.18376462161540985\n",
            "iteration 978 :train_loss:0.18461473286151886 val_loss0.18376272916793823\n",
            "iteration 979 :train_loss:0.18461287021636963 val_loss0.18376082181930542\n",
            "iteration 980 :train_loss:0.1846109926700592 val_loss0.1837589293718338\n",
            "iteration 981 :train_loss:0.18460913002490997 val_loss0.18375705182552338\n",
            "iteration 982 :train_loss:0.18460725247859955 val_loss0.18375515937805176\n",
            "iteration 983 :train_loss:0.18460538983345032 val_loss0.18375323712825775\n",
            "iteration 984 :train_loss:0.1846035122871399 val_loss0.18375134468078613\n",
            "iteration 985 :train_loss:0.18460164964199066 val_loss0.18374945223331451\n",
            "iteration 986 :train_loss:0.18459977209568024 val_loss0.1837475597858429\n",
            "iteration 987 :train_loss:0.1845978945493698 val_loss0.18374565243721008\n",
            "iteration 988 :train_loss:0.18459604680538177 val_loss0.18374375998973846\n",
            "iteration 989 :train_loss:0.18459419906139374 val_loss0.18374185264110565\n",
            "iteration 990 :train_loss:0.1845923364162445 val_loss0.18373996019363403\n",
            "iteration 991 :train_loss:0.18459045886993408 val_loss0.1837380826473236\n",
            "iteration 992 :train_loss:0.18458862602710724 val_loss0.183736190199852\n",
            "iteration 993 :train_loss:0.184586763381958 val_loss0.18373429775238037\n",
            "iteration 994 :train_loss:0.18458490073680878 val_loss0.18373240530490875\n",
            "iteration 995 :train_loss:0.18458305299282074 val_loss0.18373052775859833\n",
            "iteration 996 :train_loss:0.1845812052488327 val_loss0.18372862040996552\n",
            "iteration 997 :train_loss:0.18457935750484467 val_loss0.18372675776481628\n",
            "iteration 998 :train_loss:0.18457747995853424 val_loss0.18372486531734467\n",
            "iteration 999 :train_loss:0.1845756620168686 val_loss0.18372297286987305\n"
          ]
        }
      ],
      "source": [
        "iterations=1000\n",
        "parameters, history=create_nn_model(train_X,train_Y, val_X, val_Y,100,iterations, 0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il-Ip-wN9QqG"
      },
      "source": [
        "Plotting the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "PpfalKXd9SCa",
        "outputId": "3c468cc5-fa62-404a-ee64-d1cee5769fdb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(0,iterations),history[\"train_loss\"],'b')\n",
        "plt.plot(range(0,iterations),history[\"val_loss\"],'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWElVUFw95rX"
      },
      "source": [
        "Calculating loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbwb4F_y4g3J",
        "outputId": "72ac88d5-fa50-45f2-ca0a-c52eeec04a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss of the model on the training data is: 0.18457378447055817\n",
            "loss of the model on the validation data is: 0.18372111022472382\n"
          ]
        }
      ],
      "source": [
        "\n",
        "predicted_train=predict(parameters, train_X)\n",
        "predicted_val=predict(parameters, val_X)\n",
        "\n",
        "print(\"loss of the model on the training data is:\", float(compute_loss(train_Y,predicted_train)))\n",
        "print(\"loss of the model on the validation data is:\", float(compute_loss(val_Y,predicted_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP4s0M8WV0TW"
      },
      "source": [
        "#training and hyper-parameter tuning\n",
        "\n",
        "Increasing number of iterations because loss is still decreasing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0CU3b2LFmEP",
        "outputId": "04b895e3-ee3b-47b2-abe1-fdc2e01dc878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration 15000 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15001 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15002 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15003 :train_loss:0.18200863897800446 val_loss0.1812058538198471\n",
            "iteration 15004 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15005 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15006 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15007 :train_loss:0.18200863897800446 val_loss0.1812058538198471\n",
            "iteration 15008 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15009 :train_loss:0.18200863897800446 val_loss0.1812058538198471\n",
            "iteration 15010 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15011 :train_loss:0.18200863897800446 val_loss0.1812058538198471\n",
            "iteration 15012 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15013 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15014 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15015 :train_loss:0.18200863897800446 val_loss0.1812058538198471\n",
            "iteration 15016 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15017 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15018 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15019 :train_loss:0.18200863897800446 val_loss0.1812058538198471\n",
            "iteration 15020 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15021 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15022 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15023 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15024 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15025 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15026 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15027 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15028 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15029 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15030 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15031 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15032 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15033 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15034 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15035 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15036 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15037 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15038 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15039 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15040 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15041 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15042 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15043 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15044 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15045 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15046 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15047 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15048 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15049 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15050 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15051 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15052 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15053 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15054 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15055 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15056 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15057 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15058 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15059 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15060 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15061 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15062 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15063 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15064 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15065 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15066 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15067 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15068 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15069 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15070 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15071 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15072 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15073 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15074 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15075 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15076 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15077 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15078 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15079 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15080 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15081 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15082 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15083 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15084 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15085 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15086 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15087 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15088 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15089 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15090 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15091 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15092 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15093 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15094 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15095 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15096 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15097 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15098 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15099 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15100 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15101 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15102 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15103 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15104 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15105 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15106 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15107 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15108 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15109 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15110 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 15111 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15112 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15113 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15114 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15115 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15116 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15117 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15118 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15119 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15120 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15121 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15122 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15123 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15124 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15125 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15126 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15127 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15128 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15129 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15130 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15131 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15132 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15133 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15134 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15135 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15136 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15137 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15138 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15139 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15140 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15141 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15142 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15143 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15144 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15145 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15146 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15147 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15148 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15149 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15150 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15151 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15152 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15153 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15154 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15155 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15156 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15157 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15158 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15159 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15160 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15161 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15162 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15163 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15164 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15165 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15166 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15167 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15168 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15169 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15170 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15171 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15172 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15173 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15174 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15175 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15176 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15177 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15178 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15179 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15180 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15181 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15182 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15183 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15184 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15185 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15186 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15187 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15188 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15189 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 15190 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15191 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15192 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15193 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15194 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15195 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15196 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15197 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15198 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15199 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15200 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15201 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15202 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15203 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15204 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15205 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15206 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15207 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15208 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15209 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15210 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15211 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15212 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15213 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15214 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15215 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15216 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15217 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15218 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15219 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15220 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15221 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15222 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15223 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15224 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15225 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15226 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15227 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15228 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15229 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15230 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15231 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15232 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15233 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15234 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15235 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15236 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15237 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15238 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15239 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15240 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15241 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15242 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15243 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15244 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15245 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15246 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15247 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15248 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15249 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15250 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15251 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15252 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15253 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15254 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15255 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15256 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15257 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15258 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15259 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15260 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15261 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15262 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15263 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15264 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15265 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15266 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15267 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15268 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15269 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15270 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15271 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15272 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15273 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15274 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15275 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15276 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15277 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15278 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15279 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15280 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15281 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15282 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15283 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15284 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15285 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15286 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15287 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15288 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15289 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15290 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15291 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15292 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15293 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15294 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15295 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15296 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15297 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15298 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15299 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15300 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15301 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15302 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15303 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15304 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15305 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15306 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15307 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15308 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15309 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15310 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15311 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15312 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15313 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15314 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15315 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15316 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15317 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15318 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15319 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15320 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15321 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15322 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15323 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15324 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15325 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15326 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15327 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15328 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15329 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15330 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15331 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15332 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15333 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15334 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15335 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15336 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15337 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15338 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15339 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15340 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15341 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15342 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15343 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15344 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15345 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15346 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15347 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15348 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 15349 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15350 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15351 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15352 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15353 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15354 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15355 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15356 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15357 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15358 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15359 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15360 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15361 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15362 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15363 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15364 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15365 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15366 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15367 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15368 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15369 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15370 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15371 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15372 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15373 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15374 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15375 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15376 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15377 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15378 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15379 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15380 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15381 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15382 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15383 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15384 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15385 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15386 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15387 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15388 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15389 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15390 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15391 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15392 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15393 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15394 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15395 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15396 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15397 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15398 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15399 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15400 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15401 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15402 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15403 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15404 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15405 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15406 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15407 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15408 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15409 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15410 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15411 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15412 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15413 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15414 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15415 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15416 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15417 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15418 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15419 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15420 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15421 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15422 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15423 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15424 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15425 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15426 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15427 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15428 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15429 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15430 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15431 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15432 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15433 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15434 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15435 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15436 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15437 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15438 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15439 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15440 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15441 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15442 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15443 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15444 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15445 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15446 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15447 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15448 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15449 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15450 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15451 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15452 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15453 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15454 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15455 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15456 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15457 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15458 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15459 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15460 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15461 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15462 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15463 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15464 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15465 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15466 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15467 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15468 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 15469 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15470 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15471 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15472 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15473 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15474 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15475 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15476 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15477 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 15478 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15479 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15480 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15481 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15482 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15483 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15484 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15485 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15486 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15487 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15488 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15489 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15490 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15491 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15492 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15493 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15494 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15495 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15496 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15497 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 15498 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15499 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15500 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15501 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15502 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15503 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15504 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15505 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15506 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15507 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15508 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15509 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15510 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15511 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15512 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15513 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15514 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15515 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15516 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15517 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15518 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15519 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15520 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15521 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15522 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15523 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15524 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15525 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15526 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 15527 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15528 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15529 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15530 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15531 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15532 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15533 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15534 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15535 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15536 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15537 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15538 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15539 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15540 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15541 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15542 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15543 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15544 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15545 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15546 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15547 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15548 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15549 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15550 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15551 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15552 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15553 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15554 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15555 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15556 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15557 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15558 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15559 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15560 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15561 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 15562 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15563 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15564 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15565 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15566 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15567 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15568 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15569 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15570 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15571 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15572 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15573 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15574 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15575 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15576 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15577 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15578 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15579 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15580 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15581 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15582 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15583 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15584 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15585 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15586 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15587 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15588 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15589 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15590 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15591 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15592 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15593 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15594 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15595 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15596 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15597 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15598 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15599 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15600 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15601 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15602 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15603 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15604 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15605 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15606 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15607 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15608 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15609 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15610 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15611 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15612 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15613 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15614 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15615 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15616 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15617 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15618 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15619 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15620 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15621 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15622 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15623 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15624 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15625 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15626 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15627 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15628 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15629 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15630 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15631 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15632 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15633 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15634 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15635 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15636 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15637 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15638 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15639 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15640 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15641 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15642 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15643 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15644 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15645 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15646 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15647 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15648 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15649 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15650 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15651 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15652 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15653 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15654 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15655 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15656 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15657 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15658 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15659 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15660 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15661 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15662 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15663 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15664 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15665 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15666 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15667 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15668 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15669 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15670 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15671 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15672 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15673 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15674 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15675 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15676 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15677 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15678 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15679 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15680 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15681 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15682 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15683 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15684 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15685 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15686 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15687 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15688 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15689 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15690 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15691 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15692 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15693 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15694 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15695 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15696 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15697 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15698 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15699 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15700 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15701 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15702 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15703 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15704 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15705 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15706 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15707 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15708 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15709 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15710 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15711 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15712 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15713 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15714 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15715 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15716 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15717 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15718 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15719 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 15720 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15721 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15722 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15723 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15724 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15725 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15726 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15727 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 15728 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15729 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15730 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15731 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15732 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15733 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15734 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15735 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15736 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15737 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15738 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15739 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15740 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15741 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15742 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15743 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15744 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15745 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15746 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15747 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15748 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15749 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15750 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15751 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15752 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15753 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15754 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15755 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15756 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15757 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15758 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15759 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15760 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15761 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15762 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15763 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15764 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15765 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15766 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15767 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15768 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15769 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15770 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15771 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15772 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15773 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15774 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15775 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15776 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15777 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15778 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15779 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15780 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15781 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15782 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15783 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15784 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15785 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15786 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15787 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15788 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15789 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15790 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15791 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15792 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15793 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15794 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15795 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15796 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15797 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15798 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15799 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15800 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15801 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15802 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15803 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15804 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15805 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15806 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15807 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15808 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15809 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15810 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15811 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15812 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15813 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15814 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15815 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15816 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15817 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15818 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15819 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15820 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15821 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15822 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15823 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15824 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15825 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15826 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15827 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15828 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15829 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15830 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15831 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15832 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15833 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15834 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15835 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15836 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15837 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15838 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15839 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15840 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15841 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15842 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15843 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15844 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15845 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15846 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15847 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15848 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15849 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15850 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15851 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15852 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15853 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15854 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15855 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15856 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15857 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15858 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15859 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15860 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15861 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 15862 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15863 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15864 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15865 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15866 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15867 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15868 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15869 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15870 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15871 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15872 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15873 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15874 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15875 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15876 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15877 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15878 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15879 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15880 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15881 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15882 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15883 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15884 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15885 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15886 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15887 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15888 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15889 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15890 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15891 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 15892 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15893 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15894 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15895 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15896 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15897 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15898 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15899 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15900 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15901 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15902 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15903 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15904 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15905 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15906 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15907 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15908 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15909 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15910 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15911 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15912 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15913 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15914 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15915 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15916 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15917 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15918 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15919 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15920 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15921 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15922 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15923 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15924 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15925 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15926 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15927 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15928 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15929 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15930 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15931 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15932 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15933 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15934 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15935 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15936 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15937 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15938 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15939 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15940 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15941 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15942 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15943 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15944 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15945 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15946 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15947 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15948 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15949 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15950 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15951 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15952 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15953 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15954 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15955 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15956 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15957 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15958 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15959 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 15960 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15961 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15962 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15963 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15964 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 15965 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15966 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15967 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15968 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15969 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15970 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15971 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15972 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15973 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15974 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15975 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15976 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15977 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15978 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15979 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15980 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15981 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15982 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15983 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15984 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15985 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15986 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15987 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15988 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 15989 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15990 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15991 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15992 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15993 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15994 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15995 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15996 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 15997 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15998 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 15999 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16000 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16001 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16002 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16003 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16004 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16005 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16006 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16007 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 16008 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16009 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16010 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16011 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16012 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16013 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16014 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16015 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16016 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16017 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16018 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16019 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16020 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16021 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16022 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16023 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16024 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 16025 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16026 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16027 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16028 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16029 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16030 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16031 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16032 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16033 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16034 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16035 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16036 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16037 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16038 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16039 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16040 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16041 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16042 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16043 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16044 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16045 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16046 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16047 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16048 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16049 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16050 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16051 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16052 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16053 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16054 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16055 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16056 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16057 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16058 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16059 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16060 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16061 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16062 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16063 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16064 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16065 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16066 :train_loss:0.18200863897800446 val_loss0.18120597302913666\n",
            "iteration 16067 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16068 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16069 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16070 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16071 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16072 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16073 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16074 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16075 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16076 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16077 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16078 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16079 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16080 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16081 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16082 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16083 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16084 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16085 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16086 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16087 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16088 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16089 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16090 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16091 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16092 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16093 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16094 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16095 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16096 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16097 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16098 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16099 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16100 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16101 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16102 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16103 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16104 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16105 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16106 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16107 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16108 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16109 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16110 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16111 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16112 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16113 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16114 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16115 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16116 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16117 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16118 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16119 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16120 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16121 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16122 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16123 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16124 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16125 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16126 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16127 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16128 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16129 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16130 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16131 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16132 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16133 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16134 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16135 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16136 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16137 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16138 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16139 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16140 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16141 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16142 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16143 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16144 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16145 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16146 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16147 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16148 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16149 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16150 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16151 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16152 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16153 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16154 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16155 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16156 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16157 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16158 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16159 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16160 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16161 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16162 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16163 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16164 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16165 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16166 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16167 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16168 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16169 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16170 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16171 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16172 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16173 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16174 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16175 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16176 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16177 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16178 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16179 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16180 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16181 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16182 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16183 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16184 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16185 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16186 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16187 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16188 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16189 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16190 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16191 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16192 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16193 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16194 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16195 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16196 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16197 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16198 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16199 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16200 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16201 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16202 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16203 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16204 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16205 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16206 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16207 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16208 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16209 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16210 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16211 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16212 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16213 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16214 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16215 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16216 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16217 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16218 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16219 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16220 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16221 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16222 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16223 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16224 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16225 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16226 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16227 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16228 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16229 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16230 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16231 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16232 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16233 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16234 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16235 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16236 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16237 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16238 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16239 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16240 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16241 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16242 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16243 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16244 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16245 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16246 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16247 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16248 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16249 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16250 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16251 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16252 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16253 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16254 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16255 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16256 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16257 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16258 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16259 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16260 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16261 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16262 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16263 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16264 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16265 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16266 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16267 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16268 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16269 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16270 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16271 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16272 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16273 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16274 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16275 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16276 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16277 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16278 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16279 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16280 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16281 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16282 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16283 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16284 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16285 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16286 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16287 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16288 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16289 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16290 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16291 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16292 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16293 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16294 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16295 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16296 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16297 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16298 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16299 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16300 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16301 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16302 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16303 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16304 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16305 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16306 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16307 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16308 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16309 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16310 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16311 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16312 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16313 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16314 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16315 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16316 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16317 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16318 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16319 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16320 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16321 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16322 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16323 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16324 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16325 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16326 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16327 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16328 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16329 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16330 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16331 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16332 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16333 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16334 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16335 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16336 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16337 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16338 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16339 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16340 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16341 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16342 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16343 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16344 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16345 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16346 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16347 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16348 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16349 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16350 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16351 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16352 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16353 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16354 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16355 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16356 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16357 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16358 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16359 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16360 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16361 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16362 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16363 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16364 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16365 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16366 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16367 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16368 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16369 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16370 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16371 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16372 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16373 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16374 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16375 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16376 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16377 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16378 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16379 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16380 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16381 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16382 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16383 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16384 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16385 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16386 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16387 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16388 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16389 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16390 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16391 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16392 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16393 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16394 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16395 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16396 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16397 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16398 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16399 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16400 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16401 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16402 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16403 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16404 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16405 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16406 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16407 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16408 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16409 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16410 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16411 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16412 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16413 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16414 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16415 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16416 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16417 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16418 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16419 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16420 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16421 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16422 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16423 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16424 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16425 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16426 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16427 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16428 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16429 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16430 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16431 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16432 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16433 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16434 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16435 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16436 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16437 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16438 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16439 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16440 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16441 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16442 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16443 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16444 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16445 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16446 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16447 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16448 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16449 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 16450 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16451 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16452 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16453 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16454 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16455 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16456 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16457 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16458 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16459 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16460 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16461 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16462 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16463 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16464 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16465 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16466 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16467 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16468 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16469 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16470 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16471 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16472 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16473 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16474 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16475 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16476 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16477 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16478 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16479 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16480 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16481 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16482 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16483 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16484 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16485 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16486 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16487 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16488 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16489 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 16490 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16491 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16492 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16493 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16494 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16495 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16496 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16497 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16498 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16499 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 16500 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16501 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16502 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16503 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16504 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16505 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16506 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16507 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16508 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 16509 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16510 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16511 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16512 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16513 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16514 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16515 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16516 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16517 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16518 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16519 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16520 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16521 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16522 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16523 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16524 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16525 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16526 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16527 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16528 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16529 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16530 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16531 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16532 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16533 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16534 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16535 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16536 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16537 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16538 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16539 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16540 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16541 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16542 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16543 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16544 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16545 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16546 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16547 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16548 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16549 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16550 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16551 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16552 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16553 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16554 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16555 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16556 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16557 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16558 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16559 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16560 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16561 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16562 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16563 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16564 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16565 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16566 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16567 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16568 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16569 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16570 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16571 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16572 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16573 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16574 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16575 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16576 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16577 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16578 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16579 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 16580 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16581 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16582 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16583 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 16584 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16585 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16586 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16587 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16588 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16589 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16590 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16591 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16592 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16593 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16594 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16595 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16596 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16597 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16598 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16599 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16600 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16601 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16602 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16603 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16604 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16605 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16606 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16607 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16608 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16609 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16610 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16611 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16612 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16613 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16614 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16615 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16616 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16617 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16618 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16619 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16620 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16621 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16622 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16623 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16624 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16625 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16626 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16627 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16628 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16629 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16630 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16631 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16632 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16633 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16634 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16635 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16636 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16637 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 16638 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16639 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16640 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16641 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16642 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16643 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16644 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16645 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16646 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16647 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16648 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16649 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16650 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16651 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16652 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16653 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 16654 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16655 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16656 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16657 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16658 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16659 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16660 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16661 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16662 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16663 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16664 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16665 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16666 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16667 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16668 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16669 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16670 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16671 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16672 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16673 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16674 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16675 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16676 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16677 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16678 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16679 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16680 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16681 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16682 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16683 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16684 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16685 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16686 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16687 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16688 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16689 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16690 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16691 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16692 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16693 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16694 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16695 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16696 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16697 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16698 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16699 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16700 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16701 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16702 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16703 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16704 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16705 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16706 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16707 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16708 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16709 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16710 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16711 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16712 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16713 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16714 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16715 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16716 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16717 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16718 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16719 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16720 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16721 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16722 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16723 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16724 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16725 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16726 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16727 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16728 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16729 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16730 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16731 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16732 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16733 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16734 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16735 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16736 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16737 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16738 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16739 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16740 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16741 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16742 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16743 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16744 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16745 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16746 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16747 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16748 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16749 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16750 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16751 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16752 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16753 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16754 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16755 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16756 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16757 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16758 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16759 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16760 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16761 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16762 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16763 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16764 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16765 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16766 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16767 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16768 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16769 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16770 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16771 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16772 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16773 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16774 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16775 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16776 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16777 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16778 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16779 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16780 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16781 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16782 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16783 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16784 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16785 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16786 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16787 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16788 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16789 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16790 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16791 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16792 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16793 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16794 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16795 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16796 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16797 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16798 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16799 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16800 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16801 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16802 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16803 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16804 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16805 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16806 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16807 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16808 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16809 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16810 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16811 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16812 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16813 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16814 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16815 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16816 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16817 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16818 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16819 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16820 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16821 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16822 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16823 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16824 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16825 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16826 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16827 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16828 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16829 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16830 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16831 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16832 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16833 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16834 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16835 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16836 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16837 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16838 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16839 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16840 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16841 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16842 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16843 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16844 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16845 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16846 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16847 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16848 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16849 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16850 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16851 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16852 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16853 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16854 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16855 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16856 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16857 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16858 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16859 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16860 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16861 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16862 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16863 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16864 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16865 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16866 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16867 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16868 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16869 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16870 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16871 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16872 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16873 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16874 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16875 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16876 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16877 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16878 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16879 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16880 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16881 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16882 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16883 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16884 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16885 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16886 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16887 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16888 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16889 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16890 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16891 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16892 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16893 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16894 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16895 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16896 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16897 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16898 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16899 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16900 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16901 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16902 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16903 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16904 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16905 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16906 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16907 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16908 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16909 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16910 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16911 :train_loss:0.18200865387916565 val_loss0.18120594322681427\n",
            "iteration 16912 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16913 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16914 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16915 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16916 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16917 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16918 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16919 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16920 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16921 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16922 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16923 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16924 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16925 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16926 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16927 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16928 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16929 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16930 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16931 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16932 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16933 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16934 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16935 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16936 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16937 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16938 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16939 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16940 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16941 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16942 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16943 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16944 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16945 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16946 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16947 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16948 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 16949 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16950 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16951 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16952 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16953 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16954 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16955 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16956 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16957 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16958 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16959 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16960 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16961 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16962 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16963 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 16964 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16965 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16966 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16967 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16968 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16969 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16970 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16971 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16972 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16973 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16974 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16975 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16976 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16977 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16978 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16979 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16980 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 16981 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16982 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16983 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16984 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16985 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16986 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16987 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16988 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16989 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16990 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 16991 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16992 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16993 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16994 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16995 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16996 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16997 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 16998 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 16999 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17000 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17001 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17002 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17003 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17004 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17005 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17006 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17007 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17008 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17009 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17010 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17011 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17012 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 17013 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17014 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17015 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17016 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17017 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17018 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17019 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17020 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17021 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17022 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17023 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17024 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17025 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17026 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17027 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17028 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17029 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17030 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17031 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17032 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17033 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17034 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17035 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17036 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17037 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17038 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17039 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17040 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17041 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17042 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17043 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17044 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17045 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17046 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17047 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17048 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17049 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17050 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17051 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17052 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17053 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17054 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17055 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17056 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17057 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17058 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17059 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17060 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17061 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17062 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17063 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17064 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17065 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17066 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17067 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17068 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17069 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17070 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17071 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17072 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17073 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17074 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17075 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17076 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17077 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17078 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17079 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17080 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17081 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17082 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17083 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17084 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17085 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17086 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17087 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17088 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17089 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17090 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17091 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17092 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17093 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17094 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17095 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17096 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17097 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17098 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17099 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17100 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17101 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17102 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17103 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17104 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17105 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17106 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17107 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17108 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17109 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17110 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17111 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17112 :train_loss:0.18200860917568207 val_loss0.18120595812797546\n",
            "iteration 17113 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17114 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17115 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17116 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17117 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17118 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17119 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17120 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17121 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17122 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17123 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17124 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17125 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17126 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17127 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17128 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17129 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17130 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17131 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17132 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17133 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17134 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17135 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17136 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17137 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17138 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17139 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17140 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17141 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17142 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17143 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17144 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17145 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17146 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17147 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17148 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17149 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17150 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17151 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17152 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17153 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17154 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17155 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17156 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17157 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17158 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17159 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17160 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17161 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17162 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17163 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17164 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17165 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17166 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17167 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17168 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17169 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17170 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17171 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17172 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17173 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17174 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17175 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17176 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17177 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17178 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17179 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17180 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17181 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17182 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17183 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17184 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17185 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17186 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17187 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17188 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17189 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17190 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17191 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17192 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17193 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17194 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17195 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17196 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17197 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17198 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17199 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17200 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17201 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17202 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17203 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17204 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17205 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17206 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17207 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17208 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17209 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17210 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17211 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17212 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17213 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17214 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17215 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17216 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17217 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17218 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17219 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17220 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17221 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17222 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17223 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17224 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17225 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17226 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17227 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17228 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17229 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17230 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17231 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17232 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17233 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17234 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17235 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17236 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17237 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17238 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17239 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17240 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 17241 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17242 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17243 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17244 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17245 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17246 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17247 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17248 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17249 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17250 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17251 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17252 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17253 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17254 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17255 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17256 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17257 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17258 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17259 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17260 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17261 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17262 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17263 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17264 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17265 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17266 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17267 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17268 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17269 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17270 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17271 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17272 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 17273 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17274 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 17275 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17276 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17277 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17278 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17279 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17280 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17281 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17282 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17283 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17284 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17285 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17286 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17287 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17288 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17289 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17290 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17291 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17292 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17293 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17294 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17295 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17296 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17297 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17298 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17299 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17300 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17301 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17302 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17303 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17304 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17305 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17306 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17307 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17308 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17309 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17310 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17311 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17312 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17313 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17314 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17315 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17316 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17317 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17318 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17319 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17320 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17321 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17322 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17323 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17324 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17325 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17326 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17327 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17328 :train_loss:0.18200860917568207 val_loss0.18120594322681427\n",
            "iteration 17329 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17330 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17331 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17332 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17333 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17334 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17335 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17336 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17337 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17338 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17339 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17340 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17341 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17342 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17343 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17344 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17345 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17346 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17347 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17348 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17349 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17350 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17351 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17352 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17353 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17354 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17355 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17356 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17357 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17358 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17359 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17360 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17361 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17362 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17363 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17364 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17365 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17366 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17367 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17368 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 17369 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17370 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17371 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17372 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17373 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17374 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17375 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17376 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17377 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17378 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17379 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17380 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17381 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17382 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17383 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17384 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17385 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17386 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17387 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17388 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17389 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17390 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17391 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17392 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17393 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17394 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17395 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17396 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17397 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17398 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17399 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17400 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17401 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17402 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17403 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17404 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17405 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17406 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17407 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17408 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17409 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17410 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17411 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17412 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17413 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17414 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17415 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17416 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17417 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17418 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17419 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17420 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17421 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17422 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17423 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17424 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17425 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17426 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17427 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17428 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17429 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17430 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17431 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17432 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17433 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17434 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17435 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17436 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17437 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17438 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17439 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17440 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17441 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17442 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17443 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17444 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17445 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17446 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17447 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17448 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17449 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17450 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17451 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17452 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17453 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17454 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17455 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17456 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17457 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17458 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17459 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17460 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17461 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17462 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17463 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17464 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17465 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17466 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17467 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17468 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17469 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17470 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17471 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17472 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17473 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17474 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17475 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17476 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17477 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17478 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17479 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17480 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17481 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17482 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17483 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17484 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17485 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17486 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17487 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17488 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17489 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17490 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 17491 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17492 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17493 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17494 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17495 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17496 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17497 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17498 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17499 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17500 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17501 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17502 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17503 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17504 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17505 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17506 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17507 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17508 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17509 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17510 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17511 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17512 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17513 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17514 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17515 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17516 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17517 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17518 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17519 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17520 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17521 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17522 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17523 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17524 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17525 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17526 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17527 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17528 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 17529 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17530 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17531 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17532 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17533 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17534 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17535 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17536 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17537 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17538 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17539 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17540 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17541 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17542 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17543 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17544 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17545 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17546 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17547 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17548 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17549 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17550 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17551 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17552 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17553 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17554 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17555 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17556 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17557 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17558 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17559 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17560 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 17561 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17562 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17563 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17564 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17565 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17566 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17567 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17568 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17569 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17570 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17571 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17572 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17573 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17574 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17575 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17576 :train_loss:0.18200863897800446 val_loss0.18120594322681427\n",
            "iteration 17577 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17578 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17579 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17580 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17581 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17582 :train_loss:0.18200863897800446 val_loss0.18120595812797546\n",
            "iteration 17583 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17584 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17585 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17586 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17587 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17588 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17589 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17590 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17591 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17592 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17593 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17594 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17595 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17596 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17597 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17598 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17599 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17600 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17601 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17602 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17603 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17604 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17605 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17606 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17607 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17608 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17609 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17610 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17611 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17612 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17613 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17614 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17615 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17616 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17617 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17618 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17619 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17620 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17621 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17622 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17623 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17624 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17625 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17626 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17627 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17628 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17629 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17630 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17631 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17632 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17633 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17634 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17635 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17636 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17637 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17638 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17639 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17640 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17641 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17642 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17643 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17644 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17645 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17646 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17647 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17648 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17649 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17650 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17651 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17652 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17653 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17654 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17655 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17656 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17657 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17658 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17659 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17660 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17661 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17662 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17663 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17664 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17665 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17666 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17667 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17668 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17669 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17670 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17671 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17672 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17673 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17674 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17675 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17676 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17677 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17678 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17679 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17680 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17681 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17682 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17683 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17684 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17685 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17686 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17687 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17688 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17689 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17690 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17691 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17692 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17693 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17694 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17695 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17696 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17697 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17698 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17699 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17700 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17701 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17702 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17703 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17704 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17705 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17706 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17707 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17708 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17709 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17710 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17711 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17712 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17713 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17714 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17715 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17716 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17717 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17718 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17719 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17720 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17721 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17722 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17723 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17724 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17725 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17726 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17727 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17728 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17729 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17730 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17731 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17732 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17733 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17734 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17735 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17736 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17737 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17738 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17739 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17740 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17741 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17742 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17743 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17744 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17745 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17746 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17747 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17748 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 17749 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17750 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17751 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17752 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17753 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17754 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17755 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17756 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17757 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17758 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17759 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17760 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17761 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17762 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17763 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17764 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17765 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17766 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17767 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17768 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17769 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17770 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17771 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17772 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17773 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17774 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17775 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17776 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17777 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17778 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17779 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17780 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17781 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17782 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17783 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17784 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17785 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17786 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17787 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17788 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17789 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17790 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17791 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17792 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17793 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17794 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17795 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17796 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17797 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17798 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17799 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17800 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17801 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17802 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17803 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17804 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17805 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17806 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17807 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17808 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17809 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17810 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17811 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17812 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17813 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17814 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17815 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17816 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17817 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17818 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17819 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17820 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17821 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17822 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17823 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17824 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17825 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17826 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17827 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17828 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17829 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 17830 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17831 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17832 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17833 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17834 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17835 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17836 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17837 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17838 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17839 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17840 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17841 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17842 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17843 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17844 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17845 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17846 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17847 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17848 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17849 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17850 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17851 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17852 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17853 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17854 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17855 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17856 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 17857 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17858 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17859 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17860 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17861 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17862 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17863 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17864 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17865 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17866 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 17867 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17868 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17869 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17870 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17871 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17872 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17873 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17874 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 17875 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17876 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17877 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17878 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17879 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17880 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17881 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17882 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17883 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17884 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17885 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17886 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17887 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17888 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 17889 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17890 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17891 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17892 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17893 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17894 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17895 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17896 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17897 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17898 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17899 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17900 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17901 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17902 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17903 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17904 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17905 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17906 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 17907 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17908 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17909 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17910 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17911 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17912 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17913 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17914 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17915 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17916 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17917 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17918 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17919 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17920 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17921 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17922 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17923 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17924 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17925 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17926 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17927 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17928 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17929 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17930 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17931 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17932 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17933 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17934 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17935 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17936 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17937 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17938 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17939 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17940 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17941 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17942 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17943 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17944 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17945 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17946 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17947 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17948 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17949 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17950 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17951 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17952 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17953 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17954 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17955 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17956 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17957 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17958 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17959 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17960 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17961 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17962 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17963 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17964 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17965 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17966 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17967 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17968 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17969 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17970 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17971 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17972 :train_loss:0.18200860917568207 val_loss0.18120592832565308\n",
            "iteration 17973 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17974 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17975 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17976 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17977 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17978 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17979 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17980 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17981 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17982 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17983 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17984 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17985 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17986 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17987 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17988 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17989 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17990 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17991 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17992 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 17993 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17994 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17995 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 17996 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 17997 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17998 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 17999 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18000 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18001 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18002 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18003 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18004 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18005 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18006 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18007 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18008 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18009 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18010 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18011 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18012 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18013 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18014 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18015 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18016 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18017 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18018 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18019 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18020 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18021 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18022 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18023 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18024 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18025 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18026 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18027 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18028 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18029 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18030 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18031 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18032 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18033 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18034 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18035 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18036 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18037 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18038 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18039 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18040 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18041 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18042 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18043 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18044 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18045 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18046 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18047 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18048 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18049 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18050 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18051 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18052 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18053 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18054 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18055 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18056 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18057 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18058 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18059 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18060 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18061 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18062 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18063 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18064 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18065 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18066 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18067 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18068 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18069 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18070 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18071 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18072 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18073 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18074 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18075 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18076 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18077 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18078 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18079 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18080 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18081 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18082 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18083 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18084 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18085 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18086 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18087 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 18088 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18089 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18090 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18091 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18092 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18093 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18094 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18095 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18096 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18097 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18098 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18099 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18100 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18101 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18102 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18103 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18104 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18105 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18106 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18107 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18108 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18109 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18110 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18111 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18112 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18113 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18114 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18115 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18116 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18117 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18118 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18119 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18120 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18121 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18122 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18123 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18124 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18125 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18126 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18127 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18128 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18129 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 18130 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18131 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18132 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18133 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18134 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18135 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18136 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18137 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18138 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18139 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18140 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18141 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18142 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18143 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18144 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18145 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18146 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18147 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18148 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18149 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18150 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18151 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18152 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18153 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18154 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18155 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18156 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18157 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18158 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18159 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18160 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18161 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18162 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18163 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18164 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18165 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18166 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18167 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18168 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18169 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18170 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18171 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18172 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18173 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18174 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18175 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18176 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18177 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18178 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18179 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18180 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18181 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18182 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18183 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18184 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18185 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18186 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18187 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18188 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18189 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18190 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18191 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18192 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18193 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18194 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18195 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18196 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18197 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18198 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18199 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18200 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18201 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18202 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18203 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18204 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18205 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18206 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18207 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18208 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18209 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18210 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18211 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18212 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18213 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18214 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18215 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18216 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18217 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18218 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18219 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18220 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18221 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18222 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18223 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18224 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18225 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18226 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18227 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18228 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18229 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18230 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18231 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18232 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18233 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18234 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18235 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18236 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18237 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18238 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18239 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18240 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18241 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18242 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18243 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18244 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18245 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18246 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18247 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18248 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18249 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18250 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18251 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18252 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18253 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 18254 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18255 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18256 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18257 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18258 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18259 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18260 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18261 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18262 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18263 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18264 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18265 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18266 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18267 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18268 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18269 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18270 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18271 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18272 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18273 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18274 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18275 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18276 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18277 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18278 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18279 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18280 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18281 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18282 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18283 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18284 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18285 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18286 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18287 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18288 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18289 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18290 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18291 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18292 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18293 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18294 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18295 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18296 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18297 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18298 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18299 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18300 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18301 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18302 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 18303 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18304 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18305 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18306 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18307 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18308 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18309 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18310 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18311 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18312 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18313 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18314 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18315 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18316 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18317 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18318 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18319 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18320 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18321 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18322 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18323 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18324 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18325 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18326 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18327 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18328 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18329 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18330 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18331 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18332 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18333 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18334 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18335 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18336 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18337 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18338 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18339 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18340 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18341 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18342 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18343 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18344 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18345 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18346 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18347 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18348 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18349 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18350 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18351 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18352 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18353 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18354 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18355 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18356 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18357 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18358 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18359 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18360 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18361 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18362 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18363 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18364 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18365 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18366 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18367 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18368 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18369 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18370 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18371 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18372 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18373 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18374 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18375 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18376 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18377 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18378 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18379 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18380 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18381 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18382 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18383 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18384 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18385 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18386 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18387 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18388 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18389 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18390 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18391 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18392 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18393 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18394 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18395 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18396 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18397 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18398 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18399 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18400 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18401 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18402 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18403 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18404 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18405 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18406 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18407 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18408 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18409 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18410 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18411 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18412 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18413 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18414 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18415 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18416 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18417 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18418 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18419 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18420 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18421 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18422 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18423 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18424 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18425 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18426 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18427 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18428 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18429 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18430 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18431 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18432 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18433 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18434 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18435 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18436 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18437 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18438 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18439 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18440 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18441 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18442 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18443 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18444 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18445 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18446 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18447 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18448 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18449 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18450 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18451 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18452 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18453 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18454 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18455 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18456 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18457 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18458 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18459 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18460 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18461 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18462 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18463 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18464 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18465 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18466 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18467 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18468 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18469 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18470 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18471 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18472 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18473 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18474 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18475 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18476 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18477 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18478 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18479 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18480 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18481 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18482 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18483 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18484 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18485 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18486 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18487 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18488 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18489 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18490 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18491 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18492 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18493 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18494 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18495 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18496 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18497 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18498 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18499 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18500 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18501 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18502 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18503 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18504 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18505 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18506 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18507 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18508 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18509 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18510 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18511 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18512 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18513 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18514 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18515 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18516 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18517 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18518 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18519 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18520 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18521 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18522 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18523 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18524 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18525 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18526 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18527 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18528 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18529 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18530 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18531 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18532 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18533 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18534 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18535 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18536 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18537 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18538 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18539 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18540 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18541 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18542 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18543 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18544 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18545 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18546 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18547 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18548 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18549 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18550 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18551 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18552 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18553 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18554 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18555 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18556 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18557 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18558 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18559 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18560 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18561 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18562 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18563 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18564 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18565 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18566 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18567 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18568 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18569 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18570 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18571 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18572 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18573 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18574 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18575 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18576 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18577 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18578 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18579 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18580 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18581 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18582 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18583 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18584 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18585 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18586 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18587 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18588 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18589 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18590 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18591 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18592 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18593 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18594 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18595 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18596 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18597 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18598 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18599 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18600 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18601 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18602 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18603 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18604 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18605 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18606 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18607 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18608 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18609 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18610 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18611 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18612 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18613 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18614 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18615 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18616 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18617 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18618 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18619 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18620 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18621 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18622 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18623 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18624 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18625 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18626 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18627 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18628 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18629 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18630 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18631 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18632 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18633 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18634 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18635 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18636 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18637 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18638 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18639 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18640 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18641 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18642 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18643 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18644 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18645 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18646 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18647 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18648 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18649 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18650 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18651 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18652 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18653 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18654 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18655 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18656 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18657 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18658 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18659 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18660 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18661 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18662 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18663 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18664 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18665 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18666 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18667 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18668 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18669 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18670 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18671 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18672 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18673 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18674 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18675 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18676 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18677 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18678 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18679 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18680 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18681 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18682 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18683 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18684 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18685 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18686 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18687 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18688 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18689 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18690 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18691 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18692 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18693 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18694 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18695 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18696 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18697 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18698 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18699 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18700 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18701 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18702 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 18703 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18704 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18705 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18706 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18707 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18708 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18709 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18710 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18711 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18712 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18713 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18714 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18715 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18716 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18717 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18718 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18719 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18720 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18721 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18722 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18723 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18724 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18725 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18726 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18727 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18728 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18729 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18730 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18731 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18732 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 18733 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18734 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18735 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18736 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18737 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18738 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18739 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18740 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18741 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18742 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18743 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18744 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18745 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18746 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18747 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18748 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18749 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18750 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18751 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18752 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18753 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18754 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18755 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18756 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18757 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18758 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18759 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18760 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18761 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18762 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18763 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18764 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18765 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18766 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18767 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18768 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18769 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18770 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18771 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18772 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18773 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18774 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18775 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18776 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18777 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18778 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18779 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18780 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18781 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18782 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18783 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18784 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18785 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18786 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18787 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18788 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18789 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18790 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18791 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18792 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18793 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18794 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18795 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18796 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18797 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18798 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18799 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18800 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18801 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18802 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18803 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18804 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18805 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18806 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18807 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18808 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18809 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18810 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18811 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18812 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18813 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18814 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18815 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18816 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18817 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18818 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18819 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18820 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18821 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18822 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18823 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18824 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18825 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18826 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18827 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18828 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18829 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18830 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18831 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18832 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18833 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18834 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18835 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18836 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18837 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18838 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18839 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18840 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18841 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18842 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18843 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18844 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18845 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18846 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18847 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18848 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18849 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18850 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18851 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18852 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18853 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18854 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18855 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18856 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18857 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18858 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18859 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18860 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18861 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18862 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18863 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18864 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18865 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18866 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18867 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18868 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18869 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18870 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 18871 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18872 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18873 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18874 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18875 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18876 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18877 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18878 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18879 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18880 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18881 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18882 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18883 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18884 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18885 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18886 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18887 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18888 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18889 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18890 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18891 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18892 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18893 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18894 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18895 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18896 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18897 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18898 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18899 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18900 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18901 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18902 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18903 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18904 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18905 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18906 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 18907 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18908 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18909 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18910 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18911 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18912 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18913 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18914 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18915 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18916 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18917 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18918 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18919 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18920 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18921 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18922 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18923 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18924 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18925 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18926 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18927 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18928 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18929 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18930 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18931 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18932 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18933 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18934 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18935 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18936 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18937 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18938 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18939 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18940 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18941 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18942 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18943 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18944 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18945 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18946 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18947 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18948 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18949 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18950 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18951 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18952 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18953 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18954 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18955 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18956 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18957 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18958 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18959 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18960 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18961 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18962 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18963 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18964 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18965 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18966 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18967 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18968 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 18969 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18970 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18971 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18972 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18973 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18974 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18975 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18976 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18977 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18978 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18979 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18980 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18981 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18982 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18983 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18984 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18985 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18986 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18987 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18988 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18989 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18990 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18991 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18992 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18993 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 18994 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18995 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18996 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18997 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18998 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 18999 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19000 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19001 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19002 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19003 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19004 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19005 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19006 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19007 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19008 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19009 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19010 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19011 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19012 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19013 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19014 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19015 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19016 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19017 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19018 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19019 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19020 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19021 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19022 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19023 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19024 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19025 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19026 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19027 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19028 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19029 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19030 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19031 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19032 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19033 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19034 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19035 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19036 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19037 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19038 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19039 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19040 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19041 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19042 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19043 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19044 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19045 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19046 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19047 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19048 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19049 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19050 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19051 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19052 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19053 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19054 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19055 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19056 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19057 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19058 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19059 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19060 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19061 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19062 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19063 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19064 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19065 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19066 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19067 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19068 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19069 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19070 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19071 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19072 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19073 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19074 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19075 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19076 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19077 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19078 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19079 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19080 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19081 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19082 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19083 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19084 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19085 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19086 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19087 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19088 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19089 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19090 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19091 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19092 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19093 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19094 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19095 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19096 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19097 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19098 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19099 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 19100 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19101 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19102 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19103 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19104 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19105 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19106 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19107 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19108 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19109 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19110 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19111 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19112 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19113 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19114 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19115 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19116 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19117 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19118 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19119 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19120 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19121 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19122 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19123 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19124 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19125 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19126 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19127 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19128 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19129 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19130 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19131 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19132 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19133 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19134 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19135 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19136 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19137 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19138 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19139 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19140 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19141 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19142 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19143 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19144 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19145 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19146 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19147 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19148 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19149 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19150 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19151 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19152 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19153 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19154 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19155 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19156 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19157 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19158 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19159 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19160 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19161 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19162 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19163 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19164 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19165 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19166 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19167 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19168 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19169 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19170 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19171 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19172 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19173 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19174 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19175 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19176 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19177 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19178 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19179 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19180 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19181 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19182 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19183 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19184 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19185 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19186 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19187 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19188 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19189 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19190 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19191 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19192 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19193 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19194 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19195 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19196 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19197 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19198 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19199 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19200 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19201 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19202 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19203 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19204 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19205 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19206 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19207 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19208 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19209 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19210 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19211 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19212 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19213 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19214 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19215 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19216 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19217 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19218 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19219 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19220 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19221 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19222 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19223 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19224 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19225 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19226 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19227 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19228 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19229 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19230 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19231 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19232 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19233 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19234 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19235 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19236 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19237 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19238 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19239 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19240 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19241 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19242 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19243 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19244 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19245 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19246 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19247 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19248 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19249 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19250 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19251 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19252 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19253 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19254 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19255 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19256 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19257 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19258 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19259 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19260 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19261 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19262 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19263 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19264 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19265 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19266 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19267 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19268 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19269 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19270 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19271 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19272 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19273 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19274 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19275 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19276 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19277 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19278 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19279 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19280 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19281 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19282 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19283 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19284 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19285 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19286 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19287 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19288 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19289 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19290 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19291 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19292 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19293 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19294 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19295 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19296 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19297 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19298 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19299 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19300 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19301 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19302 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19303 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19304 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19305 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19306 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19307 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19308 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19309 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19310 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19311 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19312 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19313 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19314 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19315 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19316 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19317 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19318 :train_loss:0.18200860917568207 val_loss0.1812058836221695\n",
            "iteration 19319 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19320 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19321 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19322 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19323 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19324 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19325 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19326 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19327 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19328 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19329 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 19330 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19331 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19332 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19333 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19334 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19335 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19336 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19337 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19338 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19339 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19340 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19341 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19342 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19343 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19344 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19345 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19346 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19347 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19348 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19349 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19350 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19351 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19352 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19353 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19354 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19355 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19356 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19357 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19358 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19359 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19360 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19361 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19362 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19363 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19364 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19365 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19366 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19367 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19368 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19369 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19370 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19371 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19372 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 19373 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19374 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19375 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19376 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19377 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19378 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19379 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19380 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19381 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19382 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19383 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19384 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19385 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19386 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19387 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19388 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19389 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19390 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19391 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19392 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19393 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19394 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19395 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19396 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19397 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19398 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19399 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19400 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19401 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19402 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19403 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19404 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19405 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19406 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19407 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19408 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19409 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19410 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19411 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19412 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19413 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19414 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19415 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19416 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19417 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19418 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19419 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19420 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19421 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19422 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19423 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19424 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19425 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19426 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19427 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19428 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19429 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19430 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19431 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19432 :train_loss:0.18200860917568207 val_loss0.1812058985233307\n",
            "iteration 19433 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19434 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19435 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19436 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19437 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19438 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19439 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19440 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19441 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19442 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19443 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19444 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19445 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19446 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19447 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19448 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19449 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19450 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19451 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19452 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19453 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19454 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19455 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19456 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19457 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19458 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19459 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19460 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19461 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19462 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19463 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19464 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19465 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19466 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19467 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19468 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19469 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19470 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19471 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19472 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19473 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19474 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19475 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19476 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19477 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19478 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19479 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19480 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19481 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19482 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19483 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19484 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19485 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19486 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19487 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19488 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19489 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19490 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19491 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19492 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19493 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19494 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19495 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19496 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19497 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19498 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19499 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19500 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19501 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 19502 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19503 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19504 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19505 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19506 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19507 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19508 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19509 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19510 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19511 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19512 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19513 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19514 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19515 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19516 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19517 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19518 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19519 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19520 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19521 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19522 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19523 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19524 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19525 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19526 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19527 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19528 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19529 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19530 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19531 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19532 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19533 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19534 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19535 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19536 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19537 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19538 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19539 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19540 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19541 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19542 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19543 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19544 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19545 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19546 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19547 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19548 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19549 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19550 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19551 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19552 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19553 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19554 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19555 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19556 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19557 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19558 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19559 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19560 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19561 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19562 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19563 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19564 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19565 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19566 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19567 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19568 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19569 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19570 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19571 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19572 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19573 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19574 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19575 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19576 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19577 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19578 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19579 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19580 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19581 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19582 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19583 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19584 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19585 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19586 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19587 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19588 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19589 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19590 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19591 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19592 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19593 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19594 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19595 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19596 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19597 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19598 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19599 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19600 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19601 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19602 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19603 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19604 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19605 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19606 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19607 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19608 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19609 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19610 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19611 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19612 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19613 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19614 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19615 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19616 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19617 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19618 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19619 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19620 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19621 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19622 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19623 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19624 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19625 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19626 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19627 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19628 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19629 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19630 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19631 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19632 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19633 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19634 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19635 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19636 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19637 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19638 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19639 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19640 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19641 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19642 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19643 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19644 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19645 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19646 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19647 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19648 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19649 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19650 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19651 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19652 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19653 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19654 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19655 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19656 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19657 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19658 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19659 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19660 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19661 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19662 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19663 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19664 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19665 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19666 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19667 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19668 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19669 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19670 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19671 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19672 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19673 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19674 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19675 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19676 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19677 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19678 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19679 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19680 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19681 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19682 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19683 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19684 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19685 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19686 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19687 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19688 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19689 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19690 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19691 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19692 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19693 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19694 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19695 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19696 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19697 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19698 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19699 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19700 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19701 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19702 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19703 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19704 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19705 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19706 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19707 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19708 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19709 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19710 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19711 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19712 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19713 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19714 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19715 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19716 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19717 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19718 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19719 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19720 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19721 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19722 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19723 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19724 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19725 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19726 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19727 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19728 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19729 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19730 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19731 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19732 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19733 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19734 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19735 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19736 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19737 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19738 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19739 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19740 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19741 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19742 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19743 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19744 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19745 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19746 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19747 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19748 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19749 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19750 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19751 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19752 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19753 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19754 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19755 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19756 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19757 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 19758 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19759 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19760 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19761 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19762 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19763 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19764 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19765 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19766 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19767 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19768 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19769 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19770 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19771 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19772 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19773 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19774 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19775 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19776 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19777 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19778 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19779 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19780 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19781 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19782 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19783 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19784 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19785 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19786 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19787 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19788 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19789 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19790 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19791 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19792 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19793 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19794 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19795 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19796 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19797 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19798 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19799 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19800 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19801 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19802 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19803 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19804 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19805 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19806 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19807 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19808 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19809 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19810 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19811 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19812 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19813 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19814 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19815 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19816 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19817 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19818 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19819 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19820 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19821 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19822 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19823 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19824 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19825 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19826 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19827 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19828 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19829 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19830 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19831 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19832 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19833 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19834 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19835 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19836 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19837 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19838 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19839 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19840 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19841 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19842 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19843 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19844 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19845 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19846 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19847 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19848 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19849 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19850 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19851 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19852 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19853 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19854 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19855 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19856 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19857 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19858 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19859 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19860 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19861 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19862 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19863 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19864 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19865 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19866 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19867 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19868 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19869 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19870 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19871 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19872 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19873 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19874 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19875 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19876 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19877 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19878 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19879 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19880 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19881 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19882 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19883 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 19884 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19885 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19886 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19887 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19888 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19889 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19890 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19891 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19892 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19893 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19894 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19895 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19896 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19897 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19898 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19899 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19900 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19901 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19902 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19903 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19904 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19905 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19906 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19907 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19908 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19909 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19910 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19911 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19912 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19913 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19914 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19915 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19916 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19917 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19918 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19919 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19920 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19921 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19922 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19923 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19924 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19925 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19926 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19927 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19928 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19929 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19930 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19931 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19932 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19933 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19934 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19935 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19936 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19937 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19938 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19939 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19940 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19941 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19942 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19943 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19944 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19945 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19946 :train_loss:0.18200863897800446 val_loss0.18120592832565308\n",
            "iteration 19947 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19948 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19949 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19950 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19951 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19952 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19953 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19954 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19955 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19956 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19957 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19958 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19959 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19960 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19961 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19962 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19963 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19964 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19965 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19966 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19967 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19968 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19969 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19970 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19971 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19972 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19973 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19974 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19975 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19976 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19977 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19978 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19979 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19980 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19981 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19982 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19983 :train_loss:0.18200863897800446 val_loss0.1812058687210083\n",
            "iteration 19984 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19985 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19986 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19987 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19988 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19989 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19990 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19991 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19992 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19993 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19994 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19995 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n",
            "iteration 19996 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19997 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19998 :train_loss:0.18200863897800446 val_loss0.1812058985233307\n",
            "iteration 19999 :train_loss:0.18200863897800446 val_loss0.1812058836221695\n"
          ]
        }
      ],
      "source": [
        "iterations=20000\n",
        "parameters, history=create_nn_model(train_X,train_Y, val_X, val_Y,100,iterations, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCTZ8zhwWL94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "6daeefb0-8dbf-41b5-afe0-d5cfec317d28"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(0,iterations),history[\"train_loss\"],'b')\n",
        "plt.plot(range(0,iterations),history[\"val_loss\"],'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuFai7TRWPm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5bb1fba-353e-4fc5-95a5-e4c342d1dd29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss of the model on the training data is: 0.18200863897800446\n",
            "loss of the model on the validation data is: 0.1812058836221695\n"
          ]
        }
      ],
      "source": [
        "predicted_train=predict(parameters, train_X)\n",
        "predicted_val=predict(parameters, val_X)\n",
        "\n",
        "print(\"loss of the model on the training data is:\", float(compute_loss(train_Y,predicted_train)))\n",
        "print(\"loss of the model on the validation data is:\", float(compute_loss(val_Y,predicted_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TIY2s4v-LJ5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RbAF4ts-Kln"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Paarg0d2-IYO"
      },
      "source": [
        "#Modify the code such that the create_nn method can accept and train a neural network with any given number of hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHuujjneJ4g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLF2Jhj3_xi5"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig2q-OSZ5eeB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywh1Ft7V_zpi"
      },
      "source": [
        "Initializing parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTDITzWe5eeI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "nx is the number of neurons in the input layer (i.e., the number of features in the dataset)\n",
        "nh is the number of neurons in the hidden layer\n",
        "ny is the number of neurons in the output layer (For this example we are using one nueron in the output layer so ny=1)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def initialize_parameters(nx,nh_array,ny):\n",
        "    #set tensorflow global random seed\n",
        "    tf.random.set_seed(1)\n",
        "    parameters = dict()\n",
        "    #initialize weights to small random numbers and biases to zeros for each layer. Note that weights and biases are defined as tensorflow variables instead of numpy arrays\n",
        "\n",
        "    for i, element in enumerate(nh_array):\n",
        "        parameters[\"W\" + str(i + 1)] = tf.Variable(tf.random.uniform(shape=(element, nx if i == 0 else nh_array[i - 1]), minval=-0.01, maxval=0.01, name=\"W\" + str(i + 1)))\n",
        "        parameters[\"b\" + str(i + 1)] = tf.Variable(tf.zeros(shape=(element, 1), name=\"b\" + str(i + 1)))\n",
        "\n",
        "    parameters[\"W\" + str(len(nh_array) + 1)] = tf.Variable(tf.random.uniform(shape=(ny, nh_array[-1]), minval=-0.01, maxval=0.01), name=\"W\" + str(len(nh_array) + 1))\n",
        "    parameters[\"b\" + str(len(nh_array) + 1)] = tf.Variable(tf.zeros(shape=(ny, 1)), name=\"b\" + str(len(nh_array) + 1))\n",
        "\n",
        "\n",
        "    parameters[\"Wo\"]=tf.Variable(tf.random.uniform(shape=(ny,nh_array[-1]), minval=-0.01, maxval=0.01), name=\"Wo\")\n",
        "    parameters[\"bo\"]=tf.Variable(tf.zeros(shape=(ny,1), name=\"bo\"))\n",
        "\n",
        "\n",
        "    #create a dictionary of network parameters\n",
        "    print(f\"Parameters :{parameters}\")\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH1QH4gw_1_s"
      },
      "source": [
        "Forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30co5zuC5eeS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In forward pass we do the computations in the computational graph. We cache the intermediate nodes we will later need in the backward pass\n",
        "\"\"\"\n",
        "def forward_pass(parameters,X,nh_array):\n",
        "    X= tf.cast(X, tf.float32)\n",
        "\n",
        "    Z = []\n",
        "    A = []\n",
        "\n",
        "\n",
        "    for i in range(len(nh_array)):\n",
        "        Z.append(tf.matmul(parameters[f\"W{i + 1}\"], X if i == 0 else A[i - 1]) + parameters[\"b\" + str(i + 1)])\n",
        "        A.append(Z[i])\n",
        "\n",
        "\n",
        "    Zo=tf.matmul(parameters[\"Wo\"],A[-1])+parameters[\"bo\"]\n",
        "\n",
        "    Yhat=Zo\n",
        "\n",
        "    return Yhat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMlaaoNB_4wp"
      },
      "source": [
        "Computing loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuMxR7eU5eeX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "n is the number of examples, y is a vector of actual/observed outputs and yhat is a vector of predicted outputs\n",
        "\"\"\"\n",
        "def compute_loss(Y,Yhat):\n",
        "    #compute binary cross entropy loss for each sample\n",
        "    #per_sample_losses= tf.multiply(Y , tf.math.log(Yhat)) + tf.multiply((1 - Y) , tf.math.log(1 - Yhat))\n",
        "    per_sample_losses = abs(Y-Yhat)\n",
        "\n",
        "    # take the average loss over all samples\n",
        "    loss=tf.reduce_mean(per_sample_losses)\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbRG_Yfw_7sS"
      },
      "source": [
        "Backward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoko95-k5eeg"
      },
      "outputs": [],
      "source": [
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients= tape.gradient(loss,parameters)\n",
        "    return gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03lWZ4NiABqT"
      },
      "source": [
        "Updating parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXpbKEE45eej"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "\n",
        "  for name in parameters:\n",
        "        if name.startswith(\"W\") or name.startswith(\"b\"):\n",
        "            if gradients[name] is not None:\n",
        "                parameters[name].assign_sub(learning_rate * gradients[name])\n",
        "\n",
        "  return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lldnl4gpADoV"
      },
      "source": [
        "Creating the Neural Network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNTD9QB95een"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Arguments: train_X: is the training dataset (features)\n",
        "           train_Y: is the vector of labels for training_X\n",
        "           val_X: is the vector of validation dataset (features)\n",
        "           val_y: is the vector of labels for val_X\n",
        "           nh: is the number of neurons in the hidden layer\n",
        "           num_iterations: The number of iterations of gradient descent\n",
        "\"\"\"\n",
        "def create_nn_model(train_X,train_Y,nh_array, val_X, val_Y, num_iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    Do some safety check on the data before proceeding.\n",
        "    train_X and val_X must have the same number of features (i.e., same number of rows)\n",
        "    train_X must have the same number of examples as train_Y (i.e., same number of columns )\n",
        "    val_X must have the same number of examples as Val_Y\n",
        "    \"\"\"\n",
        "    assert(train_X.shape[0]==val_X.shape[0]), \"train_X and val_X must have the same number of features\"\n",
        "    assert(train_X.shape[1]==train_Y.size), \"train_X and train_Y must have the same number of examples\"\n",
        "    assert(val_X.shape[1]==val_Y.size), \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "\n",
        "    #getting the number of features\n",
        "    nx=train_X.shape[0]\n",
        "\n",
        "    # We want to use this network for regression, so we have only one neuron in the output layer with no activation\n",
        "    ny=1\n",
        "\n",
        "    # initializing the parameteres\n",
        "    parameters=initialize_parameters(nx,nh_array,ny)\n",
        "\n",
        "\n",
        "    #initialize lists to store the training and valideation losses.\n",
        "    val_losses=[]\n",
        "    train_losses=[]\n",
        "\n",
        "    #run num_iterations of gradient descent\n",
        "    for i in range (0, num_iterations):\n",
        "\n",
        "      \"\"\"\n",
        "      run forward pass and compute the loss function on training and validation data.\n",
        "      Note that the forward pass and loss computations on the training data are enclosed inside the gradient tape context in order to build the computational graph.\n",
        "      The gradients are only computed on the training data and used to update the parameter. Validation data is not used for training and updating the parameters.\n",
        "      \"\"\"\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        train_Yhat=forward_pass(parameters,train_X,nh_array)\n",
        "        train_loss=compute_loss(train_Y,train_Yhat)\n",
        "\n",
        "\n",
        "      #compute validation loss\n",
        "      Yhat_val= forward_pass(parameters,val_X,nh_array)\n",
        "      val_loss=compute_loss(val_Y,Yhat_val)\n",
        "\n",
        "      #print the trianing loss and validation loss for each iteration.\n",
        "      print(\"iteration {} :train_loss:{} val_loss{}\".format(i,train_loss,val_loss))\n",
        "\n",
        "      # append the train and validation loss for the current iteration to the train_losses and val_losses\n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "      \"\"\"\n",
        "      Compute the gradients and update the parameters\n",
        "      \"\"\"\n",
        "      #compute the gradients on the training data\n",
        "      gradients=backward_pass(parameters,train_loss,tape)\n",
        "\n",
        "      # update the parameters\n",
        "      parameters=update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "    #create a dictionary history and put train_loss and validaiton_loss in it\n",
        "    history={\"val_loss\": val_losses,\n",
        "             \"train_loss\": train_losses}\n",
        "\n",
        "\n",
        "    #return the parameters and the history\n",
        "    return parameters, history\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_bwMc0oAHP9"
      },
      "source": [
        "Predicting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM4ufFb55eep"
      },
      "outputs": [],
      "source": [
        "def predict(parameters,X,nh_array, prob_threshold=0.5):\n",
        "    Yhat=forward_pass(parameters, X,nh_array)\n",
        "    predicted_label=Yhat\n",
        "    return predicted_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqY4IXa2AJBZ"
      },
      "source": [
        "Training the model and calculating loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri09AzW25ee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374bce92-b8f2-4e1a-de01-d29298f84616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters :{'W1': <tf.Variable 'Variable:0' shape=(10, 39) dtype=float32, numpy=\n",
            "array([[-6.69738278e-03,  8.02962482e-03,  2.61948351e-03,\n",
            "        -1.30907726e-03, -4.16121958e-03,  2.85004172e-03,\n",
            "         9.51571018e-03, -1.29801035e-03,  3.20203789e-03,\n",
            "         2.09791679e-03,  2.73262989e-03,  2.28897575e-03,\n",
            "         7.78669864e-03,  2.55523436e-03,  6.39501028e-04,\n",
            "        -9.48043540e-03, -1.18249934e-03, -4.94658481e-03,\n",
            "         7.72446394e-03,  7.74586946e-03,  5.74563257e-03,\n",
            "        -8.80896114e-03, -8.57812352e-03, -3.83170601e-03,\n",
            "        -4.97634616e-03,  8.16941075e-03, -5.70407137e-04,\n",
            "        -5.15229674e-03,  2.66007893e-03,  1.72062125e-03,\n",
            "         8.20023939e-03,  1.40287355e-03, -7.13085756e-05,\n",
            "         1.87830161e-03,  8.28661956e-04, -1.14165526e-03,\n",
            "        -4.15038830e-03,  4.67889290e-03,  8.39407742e-03],\n",
            "       [ 3.37037072e-03, -5.67808840e-03, -6.26932364e-03,\n",
            "        -1.85665861e-03, -9.80675872e-03, -6.88428991e-04,\n",
            "        -4.07627318e-03,  5.00245113e-03,  4.37939540e-04,\n",
            "         2.74270959e-03,  1.28414668e-03,  1.41541753e-03,\n",
            "        -9.48967412e-03,  2.23036017e-03, -3.78688797e-03,\n",
            "        -2.24070624e-04, -2.70921737e-04, -8.82955734e-03,\n",
            "         7.95520097e-03, -3.28079471e-03,  8.37526284e-03,\n",
            "         2.99540069e-03,  8.58114846e-03, -1.56721100e-03,\n",
            "        -8.53425730e-03,  5.29196206e-03,  4.09176573e-04,\n",
            "        -6.76724873e-03, -4.25193319e-03,  3.63815762e-03,\n",
            "        -4.80911741e-03, -8.12422298e-03, -8.64851288e-04,\n",
            "         6.12143427e-03,  3.74802854e-03,  4.29325551e-03,\n",
            "         4.34510969e-03, -6.16814848e-03,  3.49993911e-03],\n",
            "       [ 6.90868124e-03, -4.15478228e-03, -8.15170258e-03,\n",
            "         4.36883699e-03, -4.10630694e-03,  7.88266212e-03,\n",
            "         4.91267163e-03, -5.89071028e-03,  5.70347346e-03,\n",
            "         8.09295662e-03, -5.62610151e-03,  4.23594657e-03,\n",
            "        -4.41048620e-03, -8.12622253e-03, -7.38547789e-03,\n",
            "        -6.81489939e-03, -6.87340740e-03,  6.89084455e-03,\n",
            "         1.13019254e-03,  6.14548102e-04, -6.71425322e-03,\n",
            "        -6.73788507e-03, -8.84328038e-04, -3.01062316e-03,\n",
            "         3.12964153e-03,  8.41667317e-03, -7.85163883e-03,\n",
            "        -8.85869749e-03, -4.41091787e-03, -8.67751613e-03,\n",
            "        -7.56372698e-03, -8.41490924e-04, -3.75969894e-03,\n",
            "        -6.99176453e-04,  3.06217652e-03,  3.12233903e-03,\n",
            "        -9.58254561e-04,  7.92734139e-03, -3.40100285e-03],\n",
            "       [-5.59916021e-03, -5.09388186e-03, -2.69049639e-03,\n",
            "        -9.12708696e-03, -4.53631161e-03, -6.01835968e-03,\n",
            "        -3.29565024e-03, -4.41641547e-04, -4.70983237e-03,\n",
            "        -3.69511358e-03, -1.77263748e-03,  7.51164928e-03,\n",
            "        -4.85539204e-03,  5.09914663e-03,  6.68682158e-03,\n",
            "        -3.96101736e-04, -3.69991036e-03,  4.93710022e-03,\n",
            "        -6.79604989e-03, -8.07430595e-04,  4.46987618e-03,\n",
            "        -5.06336428e-03, -1.01685990e-03,  4.40043956e-03,\n",
            "        -8.80111940e-03,  1.58018153e-03, -9.12326109e-03,\n",
            "         7.11705722e-03, -9.45539214e-03, -3.53551609e-03,\n",
            "         6.69929199e-03,  4.26036119e-03, -3.27486498e-03,\n",
            "         6.52144849e-03, -9.30299703e-03, -8.94970633e-03,\n",
            "         2.91738473e-03, -7.76294619e-05,  9.27103311e-03],\n",
            "       [ 4.89618070e-03,  5.55758923e-03, -1.46559719e-03,\n",
            "         9.05740261e-03,  8.62774625e-03,  5.93185239e-03,\n",
            "        -7.72772264e-03, -1.46318879e-03, -1.48467813e-03,\n",
            "         7.45196268e-03,  2.28723045e-03, -9.69014131e-04,\n",
            "        -1.26115046e-03, -1.96697935e-03,  4.81063128e-03,\n",
            "         1.12607703e-03,  2.11579353e-03,  8.35224800e-03,\n",
            "         3.26331332e-03, -6.16069045e-03, -9.78535134e-03,\n",
            "        -1.00425724e-03, -5.03131608e-03,  5.30965347e-03,\n",
            "        -5.62648289e-03,  1.44022703e-03,  3.84377502e-03,\n",
            "        -3.73842474e-03,  4.46685031e-03, -3.70352529e-04,\n",
            "        -9.40167438e-03, -7.58605450e-03, -3.59865418e-03,\n",
            "        -1.09727122e-03, -7.77190458e-03, -2.47921934e-03,\n",
            "        -5.36907418e-03,  8.71697254e-03, -1.20326784e-03],\n",
            "       [ 1.60720851e-03, -7.92071782e-03, -2.45716423e-04,\n",
            "         6.47905283e-04,  8.22033733e-04,  9.32214782e-03,\n",
            "         3.19000706e-03, -3.25756520e-03,  2.05223355e-03,\n",
            "        -1.08999759e-03, -4.71819146e-03, -2.64223339e-03,\n",
            "        -6.01197220e-03,  1.02391466e-03, -9.33261123e-03,\n",
            "        -5.17955516e-03,  4.79871035e-03, -2.84755463e-03,\n",
            "         3.82250734e-03,  1.59378257e-03, -4.59527597e-04,\n",
            "        -9.57082957e-03,  1.05516985e-04, -1.55969895e-03,\n",
            "         9.51503031e-03,  2.83527188e-04, -5.48719615e-03,\n",
            "        -9.99145489e-03,  6.58915192e-03,  9.57350247e-03,\n",
            "         9.23975743e-03, -9.38327517e-03,  5.74993342e-03,\n",
            "        -4.66160290e-03, -1.82660855e-03,  8.76001827e-03,\n",
            "         2.14168802e-03, -6.54761679e-04,  3.68659291e-03],\n",
            "       [-3.87413241e-03,  9.10522975e-03,  3.29238642e-03,\n",
            "        -1.71627477e-03,  2.83350423e-03, -7.49378931e-03,\n",
            "         1.59690809e-03,  1.45762879e-03,  2.46348605e-03,\n",
            "        -6.22280827e-03,  7.65169412e-03, -9.63986106e-03,\n",
            "         1.82631239e-03,  7.56047666e-05, -7.60408631e-03,\n",
            "         5.31359389e-03, -7.48713966e-03,  9.52361338e-03,\n",
            "        -2.19598785e-03,  2.48033553e-04,  3.60132474e-03,\n",
            "        -6.90479483e-03, -4.06678673e-03,  4.80017159e-03,\n",
            "         7.11774081e-03, -9.74887144e-03,  4.57152352e-03,\n",
            "        -1.60061382e-03, -9.80741996e-03,  5.29875234e-03,\n",
            "         2.89780833e-03, -2.46652588e-03, -3.62911262e-04,\n",
            "         8.84389505e-04,  8.87229666e-03,  6.63161278e-04,\n",
            "         8.61108117e-03, -2.22366070e-03,  9.32469033e-04],\n",
            "       [-7.38494610e-03,  7.59481825e-03, -3.98822548e-03,\n",
            "        -6.83469046e-03, -8.93510319e-03,  7.44471513e-03,\n",
            "         2.86200084e-03, -1.55760255e-03, -3.50590935e-03,\n",
            "        -8.56391899e-03,  6.39812462e-03,  5.85527718e-03,\n",
            "         5.47527056e-03,  8.04715417e-03, -9.45683941e-03,\n",
            "        -4.36343905e-03, -6.93505025e-03,  2.42288597e-03,\n",
            "         4.32337075e-03,  2.79247947e-03, -2.25223042e-03,\n",
            "         6.14041276e-03, -6.13172539e-03, -5.00117755e-03,\n",
            "        -9.75532550e-03, -7.86510669e-03,  5.94679825e-03,\n",
            "         2.01574992e-03, -9.81015619e-03, -7.97916856e-03,\n",
            "        -2.28522066e-03, -8.47475044e-03,  1.59269571e-03,\n",
            "         9.79849882e-03, -6.79782405e-03, -6.38262462e-03,\n",
            "         4.48899064e-03,  1.79987401e-03, -6.34250604e-03],\n",
            "       [-3.56529932e-03, -6.71241432e-05, -1.27214193e-03,\n",
            "        -4.07252321e-03, -8.86775274e-03,  1.96436420e-03,\n",
            "        -6.78166607e-03,  6.42663240e-03, -3.29253171e-03,\n",
            "         7.61330314e-03, -4.14973963e-03,  5.93141839e-03,\n",
            "        -6.23278134e-03, -2.32704170e-03, -7.06240162e-03,\n",
            "         1.39446277e-03, -6.85621239e-03,  8.15330818e-03,\n",
            "         7.89371692e-03, -3.44671728e-03,  8.36024992e-03,\n",
            "         7.98644312e-03, -8.45726486e-03,  2.60693952e-04,\n",
            "        -8.38043634e-03, -9.41962935e-03, -3.18776583e-03,\n",
            "        -8.87691509e-03, -5.37733315e-03, -8.73918738e-03,\n",
            "         2.74594780e-03, -1.70324091e-03,  7.83134438e-03,\n",
            "         6.57377020e-03, -6.47581089e-03,  8.60181637e-03,\n",
            "        -9.90511384e-03, -9.90182161e-06, -3.11105233e-03],\n",
            "       [-4.75570420e-03, -7.21367588e-03, -2.35345215e-04,\n",
            "         1.70569867e-04, -4.58681351e-03,  4.42094356e-03,\n",
            "        -2.24264152e-03,  2.89596524e-03, -8.29462707e-03,\n",
            "        -7.60648400e-04,  3.30227613e-03,  2.87794136e-03,\n",
            "        -5.23550436e-04, -2.39315256e-03, -4.21624165e-03,\n",
            "         1.64958462e-03,  9.62850824e-03, -4.09315107e-03,\n",
            "        -6.52374700e-03,  4.99724410e-03,  8.20255652e-03,\n",
            "        -2.70478474e-03,  7.80115649e-03, -9.56358388e-03,\n",
            "         6.73235953e-03, -1.62888505e-03, -3.66302254e-03,\n",
            "        -9.86013841e-03,  6.35300577e-03,  1.93623081e-03,\n",
            "        -6.09081239e-03,  9.51535627e-03, -8.92557111e-03,\n",
            "        -8.30888748e-05,  3.35333776e-03, -8.43642652e-03,\n",
            "        -4.68436955e-03,  7.43224658e-03, -4.74914070e-03]], dtype=float32)>, 'b1': <tf.Variable 'Variable:0' shape=(10, 1) dtype=float32, numpy=\n",
            "array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32)>, 'W2': <tf.Variable 'Variable:0' shape=(5, 10) dtype=float32, numpy=\n",
            "array([[ 2.0214077e-04, -1.1293646e-03, -1.8293383e-03,  9.8498464e-03,\n",
            "         3.7732795e-03, -3.0830074e-03, -1.2786603e-03,  2.0212196e-03,\n",
            "        -8.6751487e-04,  5.0539589e-03],\n",
            "       [-6.2400554e-03,  9.7513944e-04,  9.7830500e-04,  9.9054072e-04,\n",
            "        -4.4021057e-03, -7.8208707e-03,  2.6182272e-03,  7.4763615e-03,\n",
            "         6.4526126e-04,  1.2219837e-03],\n",
            "       [-4.1213152e-03,  8.5696764e-03, -7.8052757e-03,  9.2708394e-03,\n",
            "         3.7186528e-03, -4.3700696e-03,  5.8938432e-03, -8.0397939e-03,\n",
            "         9.5219426e-03,  6.6666696e-03],\n",
            "       [ 7.2184280e-03,  3.3122590e-03, -6.8766354e-03,  8.8565294e-03,\n",
            "         6.0912520e-03,  9.6253064e-03,  9.6667353e-03, -9.1473479e-03,\n",
            "        -1.4772937e-03,  1.2220433e-03],\n",
            "       [-2.3291111e-03, -6.0663698e-03, -9.1140158e-05, -2.6300717e-03,\n",
            "        -5.5303811e-03,  2.9072780e-03, -5.7333801e-03,  6.1251298e-03,\n",
            "        -8.9273881e-03, -3.0544708e-03]], dtype=float32)>, 'b2': <tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
            "array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32)>, 'W3': <tf.Variable 'Variable:0' shape=(5, 5) dtype=float32, numpy=\n",
            "array([[ 0.00658555,  0.00268456,  0.00029455, -0.00217838,  0.00161806],\n",
            "       [-0.00903036, -0.00644736,  0.00409403, -0.00016183, -0.00307915],\n",
            "       [ 0.00015821, -0.00556652,  0.00697112,  0.00888176,  0.00052785],\n",
            "       [-0.00235718, -0.00440361, -0.00932453, -0.00136187, -0.00811649],\n",
            "       [ 0.00930304,  0.00513927,  0.00345505,  0.00067842, -0.00352931]],\n",
            "      dtype=float32)>, 'b3': <tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
            "array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32)>, 'W4': <tf.Variable 'Variable:0' shape=(4, 5) dtype=float32, numpy=\n",
            "array([[-5.2709579e-03, -5.2245329e-03,  7.2624534e-05,  5.4708002e-03,\n",
            "         2.2154953e-03],\n",
            "       [ 7.4435659e-03, -4.5430539e-03,  2.3663230e-03, -5.9496830e-03,\n",
            "         7.4015148e-03],\n",
            "       [-1.5188195e-04, -6.1402097e-05, -9.3976231e-03,  3.7152935e-03,\n",
            "         2.6314445e-03],\n",
            "       [-5.8432268e-03, -1.2175348e-03, -6.2209032e-03,  5.2683139e-03,\n",
            "        -5.2284729e-04]], dtype=float32)>, 'b4': <tf.Variable 'Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
            "array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32)>, 'W5': <tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
            "array([[ 0.00469564, -0.00715575, -0.00873233, -0.006216  ],\n",
            "       [ 0.00596342,  0.00408493, -0.00182131,  0.0002855 ],\n",
            "       [-0.00855665, -0.00309367,  0.00501224, -0.00693357]],\n",
            "      dtype=float32)>, 'b5': <tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
            "array([[0.],\n",
            "       [0.],\n",
            "       [0.]], dtype=float32)>, 'W6': <tf.Variable 'W6:0' shape=(1, 3) dtype=float32, numpy=array([[-0.00334799,  0.0029167 ,  0.00399811]], dtype=float32)>, 'b6': <tf.Variable 'b6:0' shape=(1, 1) dtype=float32, numpy=array([[0.]], dtype=float32)>, 'Wo': <tf.Variable 'Wo:0' shape=(1, 3) dtype=float32, numpy=array([[0.00984333, 0.00978673, 0.00140982]], dtype=float32)>, 'bo': <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[0.]], dtype=float32)>}\n",
            "iteration 0 :train_loss:0.8708698153495789 val_loss0.8687447905540466\n",
            "iteration 1 :train_loss:0.8694526553153992 val_loss0.8673103451728821\n",
            "iteration 2 :train_loss:0.868041455745697 val_loss0.8658824563026428\n",
            "iteration 3 :train_loss:0.8666507005691528 val_loss0.8644759058952332\n",
            "iteration 4 :train_loss:0.8652749061584473 val_loss0.8630854487419128\n",
            "iteration 5 :train_loss:0.8639020323753357 val_loss0.8616980910301208\n",
            "iteration 6 :train_loss:0.8625324964523315 val_loss0.8603139519691467\n",
            "iteration 7 :train_loss:0.8611668944358826 val_loss0.8589340448379517\n",
            "iteration 8 :train_loss:0.8598067760467529 val_loss0.8575593829154968\n",
            "iteration 9 :train_loss:0.8584488034248352 val_loss0.8561871647834778\n",
            "iteration 10 :train_loss:0.8570917844772339 val_loss0.8548164367675781\n",
            "iteration 11 :train_loss:0.8557369709014893 val_loss0.8534478545188904\n",
            "iteration 12 :train_loss:0.8543877601623535 val_loss0.8520849943161011\n",
            "iteration 13 :train_loss:0.8530413508415222 val_loss0.8507249355316162\n",
            "iteration 14 :train_loss:0.8516973257064819 val_loss0.8493677377700806\n",
            "iteration 15 :train_loss:0.8503550887107849 val_loss0.8480125665664673\n",
            "iteration 16 :train_loss:0.8490143418312073 val_loss0.8466594219207764\n",
            "iteration 17 :train_loss:0.8476763367652893 val_loss0.8453084230422974\n",
            "iteration 18 :train_loss:0.8463402986526489 val_loss0.8439593315124512\n",
            "iteration 19 :train_loss:0.8450064659118652 val_loss0.8426123261451721\n",
            "iteration 20 :train_loss:0.8436744213104248 val_loss0.8412672877311707\n",
            "iteration 21 :train_loss:0.8423448801040649 val_loss0.8399247527122498\n",
            "iteration 22 :train_loss:0.8410188555717468 val_loss0.8385857343673706\n",
            "iteration 23 :train_loss:0.839695394039154 val_loss0.8372495174407959\n",
            "iteration 24 :train_loss:0.8383746147155762 val_loss0.8359160423278809\n",
            "iteration 25 :train_loss:0.8370565176010132 val_loss0.8345847725868225\n",
            "iteration 26 :train_loss:0.8357415199279785 val_loss0.8332563042640686\n",
            "iteration 27 :train_loss:0.8344299793243408 val_loss0.8319316506385803\n",
            "iteration 28 :train_loss:0.8331207633018494 val_loss0.8306095600128174\n",
            "iteration 29 :train_loss:0.8318135142326355 val_loss0.8292897343635559\n",
            "iteration 30 :train_loss:0.8305078148841858 val_loss0.8279719948768616\n",
            "iteration 31 :train_loss:0.8292039632797241 val_loss0.8266561031341553\n",
            "iteration 32 :train_loss:0.8279029726982117 val_loss0.8253434896469116\n",
            "iteration 33 :train_loss:0.8266040682792664 val_loss0.8240337371826172\n",
            "iteration 34 :train_loss:0.8253070116043091 val_loss0.8227260708808899\n",
            "iteration 35 :train_loss:0.8240127563476562 val_loss0.8214210867881775\n",
            "iteration 36 :train_loss:0.8227236866950989 val_loss0.8201208710670471\n",
            "iteration 37 :train_loss:0.8214375376701355 val_loss0.818823516368866\n",
            "iteration 38 :train_loss:0.8201565146446228 val_loss0.8175315260887146\n",
            "iteration 39 :train_loss:0.8188844919204712 val_loss0.8162496089935303\n",
            "iteration 40 :train_loss:0.817619800567627 val_loss0.8149749040603638\n",
            "iteration 41 :train_loss:0.8163648843765259 val_loss0.8137105107307434\n",
            "iteration 42 :train_loss:0.8151158690452576 val_loss0.8124523758888245\n",
            "iteration 43 :train_loss:0.8138697743415833 val_loss0.8111971616744995\n",
            "iteration 44 :train_loss:0.8126264214515686 val_loss0.8099442720413208\n",
            "iteration 45 :train_loss:0.8113868832588196 val_loss0.8086943626403809\n",
            "iteration 46 :train_loss:0.8101516366004944 val_loss0.8074482083320618\n",
            "iteration 47 :train_loss:0.808921217918396 val_loss0.8062067627906799\n",
            "iteration 48 :train_loss:0.8076958060264587 val_loss0.8049710988998413\n",
            "iteration 49 :train_loss:0.8064736127853394 val_loss0.803738534450531\n",
            "iteration 50 :train_loss:0.8052554726600647 val_loss0.8025105595588684\n",
            "iteration 51 :train_loss:0.8040415644645691 val_loss0.8012881875038147\n",
            "iteration 52 :train_loss:0.8028310537338257 val_loss0.8000696897506714\n",
            "iteration 53 :train_loss:0.80162513256073 val_loss0.7988560795783997\n",
            "iteration 54 :train_loss:0.8004246950149536 val_loss0.7976484298706055\n",
            "iteration 55 :train_loss:0.7992278933525085 val_loss0.7964444756507874\n",
            "iteration 56 :train_loss:0.7980380654335022 val_loss0.7952474355697632\n",
            "iteration 57 :train_loss:0.7968557476997375 val_loss0.7940579652786255\n",
            "iteration 58 :train_loss:0.795682430267334 val_loss0.7928772568702698\n",
            "iteration 59 :train_loss:0.794518768787384 val_loss0.7917059063911438\n",
            "iteration 60 :train_loss:0.7933610081672668 val_loss0.7905407547950745\n",
            "iteration 61 :train_loss:0.7922075390815735 val_loss0.789379894733429\n",
            "iteration 62 :train_loss:0.7910585999488831 val_loss0.7882236242294312\n",
            "iteration 63 :train_loss:0.7899147272109985 val_loss0.7870718836784363\n",
            "iteration 64 :train_loss:0.7887786030769348 val_loss0.7859282493591309\n",
            "iteration 65 :train_loss:0.7876508235931396 val_loss0.7847927808761597\n",
            "iteration 66 :train_loss:0.7865318655967712 val_loss0.7836651802062988\n",
            "iteration 67 :train_loss:0.7854205965995789 val_loss0.782545268535614\n",
            "iteration 68 :train_loss:0.7843134999275208 val_loss0.7814304828643799\n",
            "iteration 69 :train_loss:0.7832123637199402 val_loss0.780322253704071\n",
            "iteration 70 :train_loss:0.7821171283721924 val_loss0.779219925403595\n",
            "iteration 71 :train_loss:0.7810282707214355 val_loss0.7781233191490173\n",
            "iteration 72 :train_loss:0.7799484133720398 val_loss0.7770348191261292\n",
            "iteration 73 :train_loss:0.7788764238357544 val_loss0.7759544253349304\n",
            "iteration 74 :train_loss:0.7778116464614868 val_loss0.7748813033103943\n",
            "iteration 75 :train_loss:0.77675461769104 val_loss0.7738154530525208\n",
            "iteration 76 :train_loss:0.7757092714309692 val_loss0.7727606296539307\n",
            "iteration 77 :train_loss:0.7746700048446655 val_loss0.7717118263244629\n",
            "iteration 78 :train_loss:0.7736376523971558 val_loss0.7706699371337891\n",
            "iteration 79 :train_loss:0.7726142406463623 val_loss0.7696362733840942\n",
            "iteration 80 :train_loss:0.7715986371040344 val_loss0.7686096429824829\n",
            "iteration 81 :train_loss:0.7705902457237244 val_loss0.7675902247428894\n",
            "iteration 82 :train_loss:0.769586443901062 val_loss0.7665751576423645\n",
            "iteration 83 :train_loss:0.7685955762863159 val_loss0.7655752897262573\n",
            "iteration 84 :train_loss:0.7676138281822205 val_loss0.7645841836929321\n",
            "iteration 85 :train_loss:0.7666406631469727 val_loss0.7636016607284546\n",
            "iteration 86 :train_loss:0.7656776309013367 val_loss0.7626294493675232\n",
            "iteration 87 :train_loss:0.7647222876548767 val_loss0.7616654634475708\n",
            "iteration 88 :train_loss:0.7637783288955688 val_loss0.7607128024101257\n",
            "iteration 89 :train_loss:0.7628437876701355 val_loss0.7597695589065552\n",
            "iteration 90 :train_loss:0.7619181275367737 val_loss0.7588355541229248\n",
            "iteration 91 :train_loss:0.7610042095184326 val_loss0.7579137086868286\n",
            "iteration 92 :train_loss:0.7601026296615601 val_loss0.75700443983078\n",
            "iteration 93 :train_loss:0.7592074871063232 val_loss0.7561011910438538\n",
            "iteration 94 :train_loss:0.758320152759552 val_loss0.755204975605011\n",
            "iteration 95 :train_loss:0.7574519515037537 val_loss0.7543269991874695\n",
            "iteration 96 :train_loss:0.7565895915031433 val_loss0.7534551024436951\n",
            "iteration 97 :train_loss:0.7557396292686462 val_loss0.7525961399078369\n",
            "iteration 98 :train_loss:0.7548967599868774 val_loss0.7517445683479309\n",
            "iteration 99 :train_loss:0.7540608644485474 val_loss0.7509013414382935\n",
            "iteration 100 :train_loss:0.7532337307929993 val_loss0.750066876411438\n",
            "iteration 101 :train_loss:0.7524188756942749 val_loss0.749245285987854\n",
            "iteration 102 :train_loss:0.7516185641288757 val_loss0.7484378218650818\n",
            "iteration 103 :train_loss:0.750836968421936 val_loss0.7476492524147034\n",
            "iteration 104 :train_loss:0.7500686645507812 val_loss0.7468736171722412\n",
            "iteration 105 :train_loss:0.7493105530738831 val_loss0.7461080551147461\n",
            "iteration 106 :train_loss:0.7485588192939758 val_loss0.7453491687774658\n",
            "iteration 107 :train_loss:0.7478150725364685 val_loss0.7445991635322571\n",
            "iteration 108 :train_loss:0.7470767498016357 val_loss0.7438549995422363\n",
            "iteration 109 :train_loss:0.7463452816009521 val_loss0.7431174516677856\n",
            "iteration 110 :train_loss:0.7456241846084595 val_loss0.7423893213272095\n",
            "iteration 111 :train_loss:0.7449135184288025 val_loss0.741671621799469\n",
            "iteration 112 :train_loss:0.7442174553871155 val_loss0.7409685850143433\n",
            "iteration 113 :train_loss:0.7435335516929626 val_loss0.7402777075767517\n",
            "iteration 114 :train_loss:0.74285888671875 val_loss0.7395962476730347\n",
            "iteration 115 :train_loss:0.742192268371582 val_loss0.7389228940010071\n",
            "iteration 116 :train_loss:0.7415380477905273 val_loss0.7382614612579346\n",
            "iteration 117 :train_loss:0.7409051656723022 val_loss0.7376208305358887\n",
            "iteration 118 :train_loss:0.7402846217155457 val_loss0.7369924187660217\n",
            "iteration 119 :train_loss:0.739679217338562 val_loss0.7363781929016113\n",
            "iteration 120 :train_loss:0.7390807271003723 val_loss0.7357705235481262\n",
            "iteration 121 :train_loss:0.7384896874427795 val_loss0.7351701855659485\n",
            "iteration 122 :train_loss:0.7379071712493896 val_loss0.7345786094665527\n",
            "iteration 123 :train_loss:0.7373356819152832 val_loss0.7339983582496643\n",
            "iteration 124 :train_loss:0.7367684841156006 val_loss0.7334230542182922\n",
            "iteration 125 :train_loss:0.7362120151519775 val_loss0.7328585982322693\n",
            "iteration 126 :train_loss:0.7356694936752319 val_loss0.732308030128479\n",
            "iteration 127 :train_loss:0.7351340651512146 val_loss0.7317639589309692\n",
            "iteration 128 :train_loss:0.7346096634864807 val_loss0.7312310338020325\n",
            "iteration 129 :train_loss:0.7341092824935913 val_loss0.730721652507782\n",
            "iteration 130 :train_loss:0.7336217164993286 val_loss0.730225682258606\n",
            "iteration 131 :train_loss:0.7331432700157166 val_loss0.729738712310791\n",
            "iteration 132 :train_loss:0.7326743602752686 val_loss0.729261577129364\n",
            "iteration 133 :train_loss:0.7322197556495667 val_loss0.7287991642951965\n",
            "iteration 134 :train_loss:0.7317720651626587 val_loss0.7283446788787842\n",
            "iteration 135 :train_loss:0.7313400506973267 val_loss0.727907121181488\n",
            "iteration 136 :train_loss:0.7309219241142273 val_loss0.7274842858314514\n",
            "iteration 137 :train_loss:0.7305219769477844 val_loss0.727080225944519\n",
            "iteration 138 :train_loss:0.7301309704780579 val_loss0.7266848087310791\n",
            "iteration 139 :train_loss:0.7297433018684387 val_loss0.7262927293777466\n",
            "iteration 140 :train_loss:0.7293625473976135 val_loss0.7259070873260498\n",
            "iteration 141 :train_loss:0.7289845943450928 val_loss0.7255245447158813\n",
            "iteration 142 :train_loss:0.7286107540130615 val_loss0.7251462936401367\n",
            "iteration 143 :train_loss:0.7282413244247437 val_loss0.7247725129127502\n",
            "iteration 144 :train_loss:0.7278806567192078 val_loss0.7244076728820801\n",
            "iteration 145 :train_loss:0.7275347113609314 val_loss0.7240567803382874\n",
            "iteration 146 :train_loss:0.7271952629089355 val_loss0.7237117886543274\n",
            "iteration 147 :train_loss:0.7268668413162231 val_loss0.72337806224823\n",
            "iteration 148 :train_loss:0.7265443205833435 val_loss0.7230501770973206\n",
            "iteration 149 :train_loss:0.7262293696403503 val_loss0.7227297425270081\n",
            "iteration 150 :train_loss:0.7259179353713989 val_loss0.7224119901657104\n",
            "iteration 151 :train_loss:0.7256110906600952 val_loss0.7220986485481262\n",
            "iteration 152 :train_loss:0.7253090143203735 val_loss0.7217898368835449\n",
            "iteration 153 :train_loss:0.725013256072998 val_loss0.7214869260787964\n",
            "iteration 154 :train_loss:0.7247217297554016 val_loss0.7211877107620239\n",
            "iteration 155 :train_loss:0.7244328260421753 val_loss0.7208910584449768\n",
            "iteration 156 :train_loss:0.724146842956543 val_loss0.7205972075462341\n",
            "iteration 157 :train_loss:0.7238659262657166 val_loss0.7203085422515869\n",
            "iteration 158 :train_loss:0.7235904932022095 val_loss0.7200256586074829\n",
            "iteration 159 :train_loss:0.7233169674873352 val_loss0.7197450399398804\n",
            "iteration 160 :train_loss:0.7230460047721863 val_loss0.7194673418998718\n",
            "iteration 161 :train_loss:0.7227779030799866 val_loss0.7191925048828125\n",
            "iteration 162 :train_loss:0.7225189208984375 val_loss0.7189268469810486\n",
            "iteration 163 :train_loss:0.7222672700881958 val_loss0.7186686992645264\n",
            "iteration 164 :train_loss:0.7220234870910645 val_loss0.718419075012207\n",
            "iteration 165 :train_loss:0.7217825651168823 val_loss0.7181723117828369\n",
            "iteration 166 :train_loss:0.7215454578399658 val_loss0.7179299592971802\n",
            "iteration 167 :train_loss:0.7213156819343567 val_loss0.7176957130432129\n",
            "iteration 168 :train_loss:0.7210975885391235 val_loss0.717473566532135\n",
            "iteration 169 :train_loss:0.7208918333053589 val_loss0.7172636985778809\n",
            "iteration 170 :train_loss:0.7206910848617554 val_loss0.7170584797859192\n",
            "iteration 171 :train_loss:0.720493733882904 val_loss0.716856837272644\n",
            "iteration 172 :train_loss:0.7203012704849243 val_loss0.7166602611541748\n",
            "iteration 173 :train_loss:0.7201109528541565 val_loss0.7164656519889832\n",
            "iteration 174 :train_loss:0.7199227213859558 val_loss0.7162734270095825\n",
            "iteration 175 :train_loss:0.7197359204292297 val_loss0.7160823941230774\n",
            "iteration 176 :train_loss:0.7195510268211365 val_loss0.7158931493759155\n",
            "iteration 177 :train_loss:0.7193683981895447 val_loss0.7157061100006104\n",
            "iteration 178 :train_loss:0.7191873788833618 val_loss0.7155210971832275\n",
            "iteration 179 :train_loss:0.7190095782279968 val_loss0.715339720249176\n",
            "iteration 180 :train_loss:0.7188334465026855 val_loss0.7151604294776917\n",
            "iteration 181 :train_loss:0.7186588048934937 val_loss0.7149823904037476\n",
            "iteration 182 :train_loss:0.7184876203536987 val_loss0.7148078680038452\n",
            "iteration 183 :train_loss:0.7183186411857605 val_loss0.71463543176651\n",
            "iteration 184 :train_loss:0.7181537747383118 val_loss0.71446692943573\n",
            "iteration 185 :train_loss:0.7179915904998779 val_loss0.7143008708953857\n",
            "iteration 186 :train_loss:0.7178321480751038 val_loss0.7141376733779907\n",
            "iteration 187 :train_loss:0.7176789045333862 val_loss0.7139811515808105\n",
            "iteration 188 :train_loss:0.7175281047821045 val_loss0.7138271331787109\n",
            "iteration 189 :train_loss:0.717382550239563 val_loss0.7136777639389038\n",
            "iteration 190 :train_loss:0.7172453999519348 val_loss0.7135367393493652\n",
            "iteration 191 :train_loss:0.7171121835708618 val_loss0.7133995294570923\n",
            "iteration 192 :train_loss:0.7169829607009888 val_loss0.7132664918899536\n",
            "iteration 193 :train_loss:0.7168557643890381 val_loss0.7131362557411194\n",
            "iteration 194 :train_loss:0.7167313098907471 val_loss0.7130089402198792\n",
            "iteration 195 :train_loss:0.7166081666946411 val_loss0.7128831148147583\n",
            "iteration 196 :train_loss:0.7164860963821411 val_loss0.7127582430839539\n",
            "iteration 197 :train_loss:0.7163655161857605 val_loss0.7126347422599792\n",
            "iteration 198 :train_loss:0.7162479162216187 val_loss0.7125142812728882\n",
            "iteration 199 :train_loss:0.7161315083503723 val_loss0.7123949527740479\n",
            "iteration 200 :train_loss:0.716016948223114 val_loss0.7122777104377747\n",
            "iteration 201 :train_loss:0.7159043550491333 val_loss0.7121623754501343\n",
            "iteration 202 :train_loss:0.7157939672470093 val_loss0.712049663066864\n",
            "iteration 203 :train_loss:0.7156850695610046 val_loss0.7119384407997131\n",
            "iteration 204 :train_loss:0.7155784368515015 val_loss0.7118291258811951\n",
            "iteration 205 :train_loss:0.7154744863510132 val_loss0.7117224335670471\n",
            "iteration 206 :train_loss:0.7153745889663696 val_loss0.7116196751594543\n",
            "iteration 207 :train_loss:0.7152762413024902 val_loss0.7115184664726257\n",
            "iteration 208 :train_loss:0.7151787877082825 val_loss0.7114183902740479\n",
            "iteration 209 :train_loss:0.7150830626487732 val_loss0.7113202214241028\n",
            "iteration 210 :train_loss:0.7149893641471863 val_loss0.7112241983413696\n",
            "iteration 211 :train_loss:0.7148967981338501 val_loss0.7111294865608215\n",
            "iteration 212 :train_loss:0.7148051857948303 val_loss0.7110356688499451\n",
            "iteration 213 :train_loss:0.7147156000137329 val_loss0.7109437584877014\n",
            "iteration 214 :train_loss:0.7146270275115967 val_loss0.7108528017997742\n",
            "iteration 215 :train_loss:0.7145397067070007 val_loss0.7107633948326111\n",
            "iteration 216 :train_loss:0.7144540548324585 val_loss0.7106754779815674\n",
            "iteration 217 :train_loss:0.7143697142601013 val_loss0.7105889916419983\n",
            "iteration 218 :train_loss:0.7142869830131531 val_loss0.7105039954185486\n",
            "iteration 219 :train_loss:0.7142050862312317 val_loss0.7104203104972839\n",
            "iteration 220 :train_loss:0.7141242623329163 val_loss0.7103372812271118\n",
            "iteration 221 :train_loss:0.7140442132949829 val_loss0.7102553248405457\n",
            "iteration 222 :train_loss:0.7139660120010376 val_loss0.7101747989654541\n",
            "iteration 223 :train_loss:0.7138896584510803 val_loss0.7100966572761536\n",
            "iteration 224 :train_loss:0.7138146758079529 val_loss0.7100198268890381\n",
            "iteration 225 :train_loss:0.7137421369552612 val_loss0.709945559501648\n",
            "iteration 226 :train_loss:0.7136704325675964 val_loss0.7098724246025085\n",
            "iteration 227 :train_loss:0.713599443435669 val_loss0.7097998261451721\n",
            "iteration 228 :train_loss:0.7135295867919922 val_loss0.7097282409667969\n",
            "iteration 229 :train_loss:0.7134609222412109 val_loss0.7096578478813171\n",
            "iteration 230 :train_loss:0.7133933305740356 val_loss0.7095884680747986\n",
            "iteration 231 :train_loss:0.7133269309997559 val_loss0.7095203995704651\n",
            "iteration 232 :train_loss:0.7132620215415955 val_loss0.7094537615776062\n",
            "iteration 233 :train_loss:0.7131984233856201 val_loss0.7093886137008667\n",
            "iteration 234 :train_loss:0.7131364345550537 val_loss0.7093251943588257\n",
            "iteration 235 :train_loss:0.7130753993988037 val_loss0.7092626690864563\n",
            "iteration 236 :train_loss:0.713015079498291 val_loss0.709200918674469\n",
            "iteration 237 :train_loss:0.7129555344581604 val_loss0.7091399431228638\n",
            "iteration 238 :train_loss:0.7128975987434387 val_loss0.7090808153152466\n",
            "iteration 239 :train_loss:0.7128418684005737 val_loss0.7090243101119995\n",
            "iteration 240 :train_loss:0.7127988934516907 val_loss0.7089804410934448\n",
            "iteration 241 :train_loss:0.712756872177124 val_loss0.7089374661445618\n",
            "iteration 242 :train_loss:0.7127159237861633 val_loss0.7088953256607056\n",
            "iteration 243 :train_loss:0.7126757502555847 val_loss0.7088540196418762\n",
            "iteration 244 :train_loss:0.7126375436782837 val_loss0.7088143825531006\n",
            "iteration 245 :train_loss:0.712600827217102 val_loss0.7087762951850891\n",
            "iteration 246 :train_loss:0.7125648856163025 val_loss0.7087391018867493\n",
            "iteration 247 :train_loss:0.7125294804573059 val_loss0.7087027430534363\n",
            "iteration 248 :train_loss:0.7124952077865601 val_loss0.7086673378944397\n",
            "iteration 249 :train_loss:0.712461531162262 val_loss0.7086328268051147\n",
            "iteration 250 :train_loss:0.7124294638633728 val_loss0.7085996866226196\n",
            "iteration 251 :train_loss:0.7124027609825134 val_loss0.7085718512535095\n",
            "iteration 252 :train_loss:0.7123781442642212 val_loss0.7085460424423218\n",
            "iteration 253 :train_loss:0.7123538851737976 val_loss0.7085205316543579\n",
            "iteration 254 :train_loss:0.7123309373855591 val_loss0.7084963321685791\n",
            "iteration 255 :train_loss:0.7123087644577026 val_loss0.7084729075431824\n",
            "iteration 256 :train_loss:0.7122876048088074 val_loss0.7084506154060364\n",
            "iteration 257 :train_loss:0.7122675776481628 val_loss0.7084295153617859\n",
            "iteration 258 :train_loss:0.7122480869293213 val_loss0.7084090113639832\n",
            "iteration 259 :train_loss:0.71222984790802 val_loss0.708389937877655\n",
            "iteration 260 :train_loss:0.7122119069099426 val_loss0.7083709836006165\n",
            "iteration 261 :train_loss:0.7121939063072205 val_loss0.708352267742157\n",
            "iteration 262 :train_loss:0.7121763229370117 val_loss0.7083338499069214\n",
            "iteration 263 :train_loss:0.7121593952178955 val_loss0.7083162069320679\n",
            "iteration 264 :train_loss:0.7121426463127136 val_loss0.7082986235618591\n",
            "iteration 265 :train_loss:0.7121263742446899 val_loss0.7082818150520325\n",
            "iteration 266 :train_loss:0.7121111750602722 val_loss0.7082659006118774\n",
            "iteration 267 :train_loss:0.7120961546897888 val_loss0.7082502245903015\n",
            "iteration 268 :train_loss:0.7120814919471741 val_loss0.7082347273826599\n",
            "iteration 269 :train_loss:0.7120674848556519 val_loss0.7082199454307556\n",
            "iteration 270 :train_loss:0.7120551466941833 val_loss0.7082067728042603\n",
            "iteration 271 :train_loss:0.7120439410209656 val_loss0.7081948518753052\n",
            "iteration 272 :train_loss:0.7120329141616821 val_loss0.7081830501556396\n",
            "iteration 273 :train_loss:0.7120220065116882 val_loss0.708171546459198\n",
            "iteration 274 :train_loss:0.7120112776756287 val_loss0.7081601619720459\n",
            "iteration 275 :train_loss:0.7120007872581482 val_loss0.7081489562988281\n",
            "iteration 276 :train_loss:0.7119905352592468 val_loss0.7081379890441895\n",
            "iteration 277 :train_loss:0.7119804620742798 val_loss0.7081272602081299\n",
            "iteration 278 :train_loss:0.7119705677032471 val_loss0.7081167101860046\n",
            "iteration 279 :train_loss:0.711961030960083 val_loss0.7081064581871033\n",
            "iteration 280 :train_loss:0.7119516730308533 val_loss0.708096444606781\n",
            "iteration 281 :train_loss:0.711942732334137 val_loss0.7080868482589722\n",
            "iteration 282 :train_loss:0.711933970451355 val_loss0.7080773115158081\n",
            "iteration 283 :train_loss:0.7119255065917969 val_loss0.7080681324005127\n",
            "iteration 284 :train_loss:0.7119168639183044 val_loss0.7080589532852173\n",
            "iteration 285 :train_loss:0.7119084596633911 val_loss0.7080499529838562\n",
            "iteration 286 :train_loss:0.7119002938270569 val_loss0.7080409526824951\n",
            "iteration 287 :train_loss:0.7118921279907227 val_loss0.7080323100090027\n",
            "iteration 288 :train_loss:0.7118842601776123 val_loss0.7080236673355103\n",
            "iteration 289 :train_loss:0.7118765115737915 val_loss0.7080153226852417\n",
            "iteration 290 :train_loss:0.711868941783905 val_loss0.7080069780349731\n",
            "iteration 291 :train_loss:0.7118613719940186 val_loss0.7079988718032837\n",
            "iteration 292 :train_loss:0.7118540406227112 val_loss0.707990825176239\n",
            "iteration 293 :train_loss:0.7118468880653381 val_loss0.7079830765724182\n",
            "iteration 294 :train_loss:0.7118398547172546 val_loss0.7079752683639526\n",
            "iteration 295 :train_loss:0.7118328213691711 val_loss0.7079676985740662\n",
            "iteration 296 :train_loss:0.7118259072303772 val_loss0.7079601287841797\n",
            "iteration 297 :train_loss:0.711819589138031 val_loss0.7079532146453857\n",
            "iteration 298 :train_loss:0.7118142247200012 val_loss0.707947313785553\n",
            "iteration 299 :train_loss:0.7118100523948669 val_loss0.707942545413971\n",
            "iteration 300 :train_loss:0.7118064165115356 val_loss0.7079384922981262\n",
            "iteration 301 :train_loss:0.7118029594421387 val_loss0.7079344987869263\n",
            "iteration 302 :train_loss:0.711799681186676 val_loss0.7079307436943054\n",
            "iteration 303 :train_loss:0.7117964029312134 val_loss0.7079269289970398\n",
            "iteration 304 :train_loss:0.7117932438850403 val_loss0.707923173904419\n",
            "iteration 305 :train_loss:0.711790144443512 val_loss0.7079195380210876\n",
            "iteration 306 :train_loss:0.7117870450019836 val_loss0.7079160213470459\n",
            "iteration 307 :train_loss:0.7117840647697449 val_loss0.7079125046730042\n",
            "iteration 308 :train_loss:0.7117810845375061 val_loss0.707909107208252\n",
            "iteration 309 :train_loss:0.7117781639099121 val_loss0.7079057097434998\n",
            "iteration 310 :train_loss:0.7117754817008972 val_loss0.7079026103019714\n",
            "iteration 311 :train_loss:0.7117730379104614 val_loss0.7078996300697327\n",
            "iteration 312 :train_loss:0.7117703557014465 val_loss0.7078965306282043\n",
            "iteration 313 :train_loss:0.7117680311203003 val_loss0.7078937888145447\n",
            "iteration 314 :train_loss:0.7117655873298645 val_loss0.7078908681869507\n",
            "iteration 315 :train_loss:0.711763322353363 val_loss0.7078881859779358\n",
            "iteration 316 :train_loss:0.7117609977722168 val_loss0.7078853249549866\n",
            "iteration 317 :train_loss:0.7117587327957153 val_loss0.7078825831413269\n",
            "iteration 318 :train_loss:0.7117564678192139 val_loss0.7078798413276672\n",
            "iteration 319 :train_loss:0.711754322052002 val_loss0.7078772783279419\n",
            "iteration 320 :train_loss:0.7117522954940796 val_loss0.7078749537467957\n",
            "iteration 321 :train_loss:0.7117506265640259 val_loss0.7078728079795837\n",
            "iteration 322 :train_loss:0.7117496132850647 val_loss0.7078716158866882\n",
            "iteration 323 :train_loss:0.7117489576339722 val_loss0.7078706622123718\n",
            "iteration 324 :train_loss:0.711748480796814 val_loss0.7078699469566345\n",
            "iteration 325 :train_loss:0.7117480039596558 val_loss0.707869291305542\n",
            "iteration 326 :train_loss:0.7117477059364319 val_loss0.7078686356544495\n",
            "iteration 327 :train_loss:0.7117471694946289 val_loss0.7078679800033569\n",
            "iteration 328 :train_loss:0.7117468118667603 val_loss0.7078673243522644\n",
            "iteration 329 :train_loss:0.7117463946342468 val_loss0.7078668475151062\n",
            "iteration 330 :train_loss:0.7117461562156677 val_loss0.7078663110733032\n",
            "iteration 331 :train_loss:0.7117457985877991 val_loss0.7078657746315002\n",
            "iteration 332 :train_loss:0.7117455005645752 val_loss0.707865297794342\n",
            "iteration 333 :train_loss:0.7117452621459961 val_loss0.7078648209571838\n",
            "iteration 334 :train_loss:0.7117449045181274 val_loss0.7078644037246704\n",
            "iteration 335 :train_loss:0.7117447257041931 val_loss0.7078640460968018\n",
            "iteration 336 :train_loss:0.7117444276809692 val_loss0.7078635692596436\n",
            "iteration 337 :train_loss:0.7117442488670349 val_loss0.7078633308410645\n",
            "iteration 338 :train_loss:0.7117440700531006 val_loss0.7078630328178406\n",
            "iteration 339 :train_loss:0.711743950843811 val_loss0.7078627347946167\n",
            "iteration 340 :train_loss:0.7117437720298767 val_loss0.707862377166748\n",
            "iteration 341 :train_loss:0.7117435932159424 val_loss0.707862138748169\n",
            "iteration 342 :train_loss:0.7117434740066528 val_loss0.7078619003295898\n",
            "iteration 343 :train_loss:0.7117433547973633 val_loss0.7078615427017212\n",
            "iteration 344 :train_loss:0.711743175983429 val_loss0.7078613042831421\n",
            "iteration 345 :train_loss:0.7117431163787842 val_loss0.7078609466552734\n",
            "iteration 346 :train_loss:0.7117429375648499 val_loss0.7078607082366943\n",
            "iteration 347 :train_loss:0.7117428183555603 val_loss0.7078604698181152\n",
            "iteration 348 :train_loss:0.711742639541626 val_loss0.7078602313995361\n",
            "iteration 349 :train_loss:0.7117424607276917 val_loss0.707859992980957\n",
            "iteration 350 :train_loss:0.7117424011230469 val_loss0.7078597545623779\n",
            "iteration 351 :train_loss:0.7117422819137573 val_loss0.7078595161437988\n",
            "iteration 352 :train_loss:0.7117421627044678 val_loss0.7078592777252197\n",
            "iteration 353 :train_loss:0.7117420434951782 val_loss0.7078590393066406\n",
            "iteration 354 :train_loss:0.7117419838905334 val_loss0.7078588008880615\n",
            "iteration 355 :train_loss:0.7117418050765991 val_loss0.7078586220741272\n",
            "iteration 356 :train_loss:0.7117416858673096 val_loss0.7078583836555481\n",
            "iteration 357 :train_loss:0.7117415070533752 val_loss0.7078580856323242\n",
            "iteration 358 :train_loss:0.7117415070533752 val_loss0.7078579068183899\n",
            "iteration 359 :train_loss:0.7117413282394409 val_loss0.707857608795166\n",
            "iteration 360 :train_loss:0.7117412686347961 val_loss0.7078574299812317\n",
            "iteration 361 :train_loss:0.7117411494255066 val_loss0.7078571915626526\n",
            "iteration 362 :train_loss:0.711741030216217 val_loss0.707857072353363\n",
            "iteration 363 :train_loss:0.7117409110069275 val_loss0.7078567743301392\n",
            "iteration 364 :train_loss:0.7117408514022827 val_loss0.7078565955162048\n",
            "iteration 365 :train_loss:0.7117406725883484 val_loss0.7078564167022705\n",
            "iteration 366 :train_loss:0.7117406725883484 val_loss0.7078561782836914\n",
            "iteration 367 :train_loss:0.7117404937744141 val_loss0.7078559994697571\n",
            "iteration 368 :train_loss:0.7117404341697693 val_loss0.707855761051178\n",
            "iteration 369 :train_loss:0.7117403745651245 val_loss0.7078556418418884\n",
            "iteration 370 :train_loss:0.7117401957511902 val_loss0.7078553438186646\n",
            "iteration 371 :train_loss:0.7117401957511902 val_loss0.7078551650047302\n",
            "iteration 372 :train_loss:0.7117400169372559 val_loss0.7078549861907959\n",
            "iteration 373 :train_loss:0.7117399573326111 val_loss0.7078548073768616\n",
            "iteration 374 :train_loss:0.7117398977279663 val_loss0.7078545689582825\n",
            "iteration 375 :train_loss:0.7117397785186768 val_loss0.7078544497489929\n",
            "iteration 376 :train_loss:0.7117395997047424 val_loss0.7078542113304138\n",
            "iteration 377 :train_loss:0.7117395401000977 val_loss0.7078540921211243\n",
            "iteration 378 :train_loss:0.7117395401000977 val_loss0.7078538537025452\n",
            "iteration 379 :train_loss:0.7117394804954529 val_loss0.7078537344932556\n",
            "iteration 380 :train_loss:0.7117393612861633 val_loss0.7078535556793213\n",
            "iteration 381 :train_loss:0.7117393612861633 val_loss0.7078534364700317\n",
            "iteration 382 :train_loss:0.7117393016815186 val_loss0.7078532576560974\n",
            "iteration 383 :train_loss:0.7117392420768738 val_loss0.7078531384468079\n",
            "iteration 384 :train_loss:0.7117391228675842 val_loss0.7078530192375183\n",
            "iteration 385 :train_loss:0.7117391228675842 val_loss0.707852840423584\n",
            "iteration 386 :train_loss:0.7117390632629395 val_loss0.7078527212142944\n",
            "iteration 387 :train_loss:0.7117390632629395 val_loss0.7078525424003601\n",
            "iteration 388 :train_loss:0.7117388844490051 val_loss0.7078524231910706\n",
            "iteration 389 :train_loss:0.7117388844490051 val_loss0.707852303981781\n",
            "iteration 390 :train_loss:0.7117388844490051 val_loss0.7078522443771362\n",
            "iteration 391 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 392 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 393 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 394 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 395 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 396 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 397 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 398 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 399 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 400 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 401 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 402 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 403 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 404 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 405 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 406 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 407 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 408 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 409 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 410 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 411 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 412 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 413 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 414 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 415 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 416 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 417 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 418 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 419 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 420 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 421 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 422 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 423 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 424 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 425 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 426 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 427 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 428 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 429 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 430 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 431 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 432 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 433 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 434 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 435 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 436 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 437 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 438 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 439 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 440 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 441 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 442 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 443 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 444 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 445 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 446 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 447 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 448 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 449 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 450 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 451 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 452 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 453 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 454 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 455 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 456 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 457 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 458 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 459 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 460 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 461 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 462 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 463 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 464 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 465 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 466 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 467 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 468 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 469 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 470 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 471 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 472 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 473 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 474 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 475 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 476 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 477 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 478 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 479 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 480 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 481 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 482 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 483 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 484 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 485 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 486 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 487 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 488 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 489 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 490 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 491 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 492 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 493 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 494 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 495 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 496 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 497 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 498 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 499 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 500 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 501 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 502 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 503 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 504 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 505 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 506 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 507 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 508 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 509 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 510 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 511 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 512 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 513 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 514 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 515 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 516 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 517 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 518 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 519 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 520 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 521 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 522 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 523 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 524 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 525 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 526 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 527 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 528 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 529 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 530 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 531 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 532 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 533 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 534 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 535 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 536 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 537 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 538 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 539 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 540 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 541 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 542 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 543 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 544 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 545 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 546 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 547 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 548 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 549 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 550 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 551 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 552 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 553 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 554 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 555 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 556 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 557 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 558 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 559 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 560 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 561 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 562 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 563 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 564 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 565 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 566 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 567 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 568 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 569 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 570 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 571 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 572 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 573 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 574 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 575 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 576 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 577 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 578 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 579 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 580 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 581 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 582 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 583 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 584 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 585 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 586 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 587 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 588 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 589 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 590 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 591 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 592 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 593 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 594 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 595 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 596 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 597 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 598 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 599 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 600 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 601 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 602 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 603 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 604 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 605 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 606 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 607 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 608 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 609 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 610 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 611 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 612 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 613 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 614 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 615 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 616 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 617 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 618 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 619 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 620 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 621 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 622 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 623 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 624 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 625 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 626 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 627 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 628 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 629 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 630 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 631 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 632 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 633 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 634 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 635 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 636 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 637 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 638 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 639 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 640 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 641 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 642 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 643 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 644 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 645 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 646 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 647 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 648 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 649 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 650 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 651 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 652 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 653 :train_loss:0.7117388248443604 val_loss0.7078520059585571\n",
            "iteration 654 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 655 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 656 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 657 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 658 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 659 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 660 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 661 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 662 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 663 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 664 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 665 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 666 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 667 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 668 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 669 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 670 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 671 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 672 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 673 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 674 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 675 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 676 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 677 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 678 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 679 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 680 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 681 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 682 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 683 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 684 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 685 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 686 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 687 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 688 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 689 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 690 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 691 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 692 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 693 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 694 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 695 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 696 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 697 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 698 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 699 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 700 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 701 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 702 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 703 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 704 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 705 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 706 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 707 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 708 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 709 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 710 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 711 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 712 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 713 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 714 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 715 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 716 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 717 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 718 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 719 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 720 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 721 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 722 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 723 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 724 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 725 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 726 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 727 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 728 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 729 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 730 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 731 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 732 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 733 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 734 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 735 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 736 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 737 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 738 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 739 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 740 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 741 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 742 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 743 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 744 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 745 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 746 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 747 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 748 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 749 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 750 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 751 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 752 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 753 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 754 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 755 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 756 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 757 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 758 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 759 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 760 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 761 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 762 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 763 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 764 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 765 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 766 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 767 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 768 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 769 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 770 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 771 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 772 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 773 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 774 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 775 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 776 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 777 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 778 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 779 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 780 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 781 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 782 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 783 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 784 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 785 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 786 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 787 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 788 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 789 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 790 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 791 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 792 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 793 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 794 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 795 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 796 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 797 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 798 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 799 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 800 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 801 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 802 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 803 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 804 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 805 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 806 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 807 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 808 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 809 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 810 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 811 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 812 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 813 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 814 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 815 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 816 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 817 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 818 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 819 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 820 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 821 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 822 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 823 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 824 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 825 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 826 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 827 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 828 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 829 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 830 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 831 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 832 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 833 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 834 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 835 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 836 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 837 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 838 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 839 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 840 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 841 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 842 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 843 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 844 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 845 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 846 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 847 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 848 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 849 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 850 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 851 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 852 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 853 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 854 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 855 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 856 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 857 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 858 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 859 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 860 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 861 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 862 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 863 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 864 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 865 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 866 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 867 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 868 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 869 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 870 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 871 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 872 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 873 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 874 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 875 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 876 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 877 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 878 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 879 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 880 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 881 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 882 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 883 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 884 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 885 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 886 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 887 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 888 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 889 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 890 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 891 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 892 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 893 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 894 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 895 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 896 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 897 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 898 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 899 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 900 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 901 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 902 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 903 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 904 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 905 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 906 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 907 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 908 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 909 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 910 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 911 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 912 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 913 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 914 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 915 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 916 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 917 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 918 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 919 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 920 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 921 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 922 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 923 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 924 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 925 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 926 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 927 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 928 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 929 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 930 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 931 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 932 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 933 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 934 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 935 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 936 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 937 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 938 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 939 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 940 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 941 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 942 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 943 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 944 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 945 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 946 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 947 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 948 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 949 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 950 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 951 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 952 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 953 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 954 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 955 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 956 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 957 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 958 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 959 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 960 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 961 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 962 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 963 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 964 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 965 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 966 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 967 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 968 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 969 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 970 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 971 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 972 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 973 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 974 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 975 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 976 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 977 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 978 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 979 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 980 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 981 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 982 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 983 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 984 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 985 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 986 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 987 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 988 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 989 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 990 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 991 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 992 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 993 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 994 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 995 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 996 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 997 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 998 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 999 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1000 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1001 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1002 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1003 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1004 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1005 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1006 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1007 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1008 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1009 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1010 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1011 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1012 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1013 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1014 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1015 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1016 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1017 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1018 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1019 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1020 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1021 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1022 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1023 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1024 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1025 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1026 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1027 :train_loss:0.7117388248443604 val_loss0.7078520059585571\n",
            "iteration 1028 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1029 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1030 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1031 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 1032 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1033 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1034 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1035 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1036 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1037 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1038 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1039 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1040 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1041 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1042 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1043 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1044 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1045 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1046 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1047 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1048 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1049 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1050 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1051 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1052 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1053 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1054 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1055 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1056 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1057 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1058 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1059 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1060 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1061 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1062 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1063 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1064 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1065 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1066 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1067 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1068 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1069 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1070 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1071 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1072 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1073 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1074 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1075 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1076 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1077 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1078 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1079 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1080 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1081 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1082 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1083 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1084 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1085 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1086 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1087 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1088 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1089 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1090 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1091 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1092 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1093 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1094 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1095 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1096 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1097 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1098 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1099 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1100 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1101 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1102 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1103 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1104 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1105 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1106 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1107 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1108 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1109 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1110 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1111 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1112 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1113 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1114 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1115 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1116 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1117 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1118 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1119 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1120 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1121 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1122 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1123 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1124 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1125 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1126 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1127 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1128 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1129 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1130 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1131 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1132 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1133 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1134 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1135 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1136 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1137 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1138 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1139 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1140 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1141 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1142 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1143 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1144 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1145 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1146 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1147 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1148 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1149 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1150 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1151 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1152 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1153 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1154 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1155 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1156 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1157 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1158 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1159 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1160 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1161 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1162 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1163 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1164 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1165 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1166 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1167 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1168 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1169 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1170 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1171 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1172 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1173 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1174 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1175 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1176 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1177 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1178 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1179 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1180 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1181 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1182 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1183 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1184 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1185 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1186 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1187 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1188 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1189 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1190 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1191 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1192 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1193 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1194 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1195 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1196 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1197 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1198 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1199 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1200 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1201 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1202 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1203 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1204 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1205 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1206 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1207 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1208 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1209 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1210 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1211 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1212 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1213 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1214 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1215 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1216 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1217 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1218 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1219 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1220 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1221 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1222 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1223 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1224 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1225 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1226 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1227 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1228 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1229 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1230 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1231 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1232 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1233 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1234 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1235 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1236 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1237 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1238 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1239 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1240 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1241 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1242 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1243 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 1244 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1245 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1246 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1247 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1248 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1249 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1250 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1251 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1252 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1253 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1254 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1255 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1256 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1257 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1258 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1259 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1260 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1261 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1262 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1263 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1264 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1265 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1266 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1267 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1268 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1269 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1270 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1271 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1272 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1273 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1274 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1275 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1276 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1277 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 1278 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1279 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1280 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1281 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1282 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1283 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1284 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1285 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1286 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1287 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1288 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1289 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1290 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1291 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1292 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1293 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1294 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1295 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1296 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1297 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1298 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1299 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1300 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1301 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1302 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1303 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1304 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1305 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1306 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1307 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1308 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1309 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1310 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1311 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1312 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1313 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1314 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1315 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1316 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1317 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1318 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1319 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1320 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1321 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1322 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1323 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1324 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1325 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1326 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1327 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1328 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1329 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1330 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1331 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1332 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1333 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1334 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1335 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1336 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1337 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1338 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1339 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1340 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1341 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1342 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1343 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1344 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1345 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1346 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1347 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1348 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1349 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1350 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1351 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1352 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1353 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1354 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1355 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1356 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1357 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1358 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1359 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1360 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1361 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1362 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1363 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1364 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1365 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1366 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1367 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1368 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1369 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1370 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1371 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1372 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1373 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1374 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1375 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1376 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1377 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1378 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1379 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1380 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1381 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1382 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1383 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1384 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1385 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1386 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1387 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1388 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1389 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1390 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1391 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1392 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1393 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1394 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1395 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1396 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1397 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1398 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1399 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1400 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1401 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1402 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1403 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1404 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1405 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 1406 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1407 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1408 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1409 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1410 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1411 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1412 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1413 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1414 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1415 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1416 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1417 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1418 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1419 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1420 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1421 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1422 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1423 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1424 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1425 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1426 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1427 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1428 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1429 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1430 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1431 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1432 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1433 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1434 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1435 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1436 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1437 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1438 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1439 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1440 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1441 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1442 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1443 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1444 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1445 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1446 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1447 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1448 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1449 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1450 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1451 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1452 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1453 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1454 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1455 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1456 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1457 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1458 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1459 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1460 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1461 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1462 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1463 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1464 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1465 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1466 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1467 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1468 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1469 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1470 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1471 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1472 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1473 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1474 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1475 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1476 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1477 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1478 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1479 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1480 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1481 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1482 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1483 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1484 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1485 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1486 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1487 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1488 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1489 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1490 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1491 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1492 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1493 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1494 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1495 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1496 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1497 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1498 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1499 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1500 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1501 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1502 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1503 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1504 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1505 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1506 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1507 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1508 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1509 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1510 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1511 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1512 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1513 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1514 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1515 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1516 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1517 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1518 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1519 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1520 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1521 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1522 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1523 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1524 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1525 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1526 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1527 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1528 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1529 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1530 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1531 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1532 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1533 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1534 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1535 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1536 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1537 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1538 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1539 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1540 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1541 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1542 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1543 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1544 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1545 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1546 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1547 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1548 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1549 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1550 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1551 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1552 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1553 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1554 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1555 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1556 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1557 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1558 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1559 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1560 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1561 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1562 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1563 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1564 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1565 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1566 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1567 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1568 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1569 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1570 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1571 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1572 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1573 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1574 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1575 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1576 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1577 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1578 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1579 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1580 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1581 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1582 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1583 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1584 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1585 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1586 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1587 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1588 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1589 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1590 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1591 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1592 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1593 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1594 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1595 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1596 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1597 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1598 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1599 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1600 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1601 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1602 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1603 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1604 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1605 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1606 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1607 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1608 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1609 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1610 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1611 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1612 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1613 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1614 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1615 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1616 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1617 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 1618 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1619 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1620 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1621 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1622 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1623 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1624 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1625 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1626 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1627 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1628 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1629 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1630 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1631 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1632 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1633 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1634 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1635 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1636 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1637 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1638 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1639 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1640 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1641 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1642 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1643 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1644 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1645 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1646 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1647 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1648 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1649 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1650 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1651 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 1652 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1653 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1654 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1655 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1656 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1657 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1658 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1659 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1660 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1661 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1662 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1663 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1664 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1665 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1666 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1667 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1668 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1669 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1670 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1671 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1672 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1673 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1674 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1675 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1676 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1677 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1678 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1679 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1680 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1681 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1682 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1683 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1684 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1685 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1686 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1687 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1688 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1689 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1690 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1691 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1692 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1693 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1694 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1695 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1696 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1697 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1698 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1699 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1700 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1701 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1702 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1703 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1704 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1705 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1706 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1707 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1708 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1709 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1710 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1711 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1712 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1713 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1714 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1715 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1716 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1717 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1718 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1719 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1720 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1721 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1722 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1723 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1724 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1725 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1726 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1727 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1728 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1729 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1730 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1731 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1732 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1733 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1734 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1735 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1736 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1737 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1738 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1739 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1740 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1741 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1742 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1743 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1744 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1745 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1746 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1747 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1748 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1749 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1750 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1751 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1752 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1753 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1754 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1755 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1756 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1757 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1758 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1759 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1760 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1761 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1762 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1763 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1764 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1765 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1766 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1767 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1768 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1769 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1770 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1771 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1772 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1773 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1774 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1775 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1776 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1777 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1778 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1779 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 1780 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1781 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1782 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1783 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1784 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1785 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1786 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1787 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1788 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1789 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1790 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1791 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1792 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1793 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1794 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1795 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1796 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1797 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1798 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1799 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1800 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1801 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1802 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1803 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1804 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1805 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1806 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1807 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1808 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1809 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1810 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1811 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1812 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1813 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1814 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1815 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1816 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1817 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1818 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1819 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1820 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1821 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1822 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1823 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1824 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1825 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1826 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1827 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1828 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1829 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1830 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1831 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1832 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1833 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1834 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1835 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1836 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1837 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1838 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1839 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1840 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1841 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1842 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1843 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1844 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1845 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1846 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1847 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1848 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1849 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1850 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1851 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1852 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1853 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1854 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1855 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1856 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1857 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1858 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1859 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1860 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1861 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1862 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1863 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1864 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1865 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1866 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1867 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1868 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1869 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1870 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1871 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1872 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1873 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1874 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1875 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1876 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1877 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1878 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1879 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1880 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1881 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1882 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1883 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1884 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1885 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1886 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1887 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1888 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1889 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1890 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1891 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1892 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1893 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1894 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1895 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1896 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1897 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1898 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1899 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1900 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1901 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1902 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1903 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1904 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1905 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1906 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1907 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1908 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1909 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1910 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1911 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1912 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1913 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1914 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1915 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1916 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1917 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1918 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1919 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1920 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1921 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1922 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1923 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1924 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1925 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1926 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1927 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1928 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1929 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1930 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1931 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1932 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1933 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1934 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1935 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1936 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1937 :train_loss:0.7117387056350708 val_loss0.7078520655632019\n",
            "iteration 1938 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1939 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1940 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1941 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1942 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1943 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1944 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1945 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1946 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1947 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1948 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1949 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1950 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1951 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1952 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1953 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1954 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1955 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1956 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1957 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1958 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1959 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1960 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1961 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1962 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1963 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1964 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1965 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1966 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1967 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1968 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1969 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1970 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1971 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1972 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1973 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1974 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1975 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1976 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1977 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1978 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1979 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1980 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1981 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1982 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1983 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1984 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1985 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1986 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1987 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1988 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1989 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1990 :train_loss:0.7117387056350708 val_loss0.7078519463539124\n",
            "iteration 1991 :train_loss:0.7117388248443604 val_loss0.7078519463539124\n",
            "iteration 1992 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1993 :train_loss:0.7117388248443604 val_loss0.7078518271446228\n",
            "iteration 1994 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1995 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1996 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1997 :train_loss:0.7117387056350708 val_loss0.7078520059585571\n",
            "iteration 1998 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n",
            "iteration 1999 :train_loss:0.7117387056350708 val_loss0.7078518271446228\n"
          ]
        }
      ],
      "source": [
        "iterations=2000\n",
        "nh_array = np.array([10,5,5,4,3])\n",
        "parameters, history=create_nn_model(train_X,train_Y,nh_array, val_X, val_Y, iterations, 0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-WxiAM1AQbl"
      },
      "source": [
        "Plotting the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_sA8hwk5ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "b83be742-bc59-44fd-c775-92a429bd9f80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(0,iterations),history[\"train_loss\"],'b')\n",
        "plt.plot(range(0,iterations),history[\"val_loss\"],'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH6xFpT9ASie"
      },
      "source": [
        "Calculating the loss of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWNka64D2pLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee3ca01-f4e8-4e6e-ab0f-a915802ba8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss of the model on the training data is: 0.7117388248443604\n",
            "loss of the model on the validation data is: 0.7078518271446228\n"
          ]
        }
      ],
      "source": [
        "predicted_train=predict(parameters, train_X,nh_array)\n",
        "predicted_val=predict(parameters, val_X,nh_array)\n",
        "\n",
        "print(\"loss of the model on the training data is:\", float(compute_loss(train_Y,predicted_train)))\n",
        "print(\"loss of the model on the validation data is:\", float(compute_loss(val_Y,predicted_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CBdULORYSu4"
      },
      "source": [
        "**Experimenting with different hyperparameter values**\n",
        "\n",
        "Number of hidden layers:3\n",
        "\n",
        "Number of neurons in each hidden layer: [1,2,3]\n",
        "\n",
        "training loss:0.7283483743667603\n",
        "\n",
        "validation loss:0.7142391800880432\n",
        "\n",
        "Number of hidden layers:3\n",
        "\n",
        "Number of neurons in each hidden layer:[2,4,2]\n",
        "\n",
        "training loss: 0.7283487319946289\n",
        "\n",
        "validation loss:0.7142395973205566\n",
        "\n",
        "Number of hidden layers:3\n",
        "\n",
        "Number of neurons in each hidden layer:[10,5,2]\n",
        "\n",
        "training loss:0.7283509969711304\n",
        "\n",
        "validation loss:0.7142421007156372\n",
        "\n",
        "Number of hidden layers:5\n",
        "\n",
        "Number of neurons in each hidden layer:[10,5,5,4,3]\n",
        "\n",
        "training loss: 0.7117388248443604\n",
        "\n",
        "validation loss: 0.7078518271446228\n",
        "\n",
        "Number of hidden layers:2\n",
        "\n",
        "Number of neurons in each hidden layer:[2,1]\n",
        "\n",
        "training loss: 0.7283496856689453\n",
        "\n",
        "validation loss: 0.7142412066459656\n",
        "\n",
        "There isn't much difference in the loss for this model by changing the hyperparameters. But the best set of hyperparameters is\n",
        "\n",
        "number_of_hidden_layers = 5\n",
        "\n",
        "\n",
        "number of nodes in hidden layers = [10,5,5,4,3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VS0okoF77cF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iEvNajA77KC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78RHiF3-78D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219ea4b4-0e1d-4d8f-b28b-9d686745dfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse of the model on the test data is: 1.1691738367080688\n"
          ]
        }
      ],
      "source": [
        "#computing root mean square error\n",
        "def rmse(Y,Yhat):\n",
        "    return np.sqrt(np.mean((Y-Yhat)**2))\n",
        "\n",
        "#getting predictions on test data\n",
        "def predict_test(parameters,X,nh_array, prob_threshold=0.5):\n",
        "    Yhat=forward_pass(parameters, X,nh_array)\n",
        "    predicted_label=Yhat\n",
        "    return predicted_label\n",
        "\n",
        "\n",
        "predicted_test=predict_test(parameters, test_X,nh_array)\n",
        "print(\"rmse of the model on the test data is:\", float(rmse(test_Y,predicted_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Computing root mean square error\n",
        "def rmse(Y, Yhat):\n",
        "    return np.sqrt(np.mean((Y - Yhat) ** 2))\n",
        "\n",
        "# Getting predictions on test data\n",
        "def predict_test(parameters, X, nh_array, prob_threshold=0.5):\n",
        "    Yhat = forward_pass(parameters, X, nh_array)\n",
        "    predicted_label = Yhat\n",
        "    return predicted_label\n",
        "\n",
        "# predicted_test = predict_test(parameters, test_X, nh_array)\n",
        "# print(\"RMSE of the model on the test data is:\", float(rmse(test_Y, predicted_test)))\n",
        "\n",
        "# Linear Regression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(train_X.T, train_Y.T)\n",
        "\n",
        "# Predict on validation data\n",
        "predicted_val = regressor.predict(val_X.T)\n",
        "\n",
        "# Dropping NaN from test_X and test_Y\n",
        "test_X = test_X.T\n",
        "test_Y = test_Y.T\n",
        "\n",
        "# Convert to DataFrame\n",
        "test_X = pd.DataFrame(test_X)\n",
        "test_Y = pd.DataFrame(test_Y)\n",
        "\n",
        "# Drop NaN values\n",
        "test_X = test_X.dropna()\n",
        "test_Y = test_Y.loc[test_X.index]\n",
        "\n",
        "# Convert back to numpy array\n",
        "test_X = test_X.to_numpy()\n",
        "test_Y = test_Y.to_numpy()\n",
        "\n",
        "# Transpose back to original shape\n",
        "test_X = test_X.T\n",
        "test_Y = test_Y.T\n",
        "\n",
        "# Predict on test data\n",
        "predicted_test = regressor.predict(test_X.T)\n",
        "\n",
        "# Calculate RMSE\n",
        "print(\"RMSE of the model on the validation data is:\", float(rmse(val_Y.T, predicted_val)))\n",
        "print(\"RMSE of the model on the test data is:\", float(rmse(test_Y.T, predicted_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av8LQVGYpz8j",
        "outputId": "f2baf39d-43af-46e8-be7d-d4f57a72575f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE of the model on the validation data is: 0.2943569001179015\n",
            "RMSE of the model on the test data is: 0.2999820066362248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87IjtBfWC7RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rmse of the Neural Network model on the test data is: 1.1691738367080688\n",
        "\n",
        "rmse of the Linear Regression model on the test data is: 0.2999820066362248\n",
        "\n",
        "\n",
        "Linear regression seems to work better for this dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "i-jxVQBwPUVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting predicted vs true labels\n",
        "plt.scatter(test_Y,predicted_test)\n",
        "plt.xlabel(\"true labels\")\n",
        "plt.ylabel(\"predicted labels\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ePm64c0IdH_p",
        "outputId": "17a07b23-de89-4b32-bdc2-8e252b730c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOJB/irA/UPuA4Z+TnurITN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
